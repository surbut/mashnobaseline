---
title: "mashonly"
output: html_document
---
###Compare with MASH##
```{r setup, include=FALSE}
library('knitr')
system("ls; rm *rds;rm *txt; rm *pdf")
rm(list=ls())
```
Now we can compare with mash. In this model, for each gene J:

$$chat |\mu, V = \mu + \beta + E$$

$$chat - \mu = \beta + E$$

and $$E \sim N(0,V)$$
while $$\beta  ~ \sum_{p} \pi_{p} N(0, U_{p})$$
```{r}
rm(list=ls())

library('mashr')
set.seed(123)
c=chat_sim_fsingle_fixedomega(n = 1000,d = 8,omega = 2,esd = 1,reals=1)
saveRDS(c,"chatfixedomega.rds")
diag(c$covmat)


t=c$t;b=c$chat;se=c$shat;R=ncol(t)

#Now wfull will be the median
wfull=t(apply(b,1,function(x){x-median(x)}))
##show that they sum to 0

#plot(rowSums(wfull),ylim=c(-1,1))
```

Now we proceed as in mash, selecting the strongest deviations from the median:
```{r}

s.j=se/se
absmat=abs(t(apply(t,1,function(x){x-median(x)})))
tmedian=(t(apply(t,1,function(x){x-median(x)})))

index=which(rowSums(c$beta)!=0)
sparsestrongt=t[index,]
strongprojectedt=sparsestrongt
```



```{r filewrite,eval=T}

write.table(sparsestrongt,"sparsestrongt.txt",col.names = FALSE,row.names=FALSE)

```


Now, we need to project into the centered space to estimate the covariance matrix of the true deviations, using the full L since $v$ will be R, and not $R-1$.


```{r}
system('/Users/sarahurbut/miniconda3/bin/sfa -gen sparsestrongt.txt -g 1000 -k 5 -n 8 i -o sparseFmediancentered')
A="sparseFmediancentered"

factor.mat=as.matrix(read.table("sparseFmediancentered_F.out"))
lambda.mat=as.matrix(read.table("sparseFmediancentered_lambda.out"))

library("ExtremeDeconvolution")
t.stat=strongprojectedt
w=chat%*%L
R=ncol(t.stat)
init.cov=init.covmat(t.stat=t.stat,factor.mat = factor.mat,lambda.mat = lambda.mat,K=K,P=P)
init.cov.list=list()
for(i in 1:K){init.cov.list[[i]]=init.cov[i,,]}
head(init.cov.list)
mean.mat=matrix(rep(0,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat))

ydata=  t.stat
xamp= rep(1/K,K)
xcovar= init.cov.list
fixmean= TRUE     
ycovar=  v.j     
xmean=   mean.mat   
projection= list();for(l in 1:nrow(t.stat)){projection[[l]]=diag(1,R)}

e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)

max.step=deconvolution.em.with.bovy(t.stat = strongprojectedt,factor.mat = factor.mat,v.j = s.j,lambda.mat = lambda.mat,K = 3,P = 3 )

##Show how the ranks is preserved
apply(max.step$true.covs,1,function(x){qr(x)$rank})


A="sparseoldmash"

covmash=compute.hm.mash.covmat.all.max.step.fixed.omega(omega = 2,Q = 5,lambda.mat = lambda.mat,factor.mat = factor.mat,max.step = max.step,zero = TRUE,power = 2,A,bma = TRUE)$cov

sapply(covmash,max)

##Show how the ranks is preserved
apply(max.step$true.covs,1,function(x){qr(x)$rank})


all.equal(covmash[[2]]/max(diag(covmash[[2]])),max.step$true.covs[1,,]/max(diag(max.step$true.covs[1,,])))
all.equal(covmash[[3]]/max(diag(covmash[[3]])),max.step$true.covs[3,,]/max(diag(max.step$true.covs[3,,])))
all.equal(covmash[[9]]/max(diag(covmash[[9]])),max.step$true.covs[2,,]/max(diag(max.step$true.covs[2,,])))

length(covmash)
# 
# 
# covmash=compute.hm.covmat.all.max.step(max.step = max.step,b.hat = wfull,se.hat = se,t.stat = strongprojectedt,Q = 5,lambda.mat = lambda.mat,A = "sparseoldmash",factor.mat = factor.mat,zero = T,power = 2)$covmat
# 
# all.equal(covmash[[2]]/max(diag(covmash[[2]])),max.step$true.covs[1,,]/max(diag(max.step$true.covs[1,,])))
# all.equal(covmash[[3]]/max(diag(covmash[[3]])),max.step$true.covs[3,,]/max(diag(max.step$true.covs[3,,])))
# all.equal(covmash[[9]]/max(diag(covmash[[9]])),max.step$true.covs[2,,]/max(diag(max.step$true.covs[2,,])))

```

Now, proceed as always, estimating likelihood and computing posteriors.

```{r mashmethod, eval=T}
A="sparseoldmash"
compute.hm.train.log.lik.pen(train.b = wfull,se.train = se,covmat = covmash,A=A,pen=1)


pis=readRDS(paste0("pis",A,".rds"))$pihat

weightedquants=lapply(seq(1:nrow(wfull)),function(j){total.quant.per.snp(j,covmash,b.gp.hat=wfull,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})
```



Let's look at the 'learned' covariance matrix of each as well as the true covariance matrices of each, and see how they comapre to the true configurations. Recall that the true configurations looked like this:

```{r}
c=readRDS("chatfixedomega.rds")
diag(c$covmat)
```

```{r}
pis=readRDS("pissparseoldmash.rds")$pihat
pi.mat.old.mash=matrix(pis[-length(pis)],ncol=18,byrow=TRUE)
covmash=readRDS("covmatsparseoldmash.rds")

library('gplots')
library('colorRamps')

for(i in 2:9){
  x=covmash[[i]]
  heatmap.2(x/max(diag(x)),Rowv = NULL,Colv = NULL,dendrogram = "none",col=blue2green,density.info = "none",trace="none",main=paste0("CovMash",round(colSums(pi.mat.old.mash)[i],2)))}


x=c$covmat
heatmap.2(x/max(diag(x)),Rowv = NULL,Colv = NULL,dendrogram = "none",col=blue2green,density.info = "none",trace="none",main="CovTruth")
```


Test to show our calculations are accurate:

```{r,eval=T,echo=T}
mash.means=as.matrix(read.table("sparseoldmashposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("sparseoldmashlfsr.txt"))[,-1]
cov=readRDS("covmatsparseoldmash.rds")
pis=readRDS("pissparseoldmash.rds")$pihat
pi.mat.old.mash=matrix(pis[-length(pis)],ncol=18,byrow=TRUE)
#pisnobase=readRDS("pissparseLwithED.rds")$pihat
#pi.mat.nobase=matrix(pisnobase[-length(pisnobase)],ncol=18,byrow=TRUE)
barplot(colSums(pi.mat.old.mash))
#barplot(colSums(pi.mat.nobase))

w=data.frame(wfull)
R=ncol(w)
j=10

arrays=post.array.per.snp.no.baseline(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se,L = diag(1,R))
mash.arrays=post.array.per.snp(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se)
##show same algebra


k=5

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

lapply(seq(1:5),function(x){all.equal(arrays[[x]],mash.arrays[[x]])})
L=diag(1,R)
LSigL=L%*%cov[[k]]%*%t(L)
LVL=L%*%V.gp.hat%*%t(L)

LSigL_list=lapply(cov,function(x){L%*%x%*%t(L)})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L)))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L))))




b.mle=as.vector(t(w[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
U.gp1kl= (post.b.gpkl.cov(V.gp.hat.inv, cov[[k]]))
  
identical(as.numeric(mash.arrays$post.means[k,]),as.numeric(post.b.gpkl.mean(b.mle = b.mle,V.gp.hat.inv = V.gp.hat.inv,U.gp1kl = U.gp1kl)))

identical(as.numeric(mash.arrays$post.covs[k,]),as.numeric(diag(U.gp1kl)))



log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))

```

Compare RMSE:

```{r}

mle=as.matrix(wfull)
beta=as.matrix(c$beta)

mash.means=as.matrix(read.table("sparseoldmashposterior.means.txt")[,-1])

standard=sqrt(mean((beta-mle)^2))
sqrt(mean((mash.means-beta)^2))/standard
#sqrt(mean((mash.nobaselinemeans-beta)^2))/standard
```



##Comparing with truth.

Now, let's try this with the true prior. Here, we use the covariance matrices that were used to simulate the data as the output of _c$covmat_


```{r}

rm(list=ls())
library('mash')

c=readRDS("chatfixedomega.rds")
diag(c$covmat)

t=c$t;b=c$chat;se=c$shat
covl=list();covl[[1]]=c$covmat
#hist(t)
#hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is 
A="withfixedomega"
covs=compute.covmat.using.truth.fixed.omega(covlist = covl,A = "withfixedomega",omega = 2,zero = TRUE)
length(covs)
barplot(diag(covs[[2]]/max(diag(covs[[2]]))))
w=data.frame(wfull)[,-1]
```

```{r,eval=T}
A="withfixedomegamnb"
compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = covs,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))


for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = covs,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```


##Test to show our calculations are accurate:
```{r,eval=T,echo=T}
mash.means=as.matrix(read.table("withfixedomegamnbposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("withfixedomegamnblfsr.txt"))[,-1]
edcov=readRDS("covmatwithfixedomega.rds")
j=10
arrays=post.array.per.snp.no.baseline(j = 10,covmat = edcov,b.gp.hat = w,se.gp.hat = se,L = L[-1,])
k=2

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

LSigL=L[-1,]%*%edcov[[k]]%*%t(L[-1,])
LVL=L[-1,]%*%V.gp.hat%*%t(L[-1,])

LSigL_list=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,])))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,]))))

log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("piswithfixedomegamnb.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))
```

And let's also do this with the mash framework:

```{r}
rm(list=ls())
c=readRDS("chatfixedomega.rds")
covl=list();covl[[1]]=c$covmat
t=c$t;b=c$chat;se=c$shat

R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is 
covs=compute.covmat.using.truth(b.gp.hat = wfull,sebetahat = sjmat,A="test",zero=TRUE,covlist=covl,power=2)

```

```{r mashtruth,eval=T}
A="mashtruthwithfixedomega"
covs=readRDS("covmatwithfixedomega.rds")
compute.hm.train.log.lik.pen(train.b = wfull,se.train = se,covmat = covs,A=A,pen=1)


pis=readRDS(paste0("pis",A,".rds"))$pihat


weightedquants=lapply(seq(1:nrow(wfull)),function(j){total.quant.per.snp(j,covs,b.gp.hat=wfull,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})

```

And test to see if calculations are accurate:


```{r,eval=T,echo=T}
cov=readRDS("covmatwithfixedomega.rds")
mash.means=as.matrix(read.table("mashtruthwithfixedomegaposterior.means.txt")[,-1])
w=data.frame(wfull)
R=ncol(w)
j=10

arrays=post.array.per.snp.no.baseline(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se,L = diag(1,R))
mash.arrays=post.array.per.snp(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se)
##show same algebra


k=2

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

lapply(seq(1:5),function(x){all.equal(arrays[[x]],mash.arrays[[x]])})
L=diag(1,R)
LSigL=L%*%cov[[k]]%*%t(L)
LVL=L%*%V.gp.hat%*%t(L)

LSigL_list=lapply(cov,function(x){L%*%x%*%t(L)})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L)))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L))))




b.mle=as.vector(t(w[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
U.gp1kl= (post.b.gpkl.cov(V.gp.hat.inv, cov[[k]]))
  
identical(as.numeric(mash.arrays$post.means[k,]),as.numeric(post.b.gpkl.mean(b.mle = b.mle,V.gp.hat.inv = V.gp.hat.inv,U.gp1kl = U.gp1kl)))

identical(as.numeric(mash.arrays$post.covs[k,]),as.numeric(diag(U.gp1kl)))


pis=readRDS("pismashtruthwithfixedomega.rds")$pihat
log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))


mashnobase.means=as.matrix(read.table("withfixedomegamnbposterior.means.txt")[,-1])

beta=as.matrix(c$beta)
mle=as.matrix(wfull)
mash.means=as.matrix(mash.means)
standard=sqrt(mean((beta-mle)^2))
sqrt(mean((mash.means-beta)^2))/standard
sqrt(mean((mashnobase.means-beta)^2))/standard
```

Let's compare the learned matrices which receive the most weighting in mash and mashnobaseline. First, let' look at the dsitribution of matrices.

Let's do the same thing comparing the pis using mash from the truth and from mashnobaseline:

```{r}
pimash=readRDS("pismashtruthwithfixedomega.rds")$pihat
pied=readRDS("piswithfixedomegamnb.rds")$pihat
par(mfrow=c(1,2))
barplot(colSums(matrix(pied[-length(pied)],ncol=2,byrow=TRUE)),names=c("ID","Uk1"),las=2,main="Mashnobase")
barplot(colSums(matrix(pimash[-length(pied)],ncol=2,byrow=TRUE)),names=c("ID","Uk1"),las=2,main="Mash")
```

We can see that using the same set of covariance matrices, _mashnobaseline_ is better able to emphasize the true configurations because the likelihood and posterior computations incorporate L. Recall, for mash:

$$w_{j,R} ~ N(0,V+U)$$
$$w_{j,R-1} ~ N(0,LVL' + LUL')$$ where L is the R-1 x R centering matrix.

In both:

$$c = \mu + \beta$$
$$chat = c + E$$
$$v \sim \sum_p N(0,Uk)$$
$$E \sim N(0,V)$$

So even if the Uks are different, the likelihood and corresponding $\pi$s will be different.




```{r}
covmash=readRDS("covmatsparseoldmash.rds")
(covmash[[3]]-min(abs(covmash[[3]])))/max(diag(covmash[[3]]))
(covmash[[9]]-min(abs(covmash[[9]])))/max(diag(covmash[[9]]))

rmse.table=c()
```
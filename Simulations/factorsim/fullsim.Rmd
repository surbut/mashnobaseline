---
title: "Testscriptsimulations"
output: html_document
---

```{r setup, include=FALSE}
library('knitr')
knitr::opts_chunk$set(cache=TRUE)
```
 In this simulation framework, there are 1000 real associations in 10000 null across 44 tissues.
 
 Each 'real association' is simulated in the following manner:
 
```{r echo=T}
chat_sim_fact()
````
Such that for every true associations a factor is chosen and 'standardized' such that the maximum value across the diagonal is one. The true effects are then simulated according to the assigned component, scaled by some factor $\omega$, and then and this scaling is added to a chosen mean for the gene, centered at o with $\sigma^{2}$ of 1. 

The true $ceff$ is then computed as 
$$ceff = \mu + \beta$$ 

and
$$chat = ceff + E$$ where E is simulated N(0,V)$ and V is diagonal.

This function reports the true $\mu$, the true $\beta$ for the 1000 real genes and their associated componenent, as well as the standard error.

***Running***

Here, I want to show how to simulate and center. Recall that to center all t statistics, we need to use $t(L_{RxR}t(T_{JxR})$. I show how choosing those that have a maximum average deviation of at least 1 or a maximum deviation of at least 4 produces similar results:


```{r}
rm(list=ls())
library('mash')
set.seed(123)
RNGkind("Mersenne-Twister")
c=chat_sim_fact(n=10000,d=40,betasd=1,esd=0.1,K=10)

identical(c$beta[1,]+c$mumat[1,],c$ceff[1,])
identical(c$chat[1,],c$ceff[1,]+c$error[1,])

saveRDS(c,"chatsimesd0.1.rds")
t=c$t;b=c$chat;se=c$shat
hist(t)
hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se

absmat=abs(t(L%*%t(c$t)))
index=which(rowMeans(absmat)>1.1)
length(index)
mean(index<1000)
index2=which(rowSums(absmat>4)>0)
mean(index2<1000)
length(index2)
#index=which(rowMeans(abs(t(L%*%t(c$t))))>1.3)##choose the associations with average deviation large.

##show median deviations
plot((rowMeans(abs(t(L%*%t(c$t))))))
points(index,(rowMeans(abs(t(L%*%t(c$t)))))[index],col="red")
plot(rowSums(absmat>4))
points(index2,rowSums(absmat>4)[index2],col="blue")

strongprojectedtsimulations=t(L%*%t(t[index,]))



write.table(strongprojectedtsimulations,"strongprojectedtsimulations.txt",col.names = FALSE,row.names=FALSE)

```


Now, we need to project into the centered space to estimate the covariance matrix of the true deviations, using the full L since $v$ will be R, and not $R-1$.


```{r}
system('/Users/sarahurbut/miniconda3/bin/sfa -gen strongprojectedtsimulations.txt -g 737 -k 5 -n 40 i -o simulationsL')
A="simulationsL"

factor.mat=as.matrix(read.table("simulationsL_F.out"))
lambda.mat=as.matrix(read.table("simulationsL_lambda.out"))

#recall here that w will now be the L[-1,]%*%t(t[strong,]), which is equivalent to removing the first column of the strong projected t. Thus the covariance of v is initated with the RxR matrices (strong projected t), and the model is trained on the strong projected t less the first column because the model has v_{rxr}|w_{r,r-1}

strongprojectedt=strongprojectedtsimulations


lvllist=genlvllist(s.j[index,],L = L[-1,])


##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR so we can have the full set of deviations and thier standard errors to choose grid
dim(wfull)
dim(sjmat)

```

Now, first run ash. We have to run this separately because we have OpenMP conflicts.
```{r,eval=FALSE}
library("ashr")
ash.pm=matrix(NA,ncol=dim(wfull)[2],nrow=dim(wfull)[1])
ash.lfsr=matrix(NA,ncol=dim(wfull)[2],nrow=dim(wfull)[1])
for(i in 1:ncol(wfull)){

a=ash(betahat=wfull[,i],sebetahat=sjmat[,i],mixcompdist="normal")
ash.pm[,i]=a$PosteriorMean
ash.lfsr[,i]=a$lfsr
}
write.table(ash.pm,"ashmeans.txt")
write.table(ash.lfsr,"ashlfsr.txt")
```

Compute the likelihood without ED:
```{r}
A="noedcov"
noedcov=compute.covmat(b.gp.hat = wfull,sebetahat = sjmat,Q = 5,t.stat = strongprojectedt,lambda.mat = lambda.mat,P = 3,A = "noedcov",factor.mat = factor.mat,bma = T,zero = T,power = 2)$cov
```
Check that all the matrices are correctly computed:
```{r}
pca=svd(strongprojectedt);v=pca$v[,1:3];d=pca$d[1:3];u=pca$u[,1:3]
pcaprox=t(u%*%diag(d)%*%t(v))%*%(u%*%diag(d)%*%t(v))/max(diag(t(u%*%diag(d)%*%t(v))%*%(u%*%diag(d)%*%t(v))))


P=3;X.c=apply(strongprojectedt,2,function(x) x-mean(x)) ##Column centered matrix of t statistics
svd.X=svd(X.c)
M=nrow(X.c)
v=svd.X$v;u=svd.X$u;d=svd.X$d
cov.pc=1/M*v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P])%*%t(v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P]))

for(q in 1:5){
load=as.matrix(lambda.mat[,q])
fact=as.matrix(factor.mat[q,])
rank.prox=load%*%t(fact)
a=(1/(M-1)*(t(rank.prox)%*% rank.prox))/max(diag((1/(M-1)*(t(rank.prox)%*% rank.prox))))
print(all.equal(as.numeric(a),as.numeric(noedcov[[3+q]]/max(diag(noedcov[[3+q]])))))}

load=as.matrix(lambda.mat)
fact=as.matrix(factor.mat)
rank.prox=load%*%(fact)
a=(1/(M-1)*(t(rank.prox)%*% rank.prox))/max(diag((1/(M-1)*(t(rank.prox)%*% rank.prox))))
all.equal(as.numeric(a),as.numeric(noedcov[[9]]/max(diag(noedcov[[9]]))))

all.equal(cov(strongprojectedt)/max(diag(cov(strongprojectedt))),noedcov[[2]]/max(diag(noedcov[[2]])))
all.equal(as.numeric(cov.pc)/max(diag(cov.pc)),as.numeric(noedcov[[3]]/max(diag(noedcov[[3]]))))

```

Now compute likelihood without ED:
```{r}
w=data.frame(wfull)[,-1]
compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = noedcov,A = "noedcov",pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))
```

Now run with ED:
```{r}
rm(A)
rm(noedcov)
rm(pis)
library("ExtremeDeconvolution")
efunc=deconvolution.em.with.bovy.with.L(t.stat = strongprojectedt,factor.mat = factor.mat,lambda.mat = lambda.mat,v.j = lvllist,P = 3,L = L[-1,],Q = 5,w = strongprojectedt[,-1])

A="simulationsL"

max.step=efunc
edcov=compute.hm.covmat.all.max.step(b.hat = wfull,se.hat = sjmat,t.stat = strongprojectedt,Q = 5,lambda.mat = lambda.mat,A=A,factor.mat = factor.mat,max.step = efunc,zero = T,power = 2)$cov
length(edcov)
```

Now, we switch to the projected (one lost rank) case:

```{r}

w=data.frame(wfull)[,-1]

compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = edcov,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))


for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = edcov,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```
We can also compare our results to what we would get using the eqtlbma configs:

```{r}

rm(A)
A = "withbmaL"
covbma=compute.covmat.with.heterogeneity.no.shrink(b.gp.hat = wfull,sebetahat = sjmat,A = A,zero = T,power = 2 )


compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = covbma,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat


for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = covbma,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```

Compare RMSE and power:

```{r}
##now we read in computed means and lfsrs
bma.means=as.matrix(read.table("withbmaLposterior.means.txt")[,-1])
lfsr.bma=as.matrix(read.table("withbmaLlfsr.txt"))[,-1]
mash.means=as.matrix(read.table("simulationsLposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("simulationsLlfsr.txt"))[,-1]
ash.pm=as.matrix(read.table("ashmeans.txt"))
ash.lfsr=as.matrix(read.table("ashlfsr.txt"))
mle=as.matrix(wfull)
beta=as.matrix(c$beta)


standard=sqrt(mean((beta-mle)^2))
sqrt(mean((bma.means-beta)^2))/standard
sqrt(mean((mash.means-beta)^2))/standard
sqrt(mean((ash.pm-beta)^2))/standard

rmse.all.table=cbind(mash=sqrt(mean((beta-mash.means)^2))/standard,bmalite=sqrt(mean((beta-bma.means)^2))/standard,ash=sqrt(mean((beta-ash.pm)^2))/standard)
barplot(as.numeric(rmse.all.table),main="Shared, Structured Effects",
        ylab="RRMSE",xlab="Method",col=c("green","blue","red"),names=colnames(rmse.all.table),ylim=c(0,0.6),cex.main=1.5,cex.lab=1,cex.names=1.5)
	
sum(lfsr.mash<0.05)
sum(lfsr.bma<0.05)
sum(ash.lfsr<0.05)

```

Show ROC curves:
```{r}
##now we generate curves
mash.power=NULL
bma.power=NULL
ash.power=NULL

mash.fp=NULL
bma.fp=NULL
ash.fp=NULL

sign.test.mash=as.matrix(c$beta)*mash.means
sign.test.bma=as.matrix(c$beta)*bma.means
sign.test.ash=as.matrix(c$beta)*ash.pm
lfsr.ash=as.matrix(ash.lfsr)

thresholds=seq(0.01,1,by=0.01)
beta=as.matrix(c$beta)
for(s in 1:length(thresholds)){
thresh=thresholds[s]

##sign power is the proportion of true effects correctly signed at a given threshold
mash.power[s]=sum(sign.test.mash>0&lfsr.mash<=thresh)/sum(beta!=0)
bma.power[s]=sum(sign.test.bma>0&lfsr.bma<=thresh)/sum(beta!=0)
ash.power[s]=sum(sign.test.ash>0&lfsr.ash<=thresh)/sum(beta!=0)


##false positives is the proportion of null effects called at a given threshold
mash.fp[s]=sum(beta==0&lfsr.mash<=thresh)/sum(beta==0)
bma.fp[s]=sum(beta==0&lfsr.bma<=thresh)/sum(beta==0)
ash.fp[s]=sum(beta==0&lfsr.ash<=thresh)/sum(beta==0)

}





plot(mash.fp,mash.power,cex=0.5,pch=1,xlim=c(0,1),lwd=1,ylim=c(0,1),col="green",ylab="True Positive Rate",xlab="False Positive Rate",type="l",main="")
#title("True Positive vs False Positive",cex.main=1.5)
lines(ash.fp,ash.power,cex=0.5,pch=1,ylim=c(0,1),col="red")
lines(bma.fp,bma.power,cex=0.5,pch=1,ylim=c(0,1),col="blue")
legend("bottomright",legend = c("mash","bmalite","ash"),col=c("green","blue","red"),pch=c(1,1,1))

```

Test to show our calculations are accurate:
```{r}
mash.means=as.matrix(read.table("simulationsLposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("simulationsLlfsr.txt"))[,-1]
j=10
arrays=post.array.per.snp.no.baseline(j = 10,covmat = edcov,b.gp.hat = w,se.gp.hat = se,L = L[-1,])
k=5

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

LSigL=L[-1,]%*%edcov[[k]]%*%t(L[-1,])
LVL=L[-1,]%*%V.gp.hat%*%t(L[-1,])

LSigL_list=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,])))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,]))))

log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("pissimulationsL.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))
```


---
title: "runningmashnobaseline"
output: html_document
---

In this document, we will simulate uncentered effects that are null; that is the uncentered averages are identical in all conditions (i.e., all $\mu$)

$$C = \mu_ {R}$$

where \mu_{R} is an R vector of identical $\mu$

Then the observed effects are independent, with uncorrelated residuals:
$$Chat = N(C, 1/2 I_{8})$$

thus they can be written as :

$$Chat = \mu + v + E$$

In our simulation, $v$ is 0, but $\mu$ is not.

However, we want to show that the matrix of contrasts in observed effects, $LChat$ is correlated inherently, and thus might yield false positives when used to estimate the underlying effects because of the inherently correlated residuals:

$$LChat = Chat_2-Chat_1, Chat_3-Chat_1 ... Chat_8-Chat_1$$

Where the matrix L subtracts the first entry from the other r=2 ... r=8 entires:

```{r}
system("ls; rm *rds;rm *txt; rm *pdf")
set.seed(123)
R=8;n=1000
mu_mat = t(sapply(seq(1:n),function(i){rep(rnorm(1),R)})) ##the same mu for all conditions
c = mu_mat ### the uncentered true effect is thus just mu
error = t(sapply(seq(1:n),function(i){rnorm(R,1/2)}))##the errors are uncorrelated
chat=c+error

````

Then, we produce the matrix of contrasts and associated standard errors (variances):
```{r}
lchat=chat[,2:8]-chat[,1]
dim(lchat)
##v.j is the variance of lchat
v.j=matrix(rep(1,ncol(chat)*nrow(chat)),ncol=ncol(chat),nrow=nrow(chat))[,-1]##because the variance of a difference is the sum of the variances
se=sqrt(v.j)
```

Now, we can run mash on this matrix of centered observations. Let's just use one component:


```{r}
library("ExtremeDeconvolution")
init.cov.list=list()
init.cov.list[[1]]=cov(lchat)
mean.mat=matrix(rep(0,ncol(lchat)*nrow(lchat)),ncol=ncol(lchat),nrow=nrow(lchat))  
ydata= lchat ##train on the max like estimates (same as using max ws)
xamp=1
xcovar=init.cov.list
fixmean=TRUE     
ycovar=v.j   
xmean=mean.mat   
projection=list();for(l in 1:nrow(lchat)){projection[[l]]=diag(1,R-1)}##no projection in mash because we feed in centered estimates
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)##notice no projection.

```

Now, use this estimate to run mash on the matrix of observed centered averages:


```{r}
library('mashr')

w=lchat
pis=1
covlist=e$xcovar
A="mashfp"
weightedquants=lapply(seq(1:nrow(w)),function(j){total.quant.per.snp(j = j,covmat = covlist,b.gp.hat=w,se.gp.hat = se,pis = pis,A=A,checkpoint = FALSE)})
a=post.mean.with.proj(b.mle = w[1,],tinv = solve(diag(v.j[1,])+covlist[[1]]),U.k = covlist[[1]],L=diag(1,R-1))

mashtruth=read.table("mashfpposterior.means.txt")[,-1]
all.equal(as.numeric(mashtruth[1,]),as.numeric(a))

```

Now, we will also simulate noisy observations that truly are independent:

$$LChat ~ N(0,I_{7})$$

Here, because the simulated 'contrasts' do not have correlated residuals, we show very few 'significant' residuals.

```{r}

lchat_unc=t(sapply(seq(1:n),function(i){(rnorm(R-1))}))
```

```{r}
library("ExtremeDeconvolution")
init.cov.list=list()
init.cov.list[[1]]=cov(lchat)
mean.mat=matrix(rep(0,ncol(lchat)*nrow(lchat)),ncol=ncol(lchat),nrow=nrow(lchat))  
ydata= lchat_unc ##train on the max like estimates (same as using max ws)
xamp=1
xcovar=init.cov.list
fixmean=TRUE     
ycovar=v.j   
xmean=mean.mat   
projection=list();for(l in 1:nrow(lchat)){projection[[l]]=diag(1,R-1)}##no projection in mash because we feed in centered estimates
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)##notice no projection.

```

Now, use this estimate to run mash on the matrix of observed centered averages:


```{r}
library('mashr')

w=lchat_unc
pis=1
covlist=e$xcovar
A="mash_trueInd"
weightedquants=lapply(seq(1:nrow(w)),function(j){total.quant.per.snp(j = j,covmat = covlist,b.gp.hat=w,se.gp.hat = se,pis = pis,A=A,checkpoint = FALSE)})
a=post.mean.with.proj(b.mle = w[1,],tinv = solve(diag(v.j[1,])+covlist[[1]]),U.k = covlist[[1]],L=diag(1,R-1))

mashtruth=read.table("mash_trueIndposterior.means.txt")[,-1]
all.equal(as.numeric(mashtruth[1,]),as.numeric(a))

```


We can show that the false positive produces more lfsr < 0.05:

```{r}

lfsr.one=read.table("mashfplfsr.txt")[,-1]
lfsr.two=read.table("mash_trueIndlfsr.txt")[,-1]

mean(lfsr.one<0.05)
mean(lfsr.two<0.05)

```

###
Now, we want to run MASH using the matrix of "true" residuals, which is

$$L 1/2 I_{8} L'$$

```{r}

##matrix of contrasts##
L=cbind(rep(-1,R-1),diag(1,7))
chatres=t(sapply(seq(1:n),function(n)(rep(sqrt(0.5),R))))
lvlarray=genlvllist(s.j = chatres,L = L )

dim(lvlarray[[1]])
```


We will run Bovy again, this time recognizing the correlated residuals:

```{r}
init.cov.list=list()
init.cov.list[[1]]=cov(lchat)
mean.mat=matrix(rep(0,ncol(lchat)*nrow(lchat)),ncol=ncol(lchat),nrow=nrow(lchat))  
ydata= lchat ##train on the max like estimates (same as using max ws)
xamp=1
xcovar=init.cov.list
fixmean=TRUE     
ycovar=lvlarray 
xmean=mean.mat   
projection=list();for(l in 1:nrow(lchat)){projection[[l]]=diag(1,R-1)}##no projection in mash because we feed in centered estimates
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)##notice no projection.
```

Now, we can use this estimate to compute the posteriors, recongizing the correlated residuals:

```{r}
weightedquants=lapply(seq(1:nrow(lchat)),function(j){
total.quant.per.snp.with.lvlmat(j = j,covmat = e$xcovar,b.gp.hat = lchat,lvlmat = lvlarray[[1]],pis = 1,A = "withlvlarray",checkpoint = F)})

mashlvl=read.table("withlvlarrayposterior.means.txt")[,-1]

rm(a)
a=post.mean.with.proj(b.mle = lchat[1,],tinv = solve(lvlarray[[1]]+e$xcovar[[1]]),U.k = e$xcovar[[1]],L=diag(1,R-1))

all.equal(as.numeric(mashlvl[1,]),as.numeric(a))

mashlvl.lfsr=read.table("withlvlarraylfsr.txt")[,-1]
mean(mashlvl.lfsr<0.05)
```

And so we have shown that:

1) We do indeed detect many false positives when we simply compare contrasts without taking into account the correlated residuals.

2) Supplying the matrix which recognizes the correlated residuals corrects quite well for this problem.

####
Let's try running regular mash:

```{r}

write.table(lchat,"lchat.txt",col.names = F,row.names = F)
system('/Users/sarahurbut/miniconda3/bin/sfa -gen lchat.txt -g 1000 -k 5 -n 7 i -o lchat')


factor.mat=as.matrix(read.table("lchat_F.out"))
lambda.mat=as.matrix(read.table("lchat_lambda.out"))

max.step=deconvolution.em.with.bovy(t.stat = lchat,factor.mat =factor.mat,lambda.mat=lambda.mat,v.j = se,K = 3,P = 3)

##Show how the ranks is preserved
apply(max.step$true.covs,1,function(x){qr(x)$rank})

covmash=compute.hm.covmat.all.max.step(b.hat = lchat,se.hat = se,t.stat = lchat,Q = 5,lambda.mat = lambda.mat,A = "allmash",factor.mat = factor.mat,max.step = max.step,zero = T,power = 2)$covmat


all.equal(covmash[[2]]/max(diag(covmash[[2]])),max.step$true.covs[1,,]/max(diag(max.step$true.covs[1,,])))
all.equal(covmash[[3]]/max(diag(covmash[[3]])),max.step$true.covs[3,,]/max(diag(max.step$true.covs[3,,])))
all.equal(covmash[[9]]/max(diag(covmash[[9]])),max.step$true.covs[2,,]/max(diag(max.step$true.covs[2,,])))

```

Now, proceed as always, estimating likelihood and computing posteriors.

```{r mashmethod, eval=T}
A="allmash"
compute.hm.train.log.lik.pen(train.b = lchat,se.train = se,covmat = covmash,A=A,pen=1)

pis=readRDS(paste0("pis",A,".rds"))$pihat
b.test=lchat
se.test=se
weightedquants=lapply(seq(1:nrow(lchat)),function(j){total.quant.per.snp(j,covmash,b.gp.hat=lchat,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})
```

Check to see if correct:

```{r}
j=1
means=read.table("allmashposterior.means.txt")[,-1]
arrays=post.array.per.snp(j = 1,covmat = covmash,b.gp.hat = lchat,se.gp.hat = se)

log.lik.snp=log.lik.func(lchat[1,],V.gp.hat = diag(se[1,])^2,covmat = covmash)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("pisallmash.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(means[j,]))
```

Now, let's see how many false positive we caught here;

```{r}
lfsrmash=read.table("allmashlfsr.txt")[,-1]
```

You can see we catch even more then when we use one component. Now, let's try full $mash$ with the correct projection matrix. We will run Bovy again, this time recognizing the correlated residuals and using multiple matrices.

```{r}

R=ncol(lchat);K=3;P=3
init.cov=init.covmat(t.stat=lchat,factor.mat = factor.mat,lambda.mat = lambda.mat,K=K,P=P)
init.cov.list=list()
for(i in 1:K){init.cov.list[[i]]=init.cov[i,,]}
mean.mat=matrix(rep(0,ncol(lchat)*nrow(lchat)),ncol=ncol(lchat),nrow=nrow(lchat))  
ydata= lchat ##train on the max like estimates (same as using max ws)
xamp=1
xcovar=init.cov.list
fixmean=TRUE     
ycovar=lvlarray 
xmean=mean.mat   
projection=list();for(l in 1:nrow(lchat)){projection[[l]]=diag(1,R-1)}##no projection in mash because we feed in centered estimates
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)##notice no projection.
true.covs=array(dim=c(K,R,R))
for(i in 1:K){true.covs[i,,]=e$xcovar[[i]]}
pi=e$xamp
max.step=list(true.covs=true.covs,pi=pi)
```



```{r}
###recall e is the output using lvl
covmash=compute.hm.covmat.all.max.step(b.hat = lchat,se.hat = se,t.stat = lchat,Q = 5,lambda.mat = lambda.mat,A = "allmash",factor.mat = factor.mat,max.step = max.step,zero = T,power = 2)$covmat


all.equal(covmash[[2]]/max(diag(covmash[[2]])),max.step$true.covs[1,,]/max(diag(max.step$true.covs[1,,])))
all.equal(covmash[[3]]/max(diag(covmash[[3]])),max.step$true.covs[3,,]/max(diag(max.step$true.covs[3,,])))
all.equal(covmash[[9]]/max(diag(covmash[[9]])),max.step$true.covs[2,,]/max(diag(max.step$true.covs[2,,])))

```

Now, proceed as always, estimating likelihood and computing posteriors.

```{r, eval=T}
A="allmashlvl"
compute.hm.train.log.lik.pen.lvlmat(train.b = lchat,lvlmat = lvlarray[[1]],covmat = covmash,A=A,pen=1)

pis=readRDS(paste0("pis",A,".rds"))$pihat
b.test=lchat
se.test=se
weightedquants=lapply(seq(1:nrow(lchat)),function(j){total.quant.per.snp.with.lvlmat(j,covmash,b.gp.hat=lchat,lvlmat = lvlarray[[1]],pis,A=A,checkpoint = FALSE)})
```

Check to see if correct:

```{r}
means=read.table("allmashlvlposterior.means.txt")[,-1]
lfsrlvl=read.table("allmashlvllfsr.txt")[,-1]
arrays=post.array.per.snp.with.lvlmat(j = 1,covmat = covmash,b.gp.hat = lchat,lvlmat = lvlarray[[1]])

log.lik.snp=log.lik.func(lchat[1,],V.gp.hat = lvlarray[[1]],covmat = covmash)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("pisallmashlvl.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(means[j,]))
mean(lfsrlvl<0.05)

```



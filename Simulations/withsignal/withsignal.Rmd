---
title: "Simulating With Some Signal"
output: html_document
---

Now, let's simulate with some signal involved;

```{r echo=FALSE}
library("knitr")
knitr::opts_chunk$set(cache=TRUE)
rm(list=ls())
library('mashr')
library("ExtremeDeconvolution")
library("MASS")
knitr::opts_chunk$set(cache=TRUE)
#system("ls; rm *rds;rm *txt; rm *pdf")
opts_chunk$set(fig.path = "/Users/sarahurbut/Dropbox/PhDThesis/Figures/") 
```


First we will simulate with some signal, according to three different patterns of covariance. Here, we will have the first 100 genes have 'true' signal, according to MS suggestions to have some signal and some noise.

For the null effects, the centered 'true' averages are 0 - that is the uncentered true observations are identical in all conditions, and the noisy centered observations $Chat$ result only from the difference in error between the baseline condition. Thus for these conditions:

$$C = \mu_ {R}$$

where \mu_{R} is an R vector of identical $\mu$

Then the observed estimates are independent, with uncorrelated residuals:
$$Chat = N(C, 1/2 I_{8})$$

thus they can be written as :

$$Chat = \mu + v + E$$ 

where for the null effects, $v_{2...R} is 0.$

For the non-null effects, $v_{2...R}$ is simulated according to one of three MVN N(0,U_{k})


We then use the observations $$LChat = Chat_2-Chat_1, Chat_3-Chat_1 ... Chat_8-Chat_1$$ to estimate v, that is $$LChat ~ N(LC,LVL')$$

Where the matrix L subtracts the first entry from the other r=2 ... r=8 entires:

The question is, when I simulate these real effects, am I simulating v or Lv? What is the dimension of each? If I am simulating v, then shouldn't Lchat be N(0,LUL' + LVL')?


```{r}
set.seed(123)
R=8;n=1000;K=3;J=0.10*n;betasd=1
mu_mat = t(sapply(seq(1:n),function(i){rep(rnorm(1),R)})) ##the same mu for all conditions
d=R-1
F=t(sapply(seq(1:K),function(x){rnorm(d,mean=0,sd=betasd)}))###simulate random factors
covmat=lapply(seq(1:K),function(f){t=as.matrix((F[f,]))%*%F[f,]})
possible_loadings = diag(K) #set possible loadings to be "sparse" (loaded on one factor each)
z = sample(nrow(F),J,replace=TRUE)
beta=matrix(rep(0,n*R),ncol=R,nrow=n)
for(j in 1:J){
    k=z[j]
    beta[j,c(2:R)]=mvrnorm(1,mu=rep(0,d),Sigma=covmat[[k]])###deviations only simulated for non-controls
    }
c=mu_mat+beta
error = t(sapply(seq(1:n),function(i){rnorm(R,1/2)}))##the errors are uncorrelated
chat=c+error
lchat=chat[,c(2:8)]-chat[,1]

v.j=matrix(rep(1,ncol(chat)*nrow(chat)),ncol=ncol(chat),nrow=nrow(chat))[,-1]##because the variance of a difference is the sum of the variances
se=sqrt(v.j)
```

Now again, let's test if recognizing the correlated residuals with multiple matrices (as in the full mash framework) is helpful.

####Let's try running regular mash:

```{r}

write.table(lchat,"lchatwitheffect.txt",col.names = FALSE,row.names=FALSE)
system('/Users/sarahurbut/miniconda3/bin/sfa -gen lchatwitheffect.txt -g 1000 -k 5 -n 7 i -o lchatwithE')


factor.mat=as.matrix(read.table("lchatwithE_F.out"))
lambda.mat=as.matrix(read.table("lchatwithE_lambda.out"))

max.step=deconvolution.em.with.bovy(t.stat = lchat,factor.mat =factor.mat,lambda.mat=lambda.mat,v.j = se,K = 3,P = 3)




##Show how the ranks is preserved
apply(max.step$true.covs,1,function(x){qr(x)$rank})

covmash=compute.hm.covmat.all.max.step(b.hat = lchat,se.hat = se,t.stat = lchat,Q = 5,lambda.mat = lambda.mat,A = "allmash",factor.mat = factor.mat,max.step = max.step,zero = T,power = 2)$covmat


all.equal(covmash[[2]]/max(diag(covmash[[2]])),max.step$true.covs[1,,]/max(diag(max.step$true.covs[1,,])))
all.equal(covmash[[3]]/max(diag(covmash[[3]])),max.step$true.covs[3,,]/max(diag(max.step$true.covs[3,,])))
all.equal(covmash[[9]]/max(diag(covmash[[9]])),max.step$true.covs[2,,]/max(diag(max.step$true.covs[2,,])))

```

Now, proceed as always, estimating likelihood and computing posteriors.

```{r mashmethod, eval=T}
A="allmash"
compute.hm.train.log.lik.pen(train.b = lchat,se.train = se,covmat = covmash,A=A,pen=1)

pis=readRDS(paste0("pis",A,".rds"))$pihat
b.test=lchat
se.test=se
weightedquants=lapply(seq(1:nrow(lchat)),function(j){total.quant.per.snp(j,covmash,b.gp.hat=lchat,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})
```

Check to see if correct:

```{r}
j=1
means=read.table("allmashposterior.means.txt")[,-1]
arrays=post.array.per.snp(j = 1,covmat = covmash,b.gp.hat = lchat,se.gp.hat = se)

log.lik.snp=log.lik.func(lchat[1,],V.gp.hat = diag(se[1,])^2,covmat = covmash)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("pisallmash.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(means[j,]))
```

Now, let's see how many false positive we caught here;

```{r}
lfsrmash=read.table("allmashlfsr.txt")[,-1]
sum(lfsrmash[J:n,]<0.05)
```

You can see we catch even more then when we use one component. Now, let's try full $mash$ with the correct projection matrix. We will run Bovy again, this time recognizing the correlated residuals and using multiple matrices.

```{r}
L=cbind(rep(-1,R-1),diag(1,7))
chatres=t(sapply(seq(1:n),function(n)(rep(sqrt(0.5),R))))
lvlarray=genlvllist(s.j = chatres,L = L )
#lvlarray=genlvlarray(s.j=chatres,L=L)

R=ncol(lchat);K=3;P=3
init.cov=init.covmat(t.stat=lchat,factor.mat = factor.mat,lambda.mat = lambda.mat,K=K,P=P)
init.cov.list=list()
for(i in 1:K){init.cov.list[[i]]=init.cov[i,,]}
mean.mat=matrix(rep(0,ncol(lchat)*nrow(lchat)),ncol=ncol(lchat),nrow=nrow(lchat))  
ydata= lchat ##train on the max like estimates (same as using max ws)
xamp=rep(1/K,K)

xcovar=init.cov.list
fixmean=TRUE     
ycovar=lvlarray 
xmean=mean.mat   
projection=list();for(l in 1:nrow(lchat)){projection[[l]]=diag(1,R)}##no projection in mash because we feed in centered estimates
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)##notice no projection.
true.covs=array(dim=c(K,R,R))
for(i in 1:K){true.covs[i,,]=e$xcovar[[i]]}
pi=e$xamp
max.step=list(true.covs=true.covs,pi=pi)

max.step=deconvolution.em.with.bovy(t.stat = lchat,factor.mat = factor.mat,v.j = lvlarray,lambda.mat = lambda.mat,K = 3,P = 3)

sapply(seq(1:3),function(k){all.equal(as.numeric(max.step$true.covs[k,,]),as.numeric(e$xcovar[[k]]))})
```



```{r}
###recall e is the output using lvl
covmash=compute.hm.covmat.all.max.step(b.hat = lchat,se.hat = se,t.stat = lchat,Q = 5,lambda.mat = lambda.mat,A = "allmash",factor.mat = factor.mat,max.step = max.step,zero = T,power = 2)$covmat


all.equal(covmash[[2]]/max(diag(covmash[[2]])),max.step$true.covs[1,,]/max(diag(max.step$true.covs[1,,])))
all.equal(covmash[[3]]/max(diag(covmash[[3]])),max.step$true.covs[3,,]/max(diag(max.step$true.covs[3,,])))
all.equal(covmash[[9]]/max(diag(covmash[[9]])),max.step$true.covs[2,,]/max(diag(max.step$true.covs[2,,])))

```

Now, proceed as always, estimating likelihood and computing posteriors.

```{r, eval=T}
A="allmashlvl"
compute.hm.train.log.lik.pen.lvlmat(train.b = lchat,lvlmat = lvlarray[[1]],covmat = covmash,A=A,pen=1)

pis=readRDS(paste0("pis",A,".rds"))$pihat
b.test=lchat
se.test=se
weightedquants=lapply(seq(1:nrow(lchat)),function(j){total.quant.per.snp.with.lvlmat(j,covmash,b.gp.hat=lchat,lvlmat = lvlarray[[1]],pis,A=A,checkpoint = FALSE)})
```

Check to see if correct:

```{r}
means=read.table("allmashlvlposterior.means.txt")[,-1]
lfsrlvl=read.table("allmashlvllfsr.txt")[,-1]
arrays=post.array.per.snp.with.lvlmat(j = 1,covmat = covmash,b.gp.hat = lchat,lvlmat = lvlarray[[1]])

log.lik.snp=log.lik.func(lchat[1,],V.gp.hat = lvlarray[[1]],covmat = covmash)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("pisallmashlvl.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(means[j,]))

sum(lfsrlvl[J:n,]<0.05)
sum(lfsrmash[J:n,]<0.05)

sum(lfsrlvl[1:J,]<0.05)
sum(lfsrmash[1:J,]<0.05)
beta=beta[,-1]

thresholds=seq(0,0.5,by=0.01)

mashid.power=mashlv.power=mashid.fp=mashlv.fp=NULL
for(s in seq(1:length(thresholds))){

thresh=thresholds[s]
##call power is the proportion of true effects called at a given threshold
mashid.power[s]=sum(beta!=0&lfsrmash<=thresh)/sum(beta!=0)
mashlv.power[s]=sum(beta!=0&lfsrlvl<=thresh)/sum(beta!=0)


##false positives is the proportion of null effects called at a given threshold
mashid.fp[s]=sum(beta==0&lfsrmash<=thresh)/sum(beta==0)
mashlv.fp[s]=sum(beta==0&lfsrlvl<=thresh)/sum(beta==0)}


plot(mashid.fp,mashid.power,cex=0.5,pch=1,xlim=c(0,0.5),lwd=1,ylim=c(0,0.8),
     col="red",ylab="True Positive Rate",xlab="False Positive Rate",type="l",main="")
title("True Positive vs False Positive",cex.main=1.5)
lines(mashlv.fp,mashlv.power,cex=0.5,pch=1,ylim=c(0,1),col="green")
legend("bottomright",legend = c("mashLVL","mashId"),col=c("green","red"),pch=c(1,1))



```



We can also compute RMSE:

```{r}
withresmreans=as.matrix(read.table("allmashlvlposterior.means.txt")[,-1])
noresmeans=as.matrix(read.table("allmashposterior.means.txt")[,-1])

b=as.matrix(beta)

sqrt(mean((withresmreans-b)^2))
sqrt(mean((noresmeans-b)^2))
```

And you can see we do much better in terms of RMSE.

```{r rmseplotsignal}
(standard=sqrt(mean((lchat-b)^2)))
mnb.rmse=sqrt(mean((withresmreans-b)^2))/standard
ind.rmse=sqrt(mean((noresmeans-b)^2))/standard


rmse.all.table=data.frame(mnb=mnb.rmse,ind=ind.rmse)
barplot(as.numeric(rmse.all.table),main
        ="RRMSE from Non-Null Simulation",
        ylab="relative error (RRMSE)",xlab="Method",col=c("green","red"),names=colnames(rmse.all.table),ylim=c(0,1),cex.main=1.5,cex.lab=1,cex.names=1.5)


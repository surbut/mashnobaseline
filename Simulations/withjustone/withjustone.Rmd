---
title: "withjustone"
output: html_document
---

```{r setup, include=FALSE}
library('knitr')
knitr::opts_chunk$set(cache=TRUE)
```


Let's simulate data with one covariance matrix, and $\omega$ fixed at 2. We will also use only one $\omega$ in our inference matrices. The purpose of this document was to test the ability of `mash` and `mashnobaseline` to infer the truth when there was only one $\omega$ to complicate things, so that the 'shrinkage' was not an issue. You can see the inferred covariance matrices are shorter.

We can have a look at the them first:

```{r}
rm(list=ls())
library('mash')
set.seed(123)
c=chat_sim_fsingle_fixedomega(n = 10000,d = 8,omega = 2,esd = 0.1)
saveRDS(c,"chatfixedomega.rds")
diag(c$covmat)

```


And here we run our analysis:
```{r}

t=c$t;b=c$chat;se=c$shat
#hist(t)
#hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se

absmat=abs(t(L%*%t(c$t)))
index=which(rowSums(absmat>4)>0)
length(index)
mean(index<1000)

#index=which(rowMeans(abs(t(L%*%t(c$t))))>1.3)##choose the associations with average deviation large.

##show deviations
plot(rowSums(absmat>4))
points(index,rowSums(absmat>4)[index],col="blue")

sparsestrongt=t(L%*%t(t[index,]))

```

```{r filewrite,eval=T}

write.table(sparsestrongt,"sparsestrongt.txt",col.names = FALSE,row.names=FALSE)

```


Now, we need to project into the centered space to estimate the covariance matrix of the true deviations, using the full L since $v$ will be R, and not $R-1$.


```{r}
system('/Users/sarahurbut/miniconda3/bin/sfa -gen sparsestrongt.txt -g 748 -k 5 -n 8 i -o sparseF')
A="sparseF"

factor.mat=as.matrix(read.table("sparseF_F.out"))
lambda.mat=as.matrix(read.table("sparseF_lambda.out"))

#recall here that w will now be the L[-1,]%*%t(t[strong,]), which is equivalent to removing the first column of the strong projected t. Thus the covariance of v is initated with the RxR matrices (strong projected t), and the model is trained on the strong projected t less the first column because the model has v_{rxr}|w_{r,r-1}

strongprojectedt=sparsestrongt


lvllist=genlvllist(s.j[index,],L = L[-1,])


##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR so we can have the full set of deviations and thier standard errors to choose grid
dim(wfull)
dim(sjmat)

```

```{r}
w=data.frame(wfull)[,-1]
```


Now run with ED to show that 
1) the likelihood improves
2) we have successfully implemented an approach which allows us to input a list of residual variance matrices (as opposed to a matrix of vectors to be diagonalised).
3) Let's just do it with one covariance matrix from ED


```{r}
source("~/matrix_ash/R/mashnobasescripts.R")
source("~/matrix_ash/R/truthscripts.R")
rm(A)

A="withoneK"


library("ExtremeDeconvolution")
efunc=deconvolution.em.with.bovy.with.L.oneK(t.stat = strongprojectedt,v.j = lvllist,L = L[-1,],w = strongprojectedt[,-1])

dim(efunc$true.covs)
length(efunc$pi)

efunc$true.covs[1,,]-min(abs(efunc$true.covs[1,,]))

##compare to initialized matrix
X.real=as.matrix(strongprojectedt)
X.c=apply(X.real,2,function(x) x-mean(x)) ##Column centered matrix of t statistics
data.prox=((t(X.c)%*%X.c))/(nrow(X.real)-1)
(data.prox)-min(abs(data.prox))
```

Now let's create a list of covariance matrices just as we do with the truth comparions, containing the identity, the one matrix, and zero:

```{r}
covlist=list();covlist[[1]]=efunc$true.covs[1,,]
edcov=compute.covmat.using.oneK.fixed.omega(covlist = covlist,A = A,omega = 2,zero = TRUE)

length(edcov)
sapply(edcov,function(x){max(diag(x))})

all.equal(edcov[[2]]/max(diag(edcov[[2]])),efunc$true.covs[1,,]/max(diag(efunc$true.covs[1,,])))

barplot(diag(edcov[[2]]))
```

Now let's compute the likelihood with ED:

```{r eval=T}
w=data.frame(wfull)[,-1]

compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = edcov,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))

for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = edcov,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```

##Test to show our calculations are accurate:
```{r,eval=T,echo=T}
mash.means=as.matrix(read.table("withoneKposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("withoneKlfsr.txt"))[,-1]
edcov=readRDS("covmatwithoneK.rds")
j=10
arrays=post.array.per.snp.no.baseline(j = 10,covmat = edcov,b.gp.hat = w,se.gp.hat = se,L = L[-1,])
k=2

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

LSigL=L[-1,]%*%edcov[[k]]%*%t(L[-1,])
LVL=L[-1,]%*%V.gp.hat%*%t(L[-1,])

LSigL_list=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,])))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,]))))

log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("piswithoneK.rds")$pihat
barplot(pis)
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))
```




Let's compare with truth:

##Comparing with truth.

Now, let's try this with the true prior. Here, we use the covariance matrices that were used to simulate the data as the output of _c$covmat_


```{r}

rm(list=ls())
library('mash')

c=readRDS("chatfixedomega.rds")
diag(c$covmat)

t=c$t;b=c$chat;se=c$shat
covl=list();covl[[1]]=c$covmat
#hist(t)
#hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is 
A="withfixedomega"
covs=compute.covmat.using.truth.fixed.omega(covlist = covl,A = "withfixedomega",omega = 2,zero = TRUE)
length(covs)
barplot(diag(covs[[2]])) 
edcov=readRDS("covmatwithoneK.rds")
barplot(diag(edcov[[2]]))
w=data.frame(wfull)[,-1]
```

```{r,eval=T}
A="withfixedomegamnb"
compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = covs,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))


for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = covs,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```


##Test to show our calculations are accurate:
```{r,eval=T,echo=T}
mash.means=as.matrix(read.table("withfixedomegamnbposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("withfixedomegamnblfsr.txt"))[,-1]
edcov=readRDS("covmatwithfixedomega.rds")
j=10
arrays=post.array.per.snp.no.baseline(j = 10,covmat = edcov,b.gp.hat = w,se.gp.hat = se,L = L[-1,])
k=2

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

LSigL=L[-1,]%*%edcov[[k]]%*%t(L[-1,])
LVL=L[-1,]%*%V.gp.hat%*%t(L[-1,])

LSigL_list=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,])))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,]))))

log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("piswithfixedomegamnb.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))
```

And let's also do this with the mash framework:

```{r}
rm(list=ls())
c=readRDS("chatfixedomega.rds")
covl=list();covl[[1]]=c$covmat
t=c$t;b=c$chat;se=c$shat

R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is 
```

```{r mashtruth,eval=T}
A="mashtruthwithfixedomega"
covs=readRDS("covmatwithfixedomega.rds")
length(covs)
compute.hm.train.log.lik.pen(train.b = wfull,se.train = se,covmat = covs,A=A,pen=1)


pis=readRDS(paste0("pis",A,".rds"))$pihat


weightedquants=lapply(seq(1:nrow(wfull)),function(j){total.quant.per.snp(j,covs,b.gp.hat=wfull,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})

```

And test to see if calculations are accurate and show that likelihood with MNB is much better than with mash:


```{r,eval=T,echo=T}
cov=readRDS("covmatwithfixedomega.rds")
pis=readRDS(paste0("pis",A,".rds"))$pihat
likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))

mash.means=as.matrix(read.table("mashtruthwithfixedomegaposterior.means.txt")[,-1])
w=data.frame(wfull)
R=ncol(w)
j=10

arrays=post.array.per.snp.no.baseline(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se,L = diag(1,R))
mash.arrays=post.array.per.snp(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se)
##show same algebra


k=2

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

lapply(seq(1:5),function(x){all.equal(arrays[[x]],mash.arrays[[x]])})
L=diag(1,R)
LSigL=L%*%cov[[k]]%*%t(L)
LVL=L%*%V.gp.hat%*%t(L)

LSigL_list=lapply(cov,function(x){L%*%x%*%t(L)})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L)))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L))))




b.mle=as.vector(t(w[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
U.gp1kl= (post.b.gpkl.cov(V.gp.hat.inv, cov[[k]]))
  
identical(as.numeric(mash.arrays$post.means[k,]),as.numeric(post.b.gpkl.mean(b.mle = b.mle,V.gp.hat.inv = V.gp.hat.inv,U.gp1kl = U.gp1kl)))

identical(as.numeric(mash.arrays$post.covs[k,]),as.numeric(diag(U.gp1kl)))


pis=readRDS("pismashtruthwithfixedomega.rds")$pihat
log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))


mashnobase.means=as.matrix(read.table("withfixedomegamnbposterior.means.txt")[,-1])

beta=as.matrix(c$beta)
mle=as.matrix(wfull)
mash.means=as.matrix(mash.means)
mashnobaseoneK=as.matrix(read.table("withoneKposterior.means.txt"))[,-1]
standard=sqrt(mean((beta-mle)^2))
sqrt(mean((mash.means-beta)^2))/standard
sqrt(mean((mashnobaseoneK-beta)^2))/standard
sqrt(mean((mashnobase.means-beta)^2))/standard
```

Let's compare the learned matrices which receive the most weighting in mash and mashnobaseline. First, let' look at the dsitribution of matrices.

Let's do the same thing comparing the pis using mash from the truth and from mashnobaseline:

```{r}
pimash=readRDS("pismashtruthwithfixedomega.rds")$pihat
pied=readRDS("piswithfixedomegamnb.rds")$pihat
piedoneK=readRDS("piswithoneK.rds")$pihat
par(mfrow=c(1,2))
barplot(colSums(matrix(pied[-length(pied)],ncol=2,byrow=TRUE)),names=c("ID","Uk1"),las=2,main="Mashnobase")
barplot(colSums(matrix(pimash[-length(pied)],ncol=2,byrow=TRUE)),names=c("ID","Uk1"),las=2,main="Mash")
barplot(colSums(matrix(piedoneK[-length(piedoneK)],ncol=2,byrow=TRUE)),names=c("ID","Uk1"),las=2,main="piedoneK")
```

We can see that using the same set of covariance matrices, _mashnobaseline_ is better able to emphasize the true configurations because the likelihood and posterior computations incorporate L. Recall, for mash:

$$w_{j,R} ~ N(0,V+U)$$
$$w_{j,R-1} ~ N(0,LVL' + LUL')$$ where L is the R-1 x R centering matrix.

In both:

$$c = \mu + \beta$$
$$chat = c + E$$
$$v \sim \sum_p N(0,Uk)$$
$$E \sim N(0,V)$$

So even if the Uks are different, the likelihood and corresponding $\pi$s will be different.

What's strange is that running mashnobaseline with one learned Uk is really no btter than mash with truth, but much worse than mashnobaseline with the truth.

###And let's also do this with the mash framework using that one K:

```{r}
rm(list=ls())
c=readRDS("chatfixedomega.rds")
t=c$t;b=c$chat;se=c$shat

R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is 
```

```{r mashtruthwithonek,eval=T}
covs=readRDS("covmatwithoneK.rds")
length(covs)
rm(A)
A="withoneKmash"
compute.hm.train.log.lik.pen(train.b = wfull,se.train = se,covmat = covs,A=A,pen=1)


pis=readRDS(paste0("pis",A,".rds"))$pihat


weightedquants=lapply(seq(1:nrow(wfull)),function(j){total.quant.per.snp(j,covs,b.gp.hat=wfull,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})

```

And test to see if calculations are accurate:


```{r,eval=T,echo=T}
cov=readRDS("covmatwithoneK.rds")
mash.means=as.matrix(read.table("withoneKmashposterior.means.txt")[,-1])
w=data.frame(wfull)
R=ncol(w)
j=10

arrays=post.array.per.snp.no.baseline(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se,L = diag(1,R))
mash.arrays=post.array.per.snp(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se)
##show same algebra


k=2

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

lapply(seq(1:5),function(x){all.equal(arrays[[x]],mash.arrays[[x]])})
L=diag(1,R)
LSigL=L%*%cov[[k]]%*%t(L)
LVL=L%*%V.gp.hat%*%t(L)

LSigL_list=lapply(cov,function(x){L%*%x%*%t(L)})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L)))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L))))




b.mle=as.vector(t(w[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
U.gp1kl= (post.b.gpkl.cov(V.gp.hat.inv, cov[[k]]))
  
identical(as.numeric(mash.arrays$post.means[k,]),as.numeric(post.b.gpkl.mean(b.mle = b.mle,V.gp.hat.inv = V.gp.hat.inv,U.gp1kl = U.gp1kl)))

identical(as.numeric(mash.arrays$post.covs[k,]),as.numeric(diag(U.gp1kl)))



beta=as.matrix(c$beta)
mle=as.matrix(wfull)
mashoneK=as.matrix(read.table("withoneKmashposterior.means.txt"))[,-1]
standard=sqrt(mean((beta-mle)^2))
sqrt(mean((mashoneK-beta)^2))/standard

##let's test mash likelihood###

A="withoneKmash"
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))

A="mashtruthwithfixedomega"
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))

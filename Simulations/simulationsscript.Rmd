---
title: "Testscriptsimulations"
output: html_document
---

```{r setup, include=FALSE}
library('knitr')
knitr::opts_chunk$set(cache=TRUE)
```


```{r}

library('mash')
set.seed(123)
c=chat_sim(n = 10000,d = 20,betasd = 1,esd = .1,K = 10)
t=c$t;b=c$chat;se=c$shat
hist(t)
hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se

absmat=abs(t(L%*%t(c$t)))
index=which(rowSums(absmat>4)>0)
length(index)
mean(index<1000)
#plot(which(rowSums(absmat>4))>0)
#index=which(rowMeans(abs(t(L%*%t(c$t))))>1.3)##choose the associations with average deviation large.

##show average deviations
plot((rowMeans(abs(t(L%*%t(c$t))))))
points(index,(rowMeans(abs(t(L%*%t(c$t)))))[index],col="red")
strongprojectedtsimulations=t(L%*%t(t[index,]))



write.table(strongprojectedtsimulations,"strongprojectedtsimulations.txt",col.names = FALSE,row.names=FALSE)

```


Now, we need to project into the centered space to estimate the covariance matrix of the true deviations, using the full L since $v$ will be R, and not $R-1$.


```{r}
system('/Users/sarahurbut/miniconda3/bin/sfa -gen strongprojectedtsimulations.txt -g 976 -k 5 -n 20 i -o simulationsL')
A="simulationsL"

factor.mat=as.matrix(read.table("simulationsL_F.out"))
lambda.mat=as.matrix(read.table("simulationsL_lambda.out"))

#recall here that w will now be the L[-1,]%*%t(t[strong,]), which is equivalent to removing the first column of the strong projected t. Thus the covariance of v is initated with the RxR matrices (strong projected t), and the model is trained on the strong projected t less the first column because the model has v_{rxr}|w_{r,r-1}

strongprojectedt=strongprojectedtsimulations


lvllist=genlvllist(s.j[index,],L = L[-1,])
library("ExtremeDeconvolution")
efunc=deconvolution.em.with.bovy.with.L(t.stat = strongprojectedt,factor.mat = factor.mat,lambda.mat = lambda.mat,v.j = lvllist,P = 3,L = L[-1,],Q = 5,w = strongprojectedt[,-1])


##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR so we can have the full set of deviations and thier standard errors to choose grid
dim(wfull)
dim(sjmat)

# library("ashr")
# ash.pm=matrix(NA,ncol=dim(wfull)[2],nrow=dim(wfull)[1])
# ash.lfsr=matrix(NA,ncol=dim(wfull)[2],nrow=dim(wfull)[1])
# for(i in 1:ncol(wfull)){
# 
# a=ash(betahat=wfull[,1],sebetahat=sjmat[,1],mixcompdist="normal")
# ash.pm[,i]=a$PosteriorMean
# ash.lfsr[,i]=a$lfsr
# }

A="simulationsL"

max.step=efunc
edcov=compute.hm.covmat.all.max.step(b.hat = wfull,se.hat = sjmat,t.stat = strongprojectedt,Q = 5,lambda.mat = lambda.mat,A=A,factor.mat = factor.mat,max.step = efunc,zero = T,power = 2)$cov
length(edcov)
```

Now, we switch to the projected (one lost rank) case:

```{r}
w=data.frame(wfull)[,-1]

compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = edcov,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat


for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = edcov,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```
We can also compare our results to what we would get using the eqtlbma configs:

```{r}

rm(A)
A = "withbmaL"
covbma=compute.covmat.with.heterogeneity.no.shrink(b.gp.hat = wfull,sebetahat = sjmat,A = A,zero = T,power = 2 )


compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = covbma,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat


for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = covbma,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```

Compare RMSE and power:

```{r}
bma.means=as.matrix(read.table("withbmaLposterior.means.txt")[,-1])
lfsr.bma=as.matrix(read.table("withbmaLlfsr.txt"))[,-1]
mash.means=as.matrix(read.table("simulationsLposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("simulationsLlfsr.txt"))[,-1]
mle=as.matrix(wfull)
beta=as.matrix(c$beta)
sqrt(mean(bma.means-beta)^2)
sqrt(mean(mash.means-beta)^2)
#sqrt(mean(ash.pm-beta)^2)
sqrt(mean(mle-beta)^2)

sum(lfsr.mash<0.05)
sum(lfsr.bma<0.05)
#sum(ash.lfsr<0.05)

```

Show ROC curves:
```{r}
mash.power=NULL
bma.power=NULL


mash.fp=NULL
bma.fp=NULL


sign.test.mash=as.matrix(c$beta)*mash.means
sign.test.bma=as.matrix(c$beta)*bma.means

thresholds=seq(0.01,1,by=0.01)
beta=as.matrix(c$beta)
for(s in 1:length(thresholds)){
thresh=thresholds[s]

##sign power is the proportion of true effects correctly signed at a given threshold
mash.power[s]=sum(sign.test.mash>0&lfsr.mash<=thresh)/sum(beta!=0)
bma.power[s]=sum(sign.test.bma>0&lfsr.bma<=thresh)/sum(beta!=0)



##false positives is the proportion of null effects called at a given threshold
mash.fp[s]=sum(beta==0&lfsr.mash<=thresh)/sum(beta==0)
bma.fp[s]=sum(beta==0&lfsr.bma<=thresh)/sum(beta==0)

}





plot(mash.fp,mash.power,cex=0.5,pch=1,xlim=c(0,0.2),lwd=1,ylim=c(0,1),col="green",ylab="True Positive Rate",xlab="False Positive Rate",type="l",main="")
#title("True Positive vs False Positive",cex.main=1.5)
#lines(ash.fp,ash.power,cex=0.5,pch=1,ylim=c(0,1),col="red")
lines(bma.fp,bma.power,cex=0.5,pch=1,ylim=c(0,1),col="blue")
legend("bottomright",legend = c("mash","bmalite","ash"),col=c("green","blue","red"),pch=c(1,1,1))

```

Test to show our calculations are accurate:
```{r}
j=10
arrays=post.array.per.snp.no.baseline(j = 10,covmat = edcov,b.gp.hat = w,se.gp.hat = se,L = L[-1,])
k=5

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

LSigL=L[-1,]%*%edcov[[k]]%*%t(L[-1,])
LVL=L[-1,]%*%V.gp.hat%*%t(L[-1,])

LSigL_list=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,])))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,]))))

log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("pissimulationsL.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))
```


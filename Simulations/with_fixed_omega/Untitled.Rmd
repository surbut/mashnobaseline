---
title: "withtruthandbigomega"
output: html_document
---

##Comparing with truth.

```{r setup, include=FALSE}
library('knitr')
knitr::opts_chunk$set(cache=TRUE)
```

Now, let's try this with the true prior. Here, we use the covariance matrices that were used to simulate the data as the output of _c$covmat_

We can have a look at the them first:

```{r}
rm(list=ls())
library('mash')
set.seed(123)
c=chat_sim_fsingle_fixedomega(n = 10000,d = 8,omega = 2,esd = 0.1)
saveRDS(c,"chatfixedomega.rds")
diag(c$covmat)

```


```{r}
t=c$t;b=c$chat;se=c$shat
covl=list();covl[[1]]=c$covmat
#hist(t)
#hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is 
A="withfixedomega"
covs=compute.covmat.using.truth.fixed.omega(covlist = covl,A = "withfixedomega",omega = 2,zero = TRUE)
length(covs)
barplot(diag(covs[[2]]/max(diag(covs[[2]]))))
w=data.frame(wfull)[,-1]
```

```{r,eval=T}
A="withfixedomegamnb"
compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = covs,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))


for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = covs,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```


##Test to show our calculations are accurate:
```{r,eval=T,echo=T}
mash.means=as.matrix(read.table("withfixedomegamnbposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("withfixedomegamnblfsr.txt"))[,-1]
edcov=readRDS("covmatwithfixedomega.rds")
j=10
arrays=post.array.per.snp.no.baseline(j = 10,covmat = edcov,b.gp.hat = w,se.gp.hat = se,L = L[-1,])
k=5

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

LSigL=L[-1,]%*%edcov[[k]]%*%t(L[-1,])
LVL=L[-1,]%*%V.gp.hat%*%t(L[-1,])

LSigL_list=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,])))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,]))))

log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("pissparseLwithtruth.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))
```

And let's also do this with the mash framework:

```{r}
rm(list=ls())
c=readRDS("chatsimesparsef.rds")
covl=list();covl[[1]]=c$covmat
t=c$t;b=c$chat;se=c$shat

R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is 
covs=compute.covmat.using.truth(b.gp.hat = wfull,sebetahat = sjmat,A="test",zero=TRUE,covlist=covl,power=2)

```

```{r mashtruth,eval=T}
A="mashtruthwithfixedomega"
covs=readRDS("covmatwithfixedomega.rds")
compute.hm.train.log.lik.pen(train.b = wfull,se.train = se,covmat = covs,A=A,pen=1)


pis=readRDS(paste0("pis",A,".rds"))$pihat


weightedquants=lapply(seq(1:nrow(wfull)),function(j){total.quant.per.snp(j,covs,b.gp.hat=wfull,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})

```

And test to see if calculations are accurate:


```{r,eval=T,echo=T}
cov=readRDS("covmatwithfixedomega.rds")
mash.means=as.matrix(read.table("mashtruthwithfixedomegaposterior.means.txt")[,-1])
w=data.frame(wfull)
R=ncol(w)
j=10

arrays=post.array.per.snp.no.baseline(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se,L = diag(1,R))
mash.arrays=post.array.per.snp(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se)
##show same algebra


k=5

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

lapply(seq(1:5),function(x){all.equal(arrays[[x]],mash.arrays[[x]])})
L=diag(1,R)
LSigL=L%*%cov[[k]]%*%t(L)
LVL=L%*%V.gp.hat%*%t(L)

LSigL_list=lapply(cov,function(x){L%*%x%*%t(L)})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L)))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L))))




b.mle=as.vector(t(w[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
U.gp1kl= (post.b.gpkl.cov(V.gp.hat.inv, cov[[k]]))
  
identical(as.numeric(mash.arrays$post.means[k,]),as.numeric(post.b.gpkl.mean(b.mle = b.mle,V.gp.hat.inv = V.gp.hat.inv,U.gp1kl = U.gp1kl)))

identical(as.numeric(mash.arrays$post.covs[k,]),as.numeric(diag(U.gp1kl)))


pis=readRDS("pismashtruthwithfixedomega.rds")$pihat
log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))


mashnobase.means=as.matrix(read.table("withfixedomegamnbposterior.means.txt")[,-1])

beta=as.matrix(c$beta)
mle=as.matrix(wfull)
mash.means=as.matrix(mash.means)
standard=sqrt(mean((beta-mle)^2))
sqrt(mean((mash.means-beta)^2))/standard
sqrt(mean((mashnobase.means-beta)^2))/standard
```

Let's compare the learned matrices which receive the most weighting in mash and mashnobaseline. First, let' look at the dsitribution of matrices.

Let's do the same thing comparing the pis using mash from the truth and from mashnobaseline:

```{r}
pimash=readRDS("pismashtruthwithfixedomega.rds")$pihat
pied=readRDS("piswithfixedomegamnb.rds")$pihat
par(mfrow=c(1,2))
barplot(colSums(matrix(pied[-length(pied)],ncol=2,byrow=TRUE)),names=c("ID","Uk1"),las=2,main="Mashnobase")
barplot(colSums(matrix(pimash[-length(pied)],ncol=2,byrow=TRUE)),names=c("ID","Uk1"),las=2,main="Mash")
```

We can see that using the same set of covariance matrices, _mashnobaseline_ is better able to emphasize the true configurations because the likelihood and posterior computations incorporate L. Recall, for mash:

$$w_{j,R} ~ N(0,V+U)$$
$$w_{j,R-1} ~ N(0,LVL' + LUL')$$ where L is the R-1 x R centering matrix.

In both:

$$c = \mu + \beta$$
$$chat = c + E$$
$$v \sim \sum_p N(0,Uk)$$
$$E \sim N(0,V)$$

So even if the Uks are different, the likelihood and corresponding $\pi$s will be different.


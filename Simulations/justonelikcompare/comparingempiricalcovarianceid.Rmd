In this document, we are trying to determine how the output of ED varies with different starting points, and we are also trying to see how using the output of ED to compute likelihood of data varies with using the matrix that was used to simulate the data. 

Also, we simulate our data using a standard error of 1 instead of 0.1, so that the EZ and EE models are the same.

Our findings are that the likelihood is better with ED output than when using the truth, and that the results of ED do depend on the initiation point. The best results come when initializing ED with X'X or identity.

First, run while initiating ED with empirical covariance matrix:

```{r}
rm(list=ls())
library('mashr')
set.seed(123)

library("ExtremeDeconvolution")
R=8




set.seed(123)
c=chat_sim_fsingle_fixedomega(n = 10000,d = 8,omega = 2,esd = 1)
saveRDS(c,"chatfixedomegastandardone.rds")
diag(c$covmat)

t=c$t;b=c$chat;se=c$shat
identical(b,t)
#hist(t)
#hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se

index=which(rowSums(c$beta)!=0)
strongprojectedt=t(L%*%t(t[index,]))




lvllist=genlvllist(s.j[index,],L = L[-1,])


##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR so we can have the full set of deviations and thier standard errors to choose grid
dim(wfull)
dim(sjmat)

t.stat =strongprojectedt;v.j = lvllist;L = L[-1,]
init.cov.list=list()
init.cov.list[[1]]=cov(strongprojectedt)
#head(init.cov.list)
mean.mat=matrix(rep(0,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat))  
ydata=strongprojectedt[,-1]##train on the max like estimates
xamp=1
xcovar=init.cov.list
fixmean=TRUE     
ycovar=v.j     
xmean=mean.mat   
projection=list();for(l in 1:nrow(t.stat)){projection[[l]]=L}
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)
se.train=se

covlist=e$xcovar
R=8;L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
train.b=data.frame(wfull)[index,-1];se.train = se;covmat = covlist;pen = 1;L = L[-1,];J=nrow(train.b)
lik.mat=t(sapply(seq(1:J),function(j){
  V.gp.hat=diag(se.train[j,])^2;
  LVL=L%*%V.gp.hat%*%t(L);
  LSigL=L%*%covlist[[1]]%*%t(L)
  b.mle=train.b[j,]
  dmvnorm(x=b.mle, sigma=(LSigL + LVL),log=TRUE)}))

dim(lik.mat)
sum(log(exp(lik.mat)))
j=10
lik.mat[10]
V.gp.hat=diag(se.train[j,])^2;
LVL=L%*%V.gp.hat%*%t(L)
LSigL=L%*%covlist[[1]]%*%t(L)
dmvnorm(x=train.b[j,], sigma=(LSigL + LVL),log=TRUE)

```

Now do with the Identity:

```{r}
rm(list=ls())
set.seed(123)

R=8
c=readRDS("chatfixedomegastandardone.rds")
t=c$t;b=c$chat;se=c$shat
#hist(t)
#hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se

index=which(rowSums(c$beta)!=0)
strongprojectedt=t(L%*%t(t[index,]))




lvllist=genlvllist(s.j[index,],L = L[-1,])


##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR so we can have the full set of deviations and thier standard errors to choose grid
dim(wfull)
dim(sjmat)

t.stat =strongprojectedt;v.j = lvllist;L = L[-1,]
init.cov.list=list()
init.cov.list[[1]]=diag(1,R)
#head(init.cov.list)
mean.mat=matrix(rep(0,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat)) 
ydata=strongprojectedt[,-1]
xamp=1
xcovar=init.cov.list
fixmean=TRUE     
ycovar=v.j     
xmean=mean.mat   
projection=list();for(l in 1:nrow(t.stat)){projection[[l]]=L}
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)
se.train=se

covlist=e$xcovar
R=8;L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
train.b=data.frame(wfull)[index,-1];se.train = se;covmat = covlist;pen = 1;L = L[-1,];J=nrow(train.b)
lik.mat=t(sapply(seq(1:J),function(j){
  V.gp.hat=diag(se.train[j,])^2;
  LVL=L%*%V.gp.hat%*%t(L);
  LSigL=L%*%covlist[[1]]%*%t(L)
  b.mle=train.b[j,]
  dmvnorm(x=b.mle, sigma=(LSigL + LVL),log=TRUE)}))

dim(lik.mat)
sum(log(exp(lik.mat)))
j=10
lik.mat[10]
V.gp.hat=diag(se.train[j,])^2;
LVL=L%*%V.gp.hat%*%t(L)
LSigL=L%*%covlist[[1]]%*%t(L)
dmvnorm(x=train.b[j,], sigma=(LSigL + LVL),log=TRUE)
  
```

Now do with the truth:


```{r}
rm(list=ls())

set.seed(123)

R=8
c=readRDS("chatfixedomegastandardone.rds")
t=c$t;b=c$chat;se=c$shat
#hist(t)
#hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se

index=which(rowSums(c$beta)!=0)
strongprojectedt=t(L%*%t(t[index,]))




lvllist=genlvllist(s.j[index,],L = L[-1,])


##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR so we can have the full set of deviations and thier standard errors to choose grid
dim(wfull)
dim(sjmat)

t.stat =strongprojectedt;v.j = lvllist;L = L[-1,]
init.cov.list=list()
init.cov.list[[1]]=c$covmat
#head(init.cov.list)
mean.mat=matrix(rep(0,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat)) 
ydata=strongprojectedt[,-1]
xamp=1
xcovar=init.cov.list
fixmean=TRUE     
ycovar=v.j     
xmean=mean.mat   
projection=list();for(l in 1:nrow(t.stat)){projection[[l]]=L}
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)
se.train=se

covlist=e$xcovar
R=8;L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
train.b=data.frame(wfull)[index,-1];se.train = se;covmat = covlist;pen = 1;L = L[-1,];J=nrow(train.b)
lik.mat=t(sapply(seq(1:J),function(j){
  V.gp.hat=diag(se.train[j,])^2;
  LVL=L%*%V.gp.hat%*%t(L);
  LSigL=L%*%covlist[[1]]%*%t(L)
  b.mle=train.b[j,]
  dmvnorm(x=b.mle, sigma=(LSigL + LVL),log=TRUE)}))

dim(lik.mat)
sum(log(exp(lik.mat)))
j=10
lik.mat[j]
V.gp.hat=diag(se.train[j,])^2;
LVL=L%*%V.gp.hat%*%t(L)
LSigL=L%*%covlist[[1]]%*%t(L)
dmvnorm(x=train.b[j,], sigma=(LSigL + LVL),log=TRUE)
  
```

####


Let's also see what happens when we use just the truth or the output after using the empirical covariance matrix:

```{r}
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
covlist=list();covlist[[1]]=c$covmat
train.b=data.frame(wfull)[index,-1];se.train = se;covlist = covlist;pen = 1;L = L[-1,];J=nrow(train.b)
lik.mat=t(sapply(seq(1:J),function(j){
  V.gp.hat=diag(se.train[j,])^2;
  LVL=L%*%V.gp.hat%*%t(L);
  LSigL=L%*%covlist[[1]]%*%t(L)
  b.mle=train.b[j,]
  dmvnorm(x=b.mle, sigma=(LSigL + LVL),log=TRUE)}))

dim(lik.mat)
sum(log(exp(lik.mat)))
j=10
lik.mat[j]
V.gp.hat=diag(se.train[j,])^2;
LVL=L%*%V.gp.hat%*%t(L)
LSigL=L%*%covlist[[1]]%*%t(L)
dmvnorm(x=train.b[j,], sigma=(LSigL + LVL),log=TRUE)
```

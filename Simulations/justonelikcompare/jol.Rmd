---
title: "newliks"
output: html_document
---

> The purpose of this document is to:

* generate one Uk matrix according to the Bovy algorithm, using the matrix of maximal (i.e., top 1000 average deviations across subgroups) of T statistics initiliazed with X'X, and then rescaled to fit the centered $\hat{beta}$ by creating max of diagonal as 2.

* We will compare this to the results using the $true$ covariance matrix ee', where e = [0 0 0 0 0 0 1]




```{r setup, include=FALSE}
library('knitr')

```


```{r,echo=FALSE}
suppressMessages(library(mvtnorm))
suppressMessages(library(mashr))
```
Let's simulate data with one covariance matrix, and $\omega$ fixed at 2. We will also use only one $\omega$ in our inference matrices. The purpose of this document was to test the ability of `mash` and `mashnobaseline` to infer the truth when there was only one $\omega$ to complicate things, so that the 'shrinkage' was not an issue. You can see the inferred covariance matrices are shorter.

We can have a look at the them first. Here, the c and t should be the same because they are simulated with standard error of 1.

```{r}

system("ls; rm *rds;rm *txt; rm *pdf")
rm(list=ls())
library('mashr')
set.seed(123)
c=chat_sim_fsingle_fixedomega(n = 1000,d = 8,omega = 2,esd = 1,reals=1)
saveRDS(c,"chatfixedomega.rds")
diag(c$covmat)

```


And here we run our analysis:
```{r}
c=readRDS("chatfixedomega.rds")
t=c$t;b=c$chat;se=c$shat
#hist(t)
#hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se


###here we cheat and use the index as the 'real associations'

index=which(rowSums(c$beta)!=0)

#index=which(rowMeans(abs(t(L%*%t(c$t))))>1.3)##choose the associations with average deviation large.



sparsestrongt=t(L%*%t(t[index,]))

```

```{r filewrite,eval=T}

write.table(sparsestrongt,"sparsestrongt.txt",col.names = FALSE,row.names=FALSE)

```


Now, we need to project into the centered space to estimate the covariance matrix of the true deviations, using the full L since $v$ will be R, and not $R-1$.


```{r}
system('/Users/sarahurbut/miniconda3/bin/sfa -gen sparsestrongt.txt -g 1000 -k 5 -n 8 i -o sparseF')
A="sparseF"

factor.mat=as.matrix(read.table("sparseF_F.out"))
lambda.mat=as.matrix(read.table("sparseF_lambda.out"))

#recall here that w will now be the L[-1,]%*%t(t[strong,]), which is equivalent to removing the first column of the strong projected t. Thus the covariance of v is initated with the RxR matrices (strong projected t), and the model is trained on the strong projected t less the first column because the model has v_{rxr}|w_{r,r-1}

strongprojectedt=sparsestrongt


lvllist=genlvllist(s.j[index,],L = L[-1,])


##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR so we can have the full set of deviations and thier standard errors to choose grid
dim(wfull)
dim(sjmat)

```

```{r}
w=data.frame(wfull)[,-1]
```


-- Now run with ED to generate the max like covariance matrix using the maximum initialized t statistics and compare to the initial X'X:


```{r}


A="withoneK"


library("ExtremeDeconvolution")
efunc=deconvolution.em.with.bovy.with.L.oneK(t.stat = strongprojectedt,v.j = lvllist,L = L[-1,],w = strongprojectedt[,-1])

dim(efunc$true.covs)
length(efunc$pi)

#efunc$true.covs[1,,]-min(abs(efunc$true.covs[1,,]))

##compare to initialized matrix
X.real=as.matrix(strongprojectedt)
X.c=apply(X.real,2,function(x) x-mean(x)) ##Column centered matrix of t statistics
data.prox=((t(X.c)%*%X.c))/(nrow(X.real)-1)
(data.prox)-min(abs(data.prox))
```

Here you can see our 'list' of covariance matrices contains just one covariance matrix that has been scaled to contain 2 as the max on the diagonal.

```{r}
covlist=list();covlist[[1]]=2*efunc$true.covs[1,,]/max(diag(efunc$true.covs[1,,]))
edcov=edcov=covlist
max(edcov[[1]])
barplot(diag(edcov[[1]]))

```



```{r eval=T}
w=data.frame(wfull)[,-1]

j=1
A="withoneKandoneomeganew"
compute.hm.train.log.lik.pen.with.L(w[index,],se.train = se,covmat = edcov,A = A,pen = 1,L = L[-1,])

likmat=t(as.matrix(readRDS(paste0("liketrain",A,".rds"))))

sum(log(exp(likmat)))
sum(log(exp(likmat[1:1000])))
LSigL=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})

lltest=log.lik.func(b.mle=w[j,],V.gp.hat = L[-1,]%*%diag(se[j,])^2%*%t(L[-1,]),covmat =LSigL )                                                                                          

identical(as.numeric(likmat[j]),as.numeric(lltest))
```

Let's try without 2:

```{r}
covlist=list();covlist[[1]]=efunc$true.covs[1,,]
edcov=edcov=covlist
max(edcov[[1]])
barplot(diag(edcov[[1]]))

```



```{r eval=T}
w=data.frame(wfull)[,-1]
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))

j=1
A="withoneKandoneomeganewwithnoscale"
compute.hm.train.log.lik.pen.with.L(w[index,],se.train = se,covmat = edcov,A = A,pen = 1,L = L[-1,])

likmat=t(as.matrix(readRDS(paste0("liketrain",A,".rds"))))

sum(log(exp(likmat)))
sum(log(exp(likmat[1:1000])))
LSigL=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})

lltest=log.lik.func(b.mle=w[j,],V.gp.hat = L[-1,]%*%diag(se[j,])^2%*%t(L[-1,]),covmat =LSigL )                                                                                          

identical(as.numeric(likmat[j]),as.numeric(lltest))

pis=1
A="mashnobedcov"
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))

weightedquants=lapply(seq(1:nrow(w)),function(j){total.quant.per.snp.no.baseline(j,edcov,b.gp.hat=w,se.gp.hat = se,pis,A=A,checkpoint = FALSE,L=L[-1,])})

w=data.frame(wfull[,-1])
a=post.mean.with.proj(b.mle = t(w[1,]),tinv = solve(L[-1,]%*%diag(se[1,],R)%*%t(L[-1,])+L[-1,]%*%edcov[[1]]%*%t(L[-1,])),U.k = edcov[[1]],L=L[-1,])
mashnobasemeans=read.table("mashnobedcovposterior.means.txt")[,-1]
all.equal(as.numeric(mashnobasemeans[1,]),as.numeric(a))
```


Now let's compute the likelihood with the true matrix which looks like this:

```{r}
covtruthlist=list(2*c$covmat)
barplot(diag(covtruthlist[[1]]))

```


```{r}

A="truth"
compute.hm.train.log.lik.pen.with.L(train.b = w[index,],se.train = se,covmat = covtruthlist,A = A,pen = 1,L = L[-1,])

likmattruth=t(as.matrix(readRDS("liketraintruth.rds")))
LSigL=lapply(covtruthlist,function(x){L[-1,]%*%x%*%t(L[-1,])})

lltest=log.lik.func(b.mle=w[j,],V.gp.hat = L[-1,]%*%diag(se[j,])^2%*%t(L[-1,]),covmat =LSigL )                                                                                          

identical(as.numeric(likmattruth[j]),as.numeric(lltest))

sum(log(exp(likmattruth)))
sum(log(exp(likmattruth[1:1000])))


liked=t(as.matrix(readRDS("liketrainwithoneKandoneomeganewwithnoscale.rds")))



sum(log(exp(liked)))
sum(log(exp(liked[1:1000])))
liktable=c(LikelihoodwithTruth=sum(log(exp(likmattruth))),LikewithEd=sum(log(exp(liked))))
barplot(liktable,ylim=c(-10000,0),main="LogLikelihood with Truth vs ED",col=c("red","green"))
```


As you can see, the likelihood with the truth, `r ceiling(sum(log(exp(likmattruth))))` is better than the likelihood with the *learned* $U_k$, `r ceiling(sum(log(exp(likmat))))` but this is to be expected because the likelihood with the *learned* $U_k$ was computed using the max like estimates of Bovy on the Maximum deviations, while the the truth provides more weight on 0 that will help with the largely null observations.


### Now there is just one component for MASH, using the centered t and beta hats to compute likelihood: 

We use the same data as above (i.e., there is a mean) and we center the chats and t statistics before training and computing likelihood:

```{r}
rm(list=ls())

c=readRDS("chatfixedomega.rds")
t=c$t;b=c$chat;se=c$shat
length(which(rowSums(c$beta)!=0))

###Here we're just going to use the 'real associations'
index=which(rowSums(c$beta)!=0)
t=c$t[index,];b=c$chat[index,];se=c$shat[index,]
R=ncol(t)
##center the estimates
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
w=t(L%*%t(b))
wt=t(L%*%t(t))

dim(t)
identical(t,b)
strongt=wt[index,]
v.j=se



init.cov.list=list()
init.cov.list[[1]]=cov(strongt)

#head(init.cov.list)
mean.mat=matrix(rep(0,R*R),ncol=R,nrow=R)  
ydata= strongt##train on the max like estimates (same as using max ws)
xamp=1
xcovar=init.cov.list
fixmean=TRUE     
ycovar=v.j     
xmean=mean.mat   
projection=list();for(l in 1:nrow(strongt)){projection[[l]]=diag(1,R)}##no projection in mash
e=extreme_deconvolution(ydata=ydata,ycovar=ycovar,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)##notice no projection.
se.train=se



se.train=se


covlist=e$xcovar

train.b=data.frame(strongt);se.train = se;covmat = covlist;pen = 1;L = diag(1,R);J=nrow(train.b)
lik.mat=t(sapply(seq(1:J),function(j){
  V.gp.hat=diag(se.train[j,])^2;
  b.mle=train.b[j,]
  dmvnorm(x=b.mle, sigma=(covlist[[1]]+V.gp.hat),log=TRUE)}))##here the likelihood estimate is different

dim(lik.mat)
sum(log(exp(lik.mat)))
j=10
lik.mat[10]
V.gp.hat=diag(se.train[j,])^2;
dmvnorm(x=train.b[j,], sigma=(covlist[[1]]+V.gp.hat),log=TRUE)
mean(lik.mat)
e$avgloglikedata

A="mash"
compute.hm.train.log.lik.pen.with.L(train.b = w[index,],se.train = se,covmat = covlist,A = A,pen = 1,L = diag(1,R))
lik.mat=readRDS(paste0("liketrain",A,".rds"))
sum(log(exp(lik.mat)))

pis=1
A="mash"

weightedquants=lapply(seq(1:nrow(w)),function(j){total.quant.per.snp(j,covlist,b.gp.hat=w,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})
```


Let's compare RMSEs:

```{r}
mash.means=read.table("mashposterior.means.txt")[,-1]
mash.nobasemeans=read.table("mashnobedcovposterior.means.txt")[,-1]

mle=w

head(mle)
rmse.mash=sqrt(mean((mash.means-mle)^2))
rmse.mnb=sqrt(mean((mash.nobasemeans-mle)^2))

rmse.all.table=cbind(mash=rmse.mash,mnb=rmse.mnb)

rmse.mash
rmse.mnb

barplot(rmse.all.table)
U.gp1kl = post.b.gpkl.cov(V.gp.hat.inv = solve(diag(se[1,],R)),U.0k.l = covlist[[1]])

all.equal(as.numeric(post.b.gpkl.mean(b.mle = w[1,],V.gp.hat.inv = solve(diag(se[1,],R)),U.gp1kl = U.gp1kl)),as.numeric(mash.means[1,]))

```


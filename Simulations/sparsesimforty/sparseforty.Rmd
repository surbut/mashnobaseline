---
title: "sparsethirty"
output: html_document
---

```{r setup, include=FALSE}
library('knitr')
knitr::opts_chunk$set(cache=TRUE)
```


Now we're going to simulate data across 30 subgroups according to 5 factors:

```{r}
library('mash')
chat_sim_fsimple
```

You can see that the covariance matrices correspond to the factors:
```{r}
rm(list=ls())
set.seed(123)
RNGkind("Mersenne-Twister")
c=chat_sim_fsimple(n=10000,d=30,betasd=1,esd=0.1,K=10)

c$factors


identical(c$beta[1,]+c$mumat[1,],c$ceff[1,])
identical(c$chat[1,],c$ceff[1,]+c$error[1,])
```

Furthermore, you can see that the components look like such:

```{r}
table(c$components)
barplot(rowSums(c$covmat[[2]]))
comptwoers=which(c$components==2)[1:5]
(c$beta[comptwoers,])
```


Now, let's do our analysis:

```{r}
saveRDS(c,"chatsimesparsefthirty.rds")
t=c$t;b=c$chat;se=c$shat
hist(t)
hist(b)
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se

absmat=abs(t(L%*%t(c$t)))
index=which(rowSums(absmat>4)>0)
length(index)
mean(index<1000)

#index=which(rowMeans(abs(t(L%*%t(c$t))))>1.3)##choose the associations with average deviation large.

##show deviations
plot(rowSums(absmat>4))
points(index,rowSums(absmat>4)[index],col="blue")

sparsestrongt=t(L%*%t(t[index,]))



write.table(sparsestrongt,"sparsestrong_thirty.txt",col.names = FALSE,row.names=FALSE)

```


Now, we need to project into the centered space to estimate the covariance matrix of the true deviations, using the full L since $v$ will be R, and not $R-1$.


```{r}
system('/Users/sarahurbut/miniconda3/bin/sfa -gen sparsestrong_thirty.txt -g 557 -k 5 -n 30 i -o sparsestrong_thirty')
A="sparsestrongthirty"

factor.mat=as.matrix(read.table("sparsestrong_thirty_F.out"))
lambda.mat=as.matrix(read.table("sparsestrong_thirty_lambda.out"))

#recall here that w will now be the L[-1,]%*%t(t[strong,]), which is equivalent to removing the first column of the strong projected t. Thus the covariance of v is initated with the RxR matrices (strong projected t), and the model is trained on the strong projected t less the first column because the model has v_{rxr}|w_{r,r-1}

strongprojectedt=sparsestrongt


lvllist=genlvllist(s.j[index,],L = L[-1,])


##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR so we can have the full set of deviations and thier standard errors to choose grid
dim(wfull)
dim(sjmat)

```

Compute the likelihood without ED:
```{r}
A="noedcovsparsethirty"
noedcov=compute.covmat(b.gp.hat = wfull,sebetahat = sjmat,Q = 5,t.stat = strongprojectedt,lambda.mat = lambda.mat,P = 3,A = A,factor.mat = factor.mat,bma = T,zero = T,power = 2)$cov
```
Check that all the matrices are correctly computed:
```{r}
pca=svd(strongprojectedt);v=pca$v[,1:3];d=pca$d[1:3];u=pca$u[,1:3]
pcaprox=t(u%*%diag(d)%*%t(v))%*%(u%*%diag(d)%*%t(v))/max(diag(t(u%*%diag(d)%*%t(v))%*%(u%*%diag(d)%*%t(v))))


P=3;X.c=apply(strongprojectedt,2,function(x) x-mean(x)) ##Column centered matrix of t statistics
svd.X=svd(X.c)
M=nrow(X.c)
v=svd.X$v;u=svd.X$u;d=svd.X$d
cov.pc=1/M*v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P])%*%t(v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P]))

for(q in 1:5){
load=as.matrix(lambda.mat[,q])
fact=as.matrix(factor.mat[q,])
rank.prox=load%*%t(fact)
a=(1/(M-1)*(t(rank.prox)%*% rank.prox))/max(diag((1/(M-1)*(t(rank.prox)%*% rank.prox))))
print(all.equal(as.numeric(a),as.numeric(noedcov[[3+q]]/max(diag(noedcov[[3+q]])))))}

load=as.matrix(lambda.mat)
fact=as.matrix(factor.mat)
rank.prox=load%*%(fact)
a=(1/(M-1)*(t(rank.prox)%*% rank.prox))/max(diag((1/(M-1)*(t(rank.prox)%*% rank.prox))))
all.equal(as.numeric(a),as.numeric(noedcov[[9]]/max(diag(noedcov[[9]]))))

all.equal(cov(strongprojectedt)/max(diag(cov(strongprojectedt))),noedcov[[2]]/max(diag(noedcov[[2]])))
all.equal(as.numeric(cov.pc)/max(diag(cov.pc)),as.numeric(noedcov[[3]]/max(diag(noedcov[[3]]))))

```

$$Likelihood$$

Now we will replace the RxR matrix $L$ with the $R-1xR$ matrix $Lâˆ—$, effectively removing a data point from the observed uncentered statistics, such that the rank of the marginal variance of $w$ is guaranteed to be equal to the dimension of $w$.

\section{Likelihood}

Now we will replace the RxR matrix $L$ with the R-1xR matrix $L*$, effectively removing a data point from the observed uncentered statistics, such that the rank of the marginal variance of $w$ is guaranteed to be equal to the dimension of $w$.

Now for each gene J at each component k, integrating over $\vb$, 



$$L_{R-1,R} \ceff \sim \Norm (0, L_{R-1,R} U_{k} L_{R-1,R} ') $$
$$L_{R-1,R} \chat \sim \Norm (0, L_{R-1,R}  U_{k} L_{R-1,R} ' + L_{R-1,R} \hat{V} L_{R-1,R} ')$$ 

And thus we can use the Bovy et al algorithm invoked in both the Extreme Deconvolution package and in `Sarah's MixEm' where the marginal variance at each component, $T_{jp}$:


$$T_{jp} = L_{R-1,R} U_k L_{R-1,R}' + L_{R-1,R} \hat{V}_{j} L_{R-1,R}'$$


For each gene, and $w_{j} = L_{R-1,R} \chat_{j}$.

Recall that our previous approach was simplified by the fact that $\bm{w}_{j}$ was simply $\hat{\bm{b}_{j}}$ and the projection matrix was simply the $I_{r}$ identity matrix. Our inference on $\bm{b}$ was analogous to their inference on $\vb_{j}$. 


As before, we are interested in returning the prior covariance $U_k$ matrices of the `true` deviations $\vb$, which we will then rescale by choosing a set of $\omega$ that are appropriate to $L \chat $ to comprise a set of $P = KxL$ prior covariance matrices $\Sigma$.

and choose the set of $\pi$ that maximizes compute the following likelihood at each of the P components: 


$$L_{R-1,R}  \chat _j \sim \Norm (0, L_{R-1,R}  \Sigma_{p} L_{R-1,R} ' + L_{R-1,R} \hat{V_j} L_{R-1,R} ') $$
 
Now compute likelihood without ED:
```{r}
w=data.frame(wfull)[,-1]

A="noedcovsparsethirty"
compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = noedcov,A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))
```


Now run with ED to show that 
1) the likelihood improves
2) we have successfully implemented an approach which allows us to input a list of residual variance matrices (as opposed to a matrix of vectors to be diagonalised).


```{r}
rm(A)
rm(noedcov)

library("ExtremeDeconvolution")
efunc=deconvolution.em.with.bovy.with.L(t.stat = strongprojectedt,factor.mat = factor.mat,lambda.mat = lambda.mat,v.j = lvllist,P = 3,L = L[-1,],Q = 5,w = strongprojectedt[,-1])

A="sparseLthirty"


apply(efunc$true.covs,1,function(x){qr(x)$rank})

max.step=efunc
edcov=compute.hm.covmat.all.max.step(b.hat = wfull,se.hat = sjmat,t.stat = strongprojectedt,Q = 5,lambda.mat = lambda.mat,A=A,factor.mat = factor.mat,max.step = efunc,zero = T,power = 2)$cov
length(edcov)

all.equal(edcov[[2]]/max(diag(edcov[[2]])),max.step$true.covs[1,,]/max(diag(max.step$true.covs[1,,])))
all.equal(edcov[[3]]/max(diag(edcov[[3]])),max.step$true.covs[3,,]/max(diag(max.step$true.covs[3,,])))
all.equal(edcov[[9]]/max(diag(edcov[[9]])),max.step$true.covs[2,,]/max(diag(max.step$true.covs[2,,])))

```

```{r}
A="sparseLthirty"
w=data.frame(wfull)[,-1]

compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = edcov,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat

likmat=readRDS(paste0("liketrain",A,".rds"))
test=exp(likmat)
sum(log(test%*%pis))


for(j in 1:nrow(w)){
total.quant.per.snp.no.baseline(j = j,covmat = edcov,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}

```
As you can see, the likelihood is much better with ED.

##Test to show our calculations are accurate:
```{r}
mash.means=as.matrix(read.table("sparseLthirtyposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("sparseLthirtylfsr.txt"))[,-1]
edcov=readRDS("covmatsparseLthirty.rds")
j=10
arrays=post.array.per.snp.no.baseline(j = 10,covmat = edcov,b.gp.hat = w,se.gp.hat = se,L = L[-1,])
k=5

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

LSigL=L[-1,]%*%edcov[[k]]%*%t(L[-1,])
LVL=L[-1,]%*%V.gp.hat%*%t(L[-1,])

LSigL_list=lapply(edcov,function(x){L[-1,]%*%x%*%t(L[-1,])})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,])))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L[-1,]%*%edcov[[k]]%*%t(L[-1,])),U.k = edcov[[k]],L = L[-1,]))))

log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
pis=readRDS("pissparseLthirty.rds")$pihat
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))
```

###COmpare with MASH##

Now we can compare with mash. In this model, for each gene J:

$$\chat |\mu, V = \mu + \beta + E$$

$$\chat - \mu = \beta + E$$

and $$E ~ N(0,V)$$
while $$\beta  ~ \sum_{p} \pi_{p} N(0, U_{p})$$
```{r}
rm(list=ls())
c=readRDS("chatsimesparsefthirty.rds")

t=c$t;b=c$chat;se=c$shat;R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
wfull=t(L%*%t(b))
##show that they sum to 0

plot(rowSums(wfull),ylim=c(-1,1))
```

Now we proceed as in mash, selecting the strong t statistics:
```{r}

s.j=se/se
absmat=abs(t(L%*%t(c$t)))


index=which(rowSums(absmat>4)>0)
length(index)
mean(index<1000)

factor.mat=as.matrix(read.table("sparsestrong_thirty_F.out"))
lambda.mat=as.matrix(read.table("sparsestrong_thirty_lambda.out"))



strongprojectedtsimulations=t(L%*%t(t[index,]))
max.step=deconvolution.em.with.bovy(t.stat = strongprojectedtsimulations,factor.mat = factor.mat,v.j = s.j,lambda.mat = lambda.mat,K = 3,P = 3 )

##Show how the ranks is preserved
apply(max.step$true.covs,1,function(x){qr(x)$rank})


covmash=compute.hm.covmat.all.max.step(max.step = max.step,b.hat = wfull,se.hat = se,t.stat = strongprojectedtsimulations,Q = 5,lambda.mat = lambda.mat,A = "sparseoldmashthirty",factor.mat = factor.mat,zero = T,power = 2)$covmat

all.equal(covmash[[2]]/max(diag(covmash[[2]])),max.step$true.covs[1,,]/max(diag(max.step$true.covs[1,,])))
all.equal(covmash[[3]]/max(diag(covmash[[3]])),max.step$true.covs[3,,]/max(diag(max.step$true.covs[3,,])))
all.equal(covmash[[9]]/max(diag(covmash[[9]])),max.step$true.covs[2,,]/max(diag(max.step$true.covs[2,,])))

```

Now, proceed as always, estimating likelihood and computing posteriors.

```{r mashmethod, eval=T}
A="sparseoldmashthirty"
compute.hm.train.log.lik.pen(train.b = wfull,se.train = se,covmat = covmash,A=A,pen=1)


pis=readRDS(paste0("pis",A,".rds"))$pihat
b.test=t
se.test=s.j
weightedquants=lapply(seq(1:nrow(wfull)),function(j){total.quant.per.snp(j,covmash,b.gp.hat=wfull,se.gp.hat = se,pis,A=A,checkpoint = FALSE)})
```

Test to show our calculations are accurate:

```{r}
mash.means=as.matrix(read.table("sparseoldmashthirtyposterior.means.txt")[,-1])
lfsr.mash=as.matrix(read.table("sparseoldmashthirtylfsr.txt"))[,-1]
cov=readRDS("covmatsparseoldmashthirty.rds")
pis=readRDS("pissparseoldmashthirty.rds")$pihat
w=data.frame(wfull)
R=ncol(w)
j=10

arrays=post.array.per.snp.no.baseline(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se,L = diag(1,R))
mash.arrays=post.array.per.snp(j = 10,covmat = cov,b.gp.hat = w,se.gp.hat = se)
##show same algebra


k=5

V.gp.hat=diag(se[j,])^2

b.mle = as.vector(w[j,])

lapply(seq(1:5),function(x){all.equal(arrays[[x]],mash.arrays[[x]])})
L=diag(1,R)
LSigL=L%*%cov[[k]]%*%t(L)
LVL=L%*%V.gp.hat%*%t(L)

LSigL_list=lapply(cov,function(x){L%*%x%*%t(L)})
identical(LSigL_list[[k]],LSigL)
identical(as.numeric(arrays$post.means[k,]),as.numeric(post.mean.with.proj(b.mle = t(b.mle),tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L)))
identical(as.numeric(arrays$post.covs[k,]),as.numeric(diag(post.cov.with.proj(tinv = solve(LVL+L%*%cov[[k]]%*%t(L)),U.k = cov[[k]],L = L))))




b.mle=as.vector(t(w[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)
U.gp1kl= (post.b.gpkl.cov(V.gp.hat.inv, cov[[k]]))
  
identical(as.numeric(mash.arrays$post.means[k,]),as.numeric(post.b.gpkl.mean(b.mle = b.mle,V.gp.hat.inv = V.gp.hat.inv,U.gp1kl = U.gp1kl)))

identical(as.numeric(mash.arrays$post.covs[k,]),as.numeric(diag(U.gp1kl)))



log.lik.snp=log.lik.func(b.mle,LVL,LSigL_list)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

all.equal(as.numeric(post.weights%*%arrays$post.means),as.numeric(mash.means[j,]))

mle=as.matrix(wfull)
beta=as.matrix(c$beta)

mash.nobaselinemeans=as.matrix(read.table("sparseLthirtyposterior.means.txt")[,-1])
mash.means=as.matrix(read.table("sparseoldmashthirtyposterior.means.txt")[,-1])


standard=sqrt(mean((beta-mle)^2))
sqrt(mean((mash.means-beta)^2))/standard
sqrt(mean((mash.nobaselinemeans-beta)^2))/standard
```

Let's look at a few examples:

```{r}
barplot(c$factors[2,])
j=which(c$components==2)[2]

par(mfrow=c(2,2))
barplot(c$beta[j,])
barplot(c$chat[j,]-mean(c$chat))
barplot(mash.nobaselinemeans[j,])
barplot(mash.means[j,])

barplot(c$factors[3,])
j=which(c$components==3)[4]


par(mfrow=c(2,2))
barplot(c$beta[j,])
barplot(c$chat[j,]-mean(c$chat[j,]))
barplot(mash.nobaselinemeans[j,])
barplot(mash.means[j,])
```

```{r}

standard=sqrt(mean((beta[1:1000,]-mle[1:1000,])^2))
sqrt(mean((mash.means[1:1000,]-beta[1:1000,])^2))/standard
sqrt(mean((mash.nobaselinemeans[1:1000,]-beta[1:1000,])^2))/standard

```


Show ROC curves:
```{r}
##now we generate curves
mash.power=NULL
mashnobase.power=NULL

mash.fp=NULL
mashnobase.fp=NULL

sign.test.mash=as.matrix(c$beta)*mash.means
sign.test.mashnobase=as.matrix(c$beta)*mash.nobaselinemeans

lfsr.mash=as.matrix(read.table("sparseoldmashthirtylfsr.txt"))[,-1]
lfsr.nobase=as.matrix(read.table("sparseLthirtylfsr.txt"))[,-1]

thresholds=seq(0.01,1,by=0.01)
beta=as.matrix(c$beta)
for(s in 1:length(thresholds)){
thresh=thresholds[s]

##sign power is the proportion of true effects correctly signed at a given threshold
mash.power[s]=sum(sign.test.mash>0&lfsr.mash<=thresh)/sum(beta!=0)
mashnobase.power[s]=sum(sign.test.mashnobase>0&lfsr.nobase<=thresh)/sum(beta!=0)


##false positives is the proportion of null effects called at a given threshold
mash.fp[s]=sum(beta==0&lfsr.mash<=thresh)/sum(beta==0)
mashnobase.fp[s]=sum(beta==0&lfsr.nobase<=thresh)/sum(beta==0)
}





plot(mash.fp,mash.power,cex=0.5,pch=1,xlim=c(0,1),lwd=1,ylim=c(0,1),col="green",ylab="True Positive Rate",xlab="False Positive Rate",type="l",main="")
#title("True Positive vs False Positive",cex.main=1.5)
lines(mashnobase.fp,mashnobase.power,cex=0.5,pch=1,ylim=c(0,1),col="blue")
legend("bottomright",legend = c("mash","mashnobase"),col=c("green","blue"),pch=c(1,1))

```


---
title: "Analysing John's Data with our New Approach"
output: html_document
---

Let's try this out with john's data at time point 18, and notperform the centering in advnace. Critcally, I'm going to use the raw counts from John's Repo, and then perform the voom normalization myself, and use the weights from limma.
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

```{r}
library("limma")
library("edgeR")
library('mash')

full <- read.table("~/Dropbox/john/counts_per_sample.txt", header = TRUE,
                   stringsAsFactors = FALSE)
full <- full[order(full$dir), ]
rownames(full) <- paste(full$dir, full$ind, full$bact, full$time, sep = ".")
counts <- t(full[, grep("ENSG", colnames(full))])
# Filter lowly expressed genes
counts <- counts[rowSums(cpm(counts) > 1) > 6, ]
dim(counts)

groups <- full[, c("ind", "bact", "time", "extr", "rin")]
groups$bact <- gsub("\\+", "plus", groups$bact)
groups$ind <- factor(groups$ind)
groups$bact <- factor(groups$bact, levels = c("none", "Rv", "Rvplus", "GC",
                                              "BCG", "Smeg", "Yers", "Salm",
                                              "Staph"))
groups$time <- factor(groups$time, levels = c(4, 18, 48))
groups$extr <- factor(groups$extr)
head(groups)
dim(groups)

knownsamples=which(groups$time==18&groups$bact!="none")
counts_class=counts[,knownsamples]
dim(counts_class)
class_labs=groups[knownsamples,"bact"]
model_mat <- model.matrix(~as.factor(class_labs)-1)[,-1] ##remove the none column
##show how many individuals in each class##
colSums(model_mat)
beta_class <- matrix(0, dim(counts_class)[1], dim(model_mat)[2]);
sebeta_class <- matrix(0, dim(counts_class)[1], dim(model_mat)[2])
t_class = matrix(0, dim(counts_class)[1], dim(model_mat)[2])

colnames(counts_class)[10]
which(model_mat[10,]==1)

##voomobject without design matrix, voom takes a JxN matrix
dim(counts_class)
voom.obj=voom((counts_class))
norm.expression=voom.obj$E##this takes the log2 counts normalized for library size
weights=voom.obj$weights##numeric matrix of inverse variance weights
##fit with limma weights
for(k in 1:dim(model_mat)[2]){
  model_mat_temp <- cbind(model_mat[,k]);
  limma.obj <- limma::lmFit(norm.expression, model_mat_temp,weights=weights);
  beta_class[,k] <- as.matrix(limma.obj$coefficients[,1]);
  sebeta_class[,k] <- limma.obj$sigma*(as.matrix(limma.obj$stdev.unscaled[,1]));
  t_class[,k] = (as.matrix(limma.obj$coefficients[,1]))/(limma.obj$sigma*(as.matrix(limma.obj$stdev.unscaled[,1])));
  }

##show that beta_class[j,k] ##roughly## corresponds to the average expression at gene j of samples from class k, without incorporation of weights
j=100
k=2
mean(norm.expression[j,model_mat[,k]==1])
beta_class[j,k]
```

Now we compute the average for each subgroup of the uncentered read counts at each gene. Here, voom_class_adj will not acutally be adjusted byt the mean.

```{r, echo=FALSE, eval=FALSE}
write.table(beta_class,"beta_voomuncentered.txt",col.names = T)
write.table(sebeta_class,"sebeta_voomuncentered.txt",col.names = T)
write.table(t_class,"tvoom_uncentered.txt",col.names = T)
```

Check to make sure these are stored properly:
```{r,check}
j=8
k=3
lmFit(norm.expression,model_mat[,k],weights = weights)$coefficients[j,1]
beta_class[j,k]
##show that beta is close to the (unadjusted) mean for those individuals

mean(norm.expression[37,which(model_mat[,8]==1)])##mean gene expression for time point 18 individuals at gene 37
beta_class[37,8]
beta_class[37,8]/sebeta_class[37,8]
t_class[37,8]
```

Now we compute factors on the centered T statistics:

```{r}
b=read.table("beta_voomuncentered.txt")
se=read.table("sebeta_voomuncentered.txt")
t=read.table("tvoom_uncentered.txt")

###You can see uncentered###
hist(as.matrix(b),main="Uncentered Betas")
hist(as.matrix(t),main="Uncentered Ts")
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se


tcenteredfull=t(L%*%t(t))
hist(tcenteredfull,main="Centered T")
dim(tcenteredfull)
```

Critically,this should be of the same dimension as v, so use the full projection matrix. I don't use an lfsr threshold (instead I use a projected t cutoff) because we know that using the standard errors as the diagonal of $LVL'$ ignores the correlation terms.

```{r,eval=TRUE}
index=which(rowSums(abs(tcenteredfull)>2)>0)
strongprojectedt=t(L%*%t(t[index,]))
##check to make sure rowSums 0
rowSums(strongprojectedt)[1:10]
```

```{r,echo=F,eval=F}
write.table(strongprojectedt,"strongprojectedt.txt",col.names = F ,row.names = F)
```

Now, we need to project into the centered space to estimate the covariance matrix of the true deviations, using the full L since $v$ will be R, and not $R-1$.

```{r}
system('/Users/sarahurbut/miniconda3/bin/sfa -gen strongprojectedt.txt -g 892 -k 5 -n 8 i -o johnnobaseline')
A="johnnobaseline"

factor.mat=as.matrix(read.table("johnnobaseline_F.out"))
lambda.mat=as.matrix(read.table("johnnobaseline_lambda.out"))

```

Here are the matrices we will initialize BOVY with. Note that  t.stat is the matrix, MxR, of centered maximum t statistics.
```{r}
source("~/matrix_ash/R/mashnobasescripts.R")
initlist=init.covmat.single.with.projection(t.stat = strongprojectedt,factor.mat = factor.mat,lambda.mat = lambda.mat,Q = 5,P=3)
```

We check to see correctly initalised, e.g., empirical covariance of strong projected t, full rank factor mat, and svd.

```{r}
plot(initlist[1,,],cov(strongprojectedt))
M=nrow(strongprojectedt)
full.rank=lambda.mat%*%factor.mat
all.equal(as.numeric(initlist[2,,]),as.numeric((t(full.rank)%*%full.rank)/(M-1)))

P=3;X.c=apply(strongprojectedt,2,function(x) x-mean(x)) ##Column centered matrix of t statistics
svd.X=svd(X.c)
v=svd.X$v;u=svd.X$u;d=svd.X$d
cov.pc=1/M*v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P])%*%t(v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P]))
all.equal(as.numeric(cov.pc),as.numeric(initlist[3,,]))

q=3
load=as.matrix(lambda.mat[,q])
fact=as.matrix(factor.mat[q,])
rank.prox=load%*%t(fact)
a=(1/(M-1)*(t(rank.prox)%*% rank.prox))
all.equal(as.numeric(a),as.numeric(initlist[3+q,,]))
```

Great, now let's run Bovy:

Recall that we'll need $w$ to be Mx(R-1) and the initialized covariacne matrices to be RxR, so we will use the full set of deviations to initatlise our covariance matrices, and 'train Bovy' for the scales on the centered maximum T that are $Mx(R-1)$,

Here are the initalizing t.stats,e.g., the MxR matrix of maximums across all R subgroups. Recall we need this $v$ to be based on $RxR$ covariance matrices of strong t.stats.

$$v~N(0,Uk)$$
```{r}
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
t.stat=(t(L%*%t(t[index,])))
dim(t.stat)
```

Here will be the $ws$ and $lvls$, these need to be $R-1xR-1$ and of the transformed max $t$ statistcis, not $betahats$.
```{r}
L=L[-1,]
wt=t(L%*%t(t))
wtmax=wt[index,]
dim(wtmax)
lvlarray=genlvlarray(s.j = s.j[index,],L = L)
dim(lvlarray)
```

Currently, Bovy's R approach requires the S be diagonal (i.e., you put in a matrix of standard erros) when we know that w|Lv ~ N(Lv,LVL'), so we use our old covariance function with the matrices that we would have used to initalize Bovy (i.e., various rank approximations of the empirical covariance matrix of centered T statistics).


```{r,eval=FALSE}
##here, t.stat, factor.mat and lambda.mat will be MxR, KxR, and MxR (not Rx1) and
###each matrix in the lvlarray will be R-1,R-1 and w Mx(R-1) 
##t.stat represents the MxR centered observed t statistics, w the Mx(R-1) matrix
deconvolution.em.with.bovy(t.stat = t.stat,factor.mat = factor.mat,lambda.mat = lambda.mat,v.j = lvlarray,P = 3,L = L[-1,],Q = 5,w)
```

Alternatively, we can generate using our old covmat function. Let's use the EE model, so we estimate the grid, fit and compute posteriors using $w$ as the transformed betahat.

```{r}

L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
allcenteredt=(t(L%*%t(t)))
maxt=(t(L%*%t(t[index,])))
##w should represent transformed betahats using all
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR.
dim(wfull)
dim(sjmat)
A="jul22"
cov=compute.covmat(b.gp.hat = wfull,sebetahat = sjmat,Q = 5,t.stat = maxt,lambda.mat = lambda.mat,P = 3,A = "jul22",factor.mat = factor.mat,bma = T,zero = T,power = 2)$cov
##you can see that these will all by 8x8##
dim(cov[[1]])
length(cov)
```

Now, let's fit the model and compute posteriors on the reduced w:
```{r,eval=FALSE}
A="jul22"
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
R=ncol(L)
w=t(L[-R,]%*%t(b))
##which is just like removing a column of wfull
wfull[1,]
w[1,]

compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = cov,A = A,pen = 1,L = L[-1,])

pis=readRDS(paste0("pis",A,".rds"))$pihat
covmat=cov=c

for(j in 1:nrow(w)){
  total.quant.per.snp.no.baseline(j = j,covmat = cov,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])}
```

Check
```{r}

j=137
k=5
se.gp.hat=se
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
R=ncol(L)
w=t(L[-R,]%*%t(b))
b.gp.hat=data.frame(w)
R=ncol(L)
L=L[-1,]
b.mle=as.vector(t(b.gp.hat[j,]))
K=length(c)
R=ncol(L)
V.gp.hat=diag(se.gp.hat[j,])^2
LVL=L%*%V.gp.hat%*%t(L)
A="jul22"
pis=readRDS(paste0("pis",A,".rds"))$pihat
tinvlist=lapply(cov,function(x){solve(LVL+L%*%x%*%t(L))})
mupost=post.mean.with.proj(b.mle = b.mle,tinv = tinvlist[[k]],U.k = cov[[k]],L = L)
covpost=post.cov.with.proj(tinv = tinvlist[[k]],U.k = cov[[k]],L = L)
a=post.array.per.snp.no.baseline(j = j,covmat = cov,b.gp.hat = b.gp.hat,se.gp.hat = se,L = L)
all.equal(as.numeric(a$post.means[k,]),as.numeric(mupost))
all.equal(as.numeric(a$post.covs[k,]),as.numeric(diag((covpost))))

LVL=L%*%V.gp.hat%*%t(L)###redfine V.jhat as the marginal varianc of Lchat which is LVL'
LSigL=lapply(cov,function(x){L%*%x%*%t(L)})##redefine Sigma_p as a list of the variance of Lv (i.e., LUkL')
log.lik.snp=log.lik.func(b.mle,LVL,LSigL)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
  
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))
mash.means=read.table("jul22posterior.means.txt")[,-1]
colnames(mash.means)=levels(groups$bact)[-1]
all.equal(as.numeric(post.weights%*%a$post.means),as.numeric(mash.means[j,]))
```

And here, we plot the patterns of sharing:
```{r,echo=F}
library('gplots')
library('colorRamps')
barplot(colSums(matrix(pis[-433],byrow = T,ncol=18)))

for(k in 2:9){
  x=cov[[k]]
  colnames(x)=rownames(x)=levels(groups$bact)[-1]
  heatmap.2(x,revC=T,dendrogram="none",main=paste0("Uk",k),density.info = "none",trace="none",col=blue2green(256))
  }

```
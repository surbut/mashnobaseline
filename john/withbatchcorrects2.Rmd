---
title: "withbatchcorrectionandnoweights"
output: html_document
---

Let's try this out with john's data at time point 18, and notperform the centering in advnace. 

Critically, I'm going to use the batch corrected data from John to avoid having to add back in the intercept term
The weights don't make a sizeable difference, and the modeling with mean included is difficult if we need to remove the covariates for whom the mean will be partially assumed.


```{r setup, include=FALSE}
require(knitr)
knitr::opts_chunk$set(cache=TRUE)
opts_chunk$set(fig.path = "/Users/sarahurbut/Dropbox/PhDThesis/Figures/") 
```


REad in the batch corrected data. This includes the mean gene expression for each gene: 
```{r}
batchcorrectdata=read.table("~/Dropbox/oldnobaseline/john/s1table.txt",header = T,stringsAsFactors = F)[,-c(1,2)]
anno=read.table("~/Dropbox/oldnobaseline/john/annotation.txt",header=T)
library('limma')
```


Prepare the factors for use in the linear model.

```{r annotation}
groups <- anno[, c("ind", "bact", "time", "extr", "rin")]
groups$bact <- gsub("\\+", "plus", groups$bact)
groups$ind <- factor(groups$ind)
groups$bact <- factor(groups$bact, levels = c("none", "Rv", "Rvplus", "GC",
                                              "BCG", "Smeg", "Yers", "Salm",
                                              "Staph"))
groups$time <- factor(groups$time, levels = c(4, 18, 48))
groups$extr <- factor(groups$extr)
head(groups)
```
Prepare model_matrix
```{r}
design <- model.matrix(~ bact-1, data = groups)
# Clean up names of design matrix
colnames(design) <- gsub("bact", "", colnames(design))
dim(design)

beta_class <- matrix(0, dim(batchcorrectdata)[1], dim(design)[2]);
sebeta_class <- matrix(0, dim(batchcorrectdata)[1], dim(design)[2])
t_class = matrix(0, dim(batchcorrectdata)[1], dim(design)[2])
colnames(beta_class)=colnames(sebeta_class)=colnames(t_class)=colnames(design)
  
knownsamples=which(groups$time=="18")
colnames(batchcorrectdata)[10]
which(design[10,]==1)

for(k in c(1:9)){
  
  model_mat_temp <- cbind(design[knownsamples,k]);
  #vsarah <- voom(y[,knownsamples], model_mat_temp)##ccurrent model
  #vsarah2 <- voom(y[,knownsamples], model_mat)###total model
  #vjohn <- voom(y[,knownsamples], design[knownsamples,])###john'sdesign
  #limma.obj <- limma::lmFit(vsarah$E, model_mat_temp,weights=vsarah$weights);
  #limma.obj2 <- limma::lmFit(vsarah$E, model_mat_temp,weights=vsarah2$weights);
  #limma.obj3 <- limma::lmFit(vsarah$E, model_mat_temp,weights=johnsweights[,knownsamples]);
  ##ask why not the same as 
  l=lmFit(object = batchcorrectdata[,knownsamples],model_mat_temp)
  beta_class[,k] <- as.matrix(l$coefficients[,1]);
  sebeta_class[,k] <- l$sigma*(as.matrix(l$stdev.unscaled[,1]));
  t_class[,k] = (as.matrix(l$coefficients[,1]))/(l$sigma*(as.matrix(l$stdev.unscaled[,1])));
  }

##show that beta_class[j,k] ##roughly## corresponds to the average expression at gene j of samples from class k, without incorporation of weights
j=100
k=2
data=as.matrix(batchcorrectdata[,knownsamples])
mean(data[j,c(which(design[knownsamples,k]==1))])
beta_class[j,k]
```

Now we compute the average for each subgroup of the uncentered read counts at each gene. Here, voom_class_adj will not acutally be adjusted byt the mean.

```{r, echo=FALSE, eval=FALSE}
write.table(beta_class,"betabatchcorrected.txt",col.names = T)
write.table(sebeta_class,"sebetabatchcorrected.txt",col.names = T)
write.table(t_class,"tbatchcorrected.txt",col.names = T)
```

Check to make sure these are stored properly:
```{r,check}
j=8
k=3
lmFit(batchcorrectdata[,knownsamples],design[knownsamples,k])$coefficients[j,1]
beta_class[j,k]
sebeta_class[j,k]
t_class[j,k]
beta_class[j,k]/sebeta_class[j,k]
```

Now we compute factors on the centered T statistics:

```{r}
b=read.table("betabatchcorrected.txt")
se=read.table("sebetabatchcorrected.txt")
t=read.table("tbatchcorrected.txt")

###You can see uncentered###
hist(as.matrix(b),main="Uncentered Betas")
hist(as.matrix(t),main="Uncentered Ts")
R=ncol(t)
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
s.j=se/se


tcenteredfull=t(L%*%t(t))
hist(tcenteredfull,main="Centered T",nclass=100)
dim(tcenteredfull)
```

Critically,this should be of the same dimension as v, so use the full projection matrix. I don't use an lfsr threshold (instead I use a projected t cutoff) because we know that using the standard errors as the diagonal of $LVL'$ ignores the correlation terms.

```{r,eval=TRUE}
index=which(rowSums(abs(tcenteredfull)>2)>0)
strongprojectedt=t(L%*%t(t[index,]))
length(index)
##check to make sure rowSums 0
rowSums(strongprojectedt)[1:10]
```

```{r,echo=F,eval=F}
write.table(strongprojectedt,"strongprojectedtbatchcorrect.txt",col.names = F ,row.names = F)
```

Now, we need to project into the centered space to estimate the covariance matrix of the true deviations, using the full L since $v$ will be R, and not $R-1$.

```{r}
#system('/Users/sarahurbut/miniconda3/bin/sfa -gen strongprojectedtbatchcorrect.txt -g 856 -k 5 -n 8 i -o batchcorrect')
A="batchcorrect"

factor.mat=as.matrix(read.table("batchcorrect_F.out"))
lambda.mat=as.matrix(read.table("batchcorrect_lambda.out"))

```

Here are the matrices we will initialize BOVY with. Note that  t.stat is the matrix, MxR, of centered maximum t statistics.
```{r}
library('mashr')
initlist=init.covmat.single.with.projection(t.stat = strongprojectedt,factor.mat = factor.mat,lambda.mat = lambda.mat,Q = 5,P=3)
```

We check to see correctly initalised, e.g., empirical covariance of strong projected t, full rank factor mat, and svd.

```{r}
plot(initlist[1,,],cov(strongprojectedt))
M=nrow(strongprojectedt)
full.rank=lambda.mat%*%factor.mat
all.equal(as.numeric(initlist[2,,]),as.numeric((t(full.rank)%*%full.rank)/(M-1)))

P=3;X.c=apply(strongprojectedt,2,function(x) x-mean(x)) ##Column centered matrix of t statistics
svd.X=svd(X.c)
v=svd.X$v;u=svd.X$u;d=svd.X$d
cov.pc=1/M*v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P])%*%t(v[,1:P]%*%diag(d[1:P])%*%t(u[,1:P]))
all.equal(as.numeric(cov.pc),as.numeric(initlist[3,,]))

q=3
load=as.matrix(lambda.mat[,q])
fact=as.matrix(factor.mat[q,])
rank.prox=load%*%t(fact)
a=(1/(M-1)*(t(rank.prox)%*% rank.prox))
all.equal(as.numeric(a),as.numeric(initlist[3+q,,]))
```

Great, now let's run Bovy:

Recall that we'll need $w$ to be Mx(R-1) and the initialized covariacne matrices to be RxR, so we will use the full set of deviations to initatlise our covariance matrices, and 'train Bovy' for the scales on the centered maximum T that are $Mx(R-1)$,

Here are the initalizing t.stats,e.g., the MxR matrix of maximums across all R subgroups. Recall we need this $v$ to be based on $RxR$ covariance matrices of strong t.stats.

$$v~N(0,Uk)$$
```{r}
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
t.stat=(t(L%*%t(t[index,])))
dim(t.stat)
all.equal(t.stat,strongprojectedt)
```

Here will be the $ws$ and $lvls$, these need to be $R-1xR-1$ and of the transformed max $t$ statistcis, not $betahats$.
```{r}
L=L[-1,]
w=t(L%*%t(b))

#wt=t(L%*%t(t))
#wtmax=wt[index,]
#dim(wtmax)
lvlarray=genlvlarray(s.j = se[index,],L = L)
dim(lvlarray)
lvllist=genlvllist(s.j = se[index,],L = L)
length(lvllist)
dim(lvllist[[1]])
```

Currently, Bovy's R approach requires the S be diagonal (i.e., you put in a matrix of standard erros) when we know that w|Lv ~ N(Lv,LVL'), so we use our old covariance function with the matrices that we would have used to initalize Bovy (i.e., various rank approximations of the empirical covariance matrix of centered T statistics).


```{r,eval=T}
##here, t.stat, factor.mat and lambda.mat will be MxR, KxR, and MxR (not Rx1) and
###each matrix in the lvlarray will be R-1,R-1 and w Mx(R-1) 
##t.stat represents the MxR centered observed t statistics, w the Mx(R-1) matrix

L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
R=ncol(L)

# deconvolution.em.with.bovy.with.L(t.stat = t.stat,factor.mat = factor.mat,lambda.mat = lambda.mat,v.j = lvllist,P = 3,L = L[-1,],Q = 5,w=wtmax)




L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
t.stat=strongprojectedt
library("ExtremeDeconvolution")
##now we train on max centered chats

efunc=deconvolution.em.with.bovy.with.L(t.stat = strongprojectedt,factor.mat = factor.mat,lambda.mat = lambda.mat,v.j = lvllist,P = 3,L = L[-1,],Q = 5,w = w[index,])

Q=5;P=3;K=8
init.cov=init.covmat.single.with.projection(t.stat=t.stat,factor.mat = factor.mat,lambda.mat = lambda.mat,P=P,Q=Q)
init.cov.list=list()
for(i in 1:K){init.cov.list[[i]]=init.cov[i,,]}
mean.mat=matrix(rep(0,ncol(t.stat)*nrow(t.stat)),ncol=ncol(t.stat),nrow=nrow(t.stat))
  
  ydata=  w[index,]##make sure to train on maximum chats

  xamp= rep(1/K,K)
  xcovar= init.cov.list
  fixmean= TRUE     
  ycovar2=  lvllist  
  xmean=   mean.mat   
  projection=list();for(l in 1:nrow(t.stat)){projection[[l]]=L[-1,]}
  
  e=extreme_deconvolution(ydata=ydata,ycovar=ycovar2,xamp=xamp,xmean=xmean,xcovar=init.cov.list,fixmean=T,projection=projection)

sapply(seq(1:8),function(k){all.equal(as.numeric(e$xcovar[[k]]),as.numeric(efunc$true.covs[k,,]))})

```

Alternatively, we can generate using our old covmat function. Let's use the EE model, so we estimate the grid, fit and compute posteriors using $w$ as the transformed betahat.

```{r}

L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
allcenteredt=(t(L%*%t(t)))
maxt=(t(L%*%t(t[index,])))
all.equal(maxt,strongprojectedt)
##w should represent transformed betahats using all R subgroups and j genes
wfull=t(L%*%t(b))
sjmat=convertstandarderrors(s.j = se,L=L)##make this be the standard errors of all, it is the sqrt of the diagonal of LVL' for each j. Recall here L is still RxR.
dim(wfull)
dim(sjmat)
A="batchcorrect"
cov=compute.covmat(b.gp.hat = wfull,sebetahat = sjmat,Q = 5,t.stat = maxt,lambda.mat = lambda.mat,P = 3,A = "batchcorrect",factor.mat = factor.mat,bma = T,zero = T,power = 2)$cov
##you can see that these will all by 8x8##
dim(cov[[1]])
length(cov)

max.step=efunc
edcov=compute.hm.covmat.all.max.step(b.hat = wfull,se.hat = sjmat,t.stat = maxt,Q = 5,lambda.mat = lambda.mat,A = "witheffunc",factor.mat = factor.mat,max.step = max.step,zero = T,power = 2)$cov
```

Now, let's fit the model and compute posteriors on the reduced w:
```{r,eval=TRUE}
A="batchcorrectwithED"
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
R=ncol(L)
w=t(L[-1,]%*%t(b))
##which is just like removing a column of wfull
as.numeric(wfull[1,])
as.numeric(w[1,])
all.equal(w,wfull[,-1])
#compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = cov,A = A,pen = 1,L = L[-1,])
compute.hm.train.log.lik.pen.with.L(w,se.train = se,covmat = edcov,A = A,pen = 1,L = L[-1,])
pis=readRDS(paste0("pis",A,".rds"))$pihat


lik.mated=readRDS("liketrainbatchcorrectwithED.rds")
lik.matbatch=readRDS("liketrainbatchcorrect.rds")
pibatch=readRDS("pisbatchcorrect.rds")$pihat
pied=readRDS('pisbatchcorrectwithED.rds')$pihat
length(pied)
length(pibatch)

sum(log(exp(lik.mated)%*%pied))
test=exp(lik.mated)
total.lik.func(test = test,pis = pied)
sum(log(exp(lik.matbatch)%*%pibatch))


w=data.frame(w)
for(j in 1:nrow(w)){
## total.quant.per.snp.no.baseline(j = j,covmat = cov,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])
total.quant.per.snp.no.baseline(j = j,covmat = edcov,b.gp.hat = w,se.gp.hat = se,pis = pis,A = A,checkpoint = F,L = L[-1,])  }
```

Check
```{r}

A="batchcorrectwithED"

j=137
k=5
se.gp.hat=se
L=diag(R)-1/R*as.vector(rep(1,R))%*%t(as.vector(rep(1,R)))
R=ncol(L)
w=t(L[-1,]%*%t(b))
b.gp.hat=data.frame(w)
R=ncol(L)
L=L[-1,]
b.mle=as.vector(t(b.gp.hat[j,]))
K=length(c)
R=ncol(L)
V.gp.hat=diag(se.gp.hat[j,])^2
LVL=L%*%V.gp.hat%*%t(L)

pis=readRDS(paste0("pis",A,".rds"))$pihat
tinvlist=lapply(cov,function(x){solve(LVL+L%*%x%*%t(L))})
cov=edcov
mupost=post.mean.with.proj(b.mle = b.mle,tinv = tinvlist[[k]],U.k = cov[[k]],L = L)
covpost=post.cov.with.proj(tinv = tinvlist[[k]],U.k = cov[[k]],L = L)
a=post.array.per.snp.no.baseline(j = j,covmat = cov,b.gp.hat = b.gp.hat,se.gp.hat = se,L = L)
all.equal(as.numeric(a$post.means[k,]),as.numeric(mupost))
all.equal(as.numeric(a$post.covs[k,]),as.numeric(diag((covpost))))

LVL=L%*%V.gp.hat%*%t(L)###redfine V.jhat as the marginal varianc of Lchat which is LVL'
LSigL=lapply(cov,function(x){L%*%x%*%t(L)})##redefine Sigma_p as a list of the variance of Lv (i.e., LUkL')
log.lik.snp=log.lik.func(b.mle,LVL,LSigL)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))
#mash.means=read.table("batchcorrectposterior.means.txt")[,-1]
mash.means=read.table("batchcorrectwithEDposterior.means.txt")[,-1]
colnames(mash.means)=levels(groups$bact)[-1]
all.equal(as.numeric(post.weights%*%a$post.means),as.numeric(mash.means[j,]))

library('gplots')
library('colorRamps')
x=as.matrix(mash.means)

# png("estimatedeffectsJohn.png")
# heatmap.2(x/max(diag(x)),Rowv = NULL,Colv = NULL,dendrogram = "none",col=blue2green,density.info = "none",trace="none",main=paste0("EstimatedEffects, MASH"))
# dev.off()
```

And here, we plot the patterns of sharing:
```{r,echo=F}
library('gplots')
library('colorRamps')
barplot(colSums(matrix(pis[-433],byrow = T,ncol=18)))

for(k in 2:9){
  x=cov[[k]]/max(diag(cov[[k]]))
  colnames(x)=rownames(x)=levels(groups$bact)[-1]
  heatmap.2(x,revC=T,dendrogram="none",main=paste0("Uk",k,round(colSums(matrix(pis[-433],byrow = T,ncol=18))[k],2)),density.info = "none",trace="none",col=blue2green(256))
  }

```

Let's also plot pairwise sharing by magnitude and sign:



```{r pairwisesharingmnb}
library("mashr")
post.means=read.table("batchcorrectwithEDposterior.means.txt")[,-1]
lfsr.mash=read.table("batchcorrectwithEDlfsr.txt")[,-1]
se.matched=as.matrix(read.table("sebetabatchcorrected.txt"))[,-1]
strain.names=read.table("StrainNames.txt",header = T,stringsAsFactors = F)[,1]
pm.mash.beta=post.means*se.matched
colnames(pm.mash.beta)=colnames(lfsr.mash)=strain.names
lfsr.mash.sig=lfsr.mash[rowSums(lfsr.mash<0.05)>0,]##only 137,223 are significant in at least one subgroup
pm.mash.sig=pm.mash.beta[rowSums(lfsr.mash<0.05)>0,]



library(RColorBrewer)
p=colorRampPalette(brewer.pal(9,"Greens"))(100)
g=colorRampPalette(brewer.pal(9,"Blues"))(100)

signheatmap=compute.sharing.by.sign(lfsr.mash = lfsr.mash.sig,thresh = 0.05,pm.mash.beta = pm.mash.sig)
signheatmap[lower.tri(signheatmap)] <- NA
magheatmap=compute.mag.by.sharing(lfsr.mash = lfsr.mash.sig,thresh = 0.05,pm.mash.beta = pm.mash.sig)
magheatmap[lower.tri(magheatmap)] <- NA


library('colorRamps')
library('corrplot')
library(gplots)
library(ggplot2)

class(signheatmap)



library('lattice')

clrs <- colorRampPalette(rev(c("#D73027","#FC8D59","#FEE090","#FFFFBF",
                               "#E0F3F8","#91BFDB","#4575B4")))(64)
#clrs[63:64] <- "darkviolet"
lat=signheatmap
lat[lower.tri(lat)] <- NA
print(levelplot(lat,col.regions = p,xlab = "",ylab = "",colorkey = TRUE,main="PairwiseSharingBySign"))


heatmap.2(magheatmap,Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("Pairwise Sharing by Magnitude"),
          cexRow=0.6,cexCol=2,cex.main=0.5,#breaks=seq(0.35,1,0.01),
          labCol=NA)

library('lattice')

clrs <- colorRampPalette(rev(c("#D73027","#FC8D59","#FEE090","#FFFFBF",
                               "#E0F3F8","#91BFDB","#4575B4")))(64)
#clrs[63:64] <- "darkviolet"
absmagheatmap=compute.mag.by.sharing(lfsr.mash = lfsr.mash.sig,thresh = 0.05,pm.mash.beta = abs(pm.mash.sig))
absmagheatmap[lower.tri(absmagheatmap)] <- NA
lat=absmagheatmap
lat[lower.tri(lat)] <- NA
print(levelplot(lat,col.regions = g,
                xlab = "",ylab = "",colorkey = TRUE,main="PairwiseSharingByMagnitude"))
```

Now, let's look at sharing my absolute value of magnitude:
```{r}

heatmap.2(signheatmap,Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("Pairwise Sharing by Sign"),
          cexRow=0.6,cexCol=0.5,cex.main=0.5)
#,breaks=seq(0.7,1,0.01))

heatmap.2(1-signheatmap,Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("Pairwise Sharing by Opposite Sign"),
          cexRow=0.6,cexCol=0.5,cex.main=0.5)
#,breaks=seq(0.7,1,0.01))



absmagheatmap=compute.mag.by.sharing(lfsr.mash = lfsr.mash.sig,thresh = 0.05,pm.mash.beta = abs(pm.mash.sig))
absmagheatmap[lower.tri(absmagheatmap)] <- NA
heatmap.2(absmagheatmap,Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("Pairwise Sharing by abs(Magnitude)"),
          cexRow=0.6,cexCol=2,cex.main=0.5,#breaks=seq(0.35,1,0.01),
          labCol=NA)

heatmap.2(magheatmap,Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("Pairwise Sharing by Magnitude"),
          cexRow=0.6,cexCol=2,cex.main=0.5,#breaks=seq(0.35,1,0.01),
          labCol=NA)
```






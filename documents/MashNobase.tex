\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 

\usepackage{amsmath}

% EPS and PDF figures
\usepackage{graphicx}

% EPS and PDF figures
%\usepackage[nomarkers,figuresonly]{endfloat}


\usepackage{amssymb}
\usepackage{bm}
% \graphicspath{{./figures/}} % save all figures in the same directory
\usepackage{color} 
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\newcommand{\Prob}{\mathbb{P}} % symbol for proba
\newcommand{\Prd}{\mathsf{P}} % symbol for discrete proba
\newcommand{\Exp}{\mathbb{E}} % symbol for expectation
\newcommand{\Var}{\mathbb{V}} % symbol for variance
\newcommand{\Cov}{\mathbb{C}} % symbol for covariance
\newcommand{\Norm}{{\mathcal{N}}} % symbol for Normal distribution
\newcommand{\BF}{{\text{BF}}} % symbol for Bayes factor
\newcommand{\Lik}{{\mathcal{L}}} % symbol for likelihood
\newcommand{\beff}{\bm{b}}
\newcommand{\ceff}{\bm{C}}
\newcommand{\wfit}{\bm{w}}
\newcommand{\onemat}{\bm{1}\bm{1}'}
\newcommand{\chat}{\bm{\hat{C}}}
\newcommand{\zhat}{\hat{Z}}
\newcommand{\vb}{\bm{v}}
%\def\lstar{{\it L*}}
\def\lstar{\text{ L}}

\def\lfsr{{\it lfsr}}
\def\lfdr{{\it lfdr}}
\def\mash{{\tt mash }}
\def\mnb{{\tt mashnobaseline }}
\def\bnorm{\tilde{b}}
\def\bhat{\hat{b}}
\def\vbhat{\bm \bhat}

\def\shat{\hat{s}}
\def\vshat{\bm{\shat}}
\def\Bhat{\hat{B}}
\def\B{B}
\def\pmean{\hat{\hat{b}}}
\def\L{L}
\def\F{F}
\def\CI{\text{CI}}
\nonstopmode

\title{ $\mash$ No Baseline}
\author{Sarah Urbut}
%\date{}							% Activate to display a given date or no date

\date{\today}
\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Purpose}

The purpose of this document is to propose a method for extending $\mash$ to estimate `true' effects across conditions in a setting in which no obvious baseline exists. We assume that we observe noisy, uncentered averages $\chat_{jr}$ in each of $R$ conditions, and seek to estimate the underlying true `deviations` from average measurement across conditions and can be seen as the effects in $\mash$.

Here, the use of bold-face notation indicates a vector, while matrix quantities are typeset in capital but unboldface letters.

%The purpose of this document is to express a way of selecting a set of covariance matrices for fitting a mixture of multivariate normals. Previously, we have used a fixed set of $U_k$ to represent the prior covariance matrices on the vector of `true` effects across tissues, $\bm{b}$. We then estimate the weights on each of these matrices $\pi$ hierarchically, using the EM algorithm. Here, we propose to estimates these covariance matrices simultaneously, thus reflecting the ideal patterns of covariance present in the data. We then rescale these matrices before, as in $\mash$. For the


\section{Defining the Model}

%For a given gene-snp pair, $\bm{b}$ represents the $R$ vector of unknown standardized effect. We model the prior distribution from which $\bm{b}$ is drawn as a mixture of multivariate {\it Normals}.
% 
% \begin{equation}
%  \label{prior_b_mixt_grid}
%  \bm{b} | \bm{\pi},\bf{U} \sim \sum_{k,l} \pi_{k,l} \;{\it N}_R(\bm{0}, \omega_l U_{k})
%\end{equation}
%
%Furthermore, for a given gene-snp pair, the Likelihood on $\bm{b}$: 
%\begin{equation}
%  \label{new_lik}
%  \hat{\bm{b}} | \bm{b} \sim {\it N}_R(\bm{b}, \hat{V})
%\end{equation}

Now, we observe for each gene $j$ a vector of uncentered noisy average feature expression $\chat$ across R conditions:

\begin{equation}
  \label{nobaselike}
\chat | \bm{\ceff} \sim {\it N}_R(\bm{\ceff}, \hat{V})
\end{equation}

where the `true' uncentered averages $\ceff$ can be written as follows:

\begin{equation}
  \label{uncenteredprior}
\ceff | \mu, \vb  = \mu \bm{1} + \vb
\end{equation}

Where \mu is a scalar that is the mean of the `true' uncentered averages \ceff.

\vb is a zero-centered mixture of multivariate normals:


 \begin{equation}
  \label{prior_v_mix}
  \vb | \bm{\pi},\bf{U} \sim \sum_{k,l} \pi_{k,l} \;{\it N}_R(\bm{0}, \omega_l U_{k})
\end{equation}

Critically, our quantity of interest now, $\vb$  represents the true `deviations` from average gene expression across each condition and can be seen as the effects in \mash{}.

Thus:

Let 

\begin{equation}
L \ceff = L \mu \bm{1} + L \vb
\end{equation}

L is the $RxR$ centering matrix $L_{r}=I_{r}-{\tfrac  {1}{r}}{\mathbf  {1}}{\mathbf  {1}}^{\top }$ which removes the mean of each R column vector.

Then :

\begin{equation}
\label{model}
\begin{aligned}
L \ceff &= L \mu \bm{1}+ L \vb \\
L \ceff &= 0 + L \vb \\
L \chat &= L \vb + E \\
\end{aligned}
\end{equation}

Where $E \sim \Norm (0, L\hat{V} L')$. Note that this is identical to Bovy equation (1). 




\section{Applications}

We will again apply a two-step process to our selection of covariance matrices, where we select a set of denoised `pattern` matrices $U_{k}$ by using the EM algorithm on the max effects across conditions,
and then expanding this list by a fixed grid of scalar weights $\omega_{l}$ such that we conclude with a list of $P=KxL$ covariance matrices $\Sigma$. We can then:
\begin{itemize}
\item estimate the $P$ prior weights $\bm\pi$ on this fixed P-list of covariance matrices from a training matrix of randomly selected feature expression measurements across conditions
\item compute the posterior distribution $\vb | L \chat, \bm{s_{j}}$
\end{itemize}
Let 

\begin{equation}
L \ceff = L \mu \bm{1} + L \vb
\end{equation}

L is the $RxR$ centering matrix $L_{r}=I_{r}-{\tfrac  {1}{r}}{\mathbf  {1}}{\mathbf  {1}}^{\top }$ which removes the mean of each R column vector.

Then :

\begin{equation}
\label{model}
\begin{aligned}
L \ceff &= L \mu \bm{1}+ L \vb \\
L \ceff &= 0 + L \vb \\
L \chat &= L \vb + E \\
\end{aligned}
\end{equation}



Where $E \sim \Norm (0, L\hat{V} L')$. Note that this is identical to Bovy equation (1). 





\section{Likelihood with \mnb{}}

With \mnb{}, we will replace the RxR matrix $L$ with the R-1xR matrix \lstar, effectively removing a data point from the observed uncentered statistics, such that the rank of the marginal variance of \wfit is guaranteed to be equal to the dimension of \wfit.

Now for each gene J at each component k, integrating over $\vb$, 


\begin{equation}
\begin{aligned}
\label{maxlike}
$\lstar$ \ceff \sim \Norm (0, $\lstar$ U_{k} $\lstar$ ') \\
$\lstar$ \chat \sim \Norm (0, $\lstar$  U_{k} $\lstar$ ' + $\lstar$ \hat{V} $\lstar$ ') 
\end{aligned}
\end{equation}

Note that because $\lstar$ removes one rank from every vector, adding a rank 1 matrix ($\onemat$) to  $U_{k}$ will not change the likelihood.

And thus we can use the Bovy et al algorithm invoked in both the Extreme Deconvolution package {ref:%and in `Sarah's MixEm' 

where according to Bovy's language we observe a noisy estimate of uncentered averages $\chat$ and can center them to and $w_{j} = $\lstar$ \chat_{j}$.

Then we can apply Bovy:

\begin{equation}
\begin{aligned}
%\Lik \pi
\textbf{w} = $\lstar$ \chat \\
\textbf{w} | \vb, \hat{V} \sim \Norm ($\lstar$ \vb, $\lstar$ \hat{V}  $\lstar$ ') \\
  \vb | \bm{\pi},\bf{U} \sim \sum_{k,l} \pi_{k,l} \;{\it N}_R(\bm{0}, \omega_l U_{k})\\
\end{aligned}
\end{equation}
%
%
%
%Recall that our previous approach was simplified by the fact that $\bm{w}_{j}$ was simply $\hat{\bm{b}_{j}}$ and the projection matrix was simply the $I_{r}$ identity matrix. Our inference on $\bm{b}$ was analogous to their inference on $\vb_{j}$. 
%

As before, we are interested in returning the prior covariance $U_k$ matrices of the `true' deviations $\vb$ that maximizes (\ref{cdllone}):

\begin{equation}
\begin{aligned}
L(\theta ) &:= p(\textbf{w} | \pi, V, U_k) \\
 &= \prod_{j=1}^J p(\textbf{w}_j | \pi, V, U_k) \\
%&= p(\hat{\bm{b}} | \bm{s},\bpi) \\
%&= \prod_{j=1}^J \sum_{p}^{P} \pi_{p} P(\hat{\bm{b}}_{j} | \hat{\bm{s}}_{j}, z_{j}=p)\\
&=  \prod_{j=1}^J \sum_{k}^{K} \pi_{k} {\it N}_R(\textbf{w}_j; \bm{0}, \lstar U_k \lstar '+ \lstar V_{j} \lstar ').\\
\label{cdllone}
\end{aligned}
\end{equation}

Identical to the framework in Bovy. 

We will then rescale each of the $U_k$ by choosing a set of $\omega$ that are appropriate to $L \chat $ to comprise a set of $P = KxL$ prior covariance matrices $\Sigma$ that maximizes :

\begin{equation}
\begin{aligned}
L(\pi) &:= p(\textbf{w} | \bpi, V, \Sigma  \\
 &= \prod_{j=1}^J p(\textbf{w}_j | \pi, V, \Sigma ) \\
%&= p(\hat{\bm{b}} | \bm{s},\bpi) \\
%&= \prod_{j=1}^J \sum_{p}^{P} \pi_{p} P(\hat{\bm{b}}_{j} | \hat{\bm{s}}_{j}, z_{j}=p)\\
&=  \prod_{j=1}^J \sum_{p}^{P} \pi_{p} {\it N}_R(\textbf{w}_j; \bm{0}, \lstar \Sigma_{p} \lstar '+ \lstar V_{j} \lstar ').
\label{eq:cdll}
\end{aligned}

\end{equation}
%
%and choose the set of $\pi$ and covariance matrices $ that maximizes:
%
%
%\begin{equation}
%
%\Lik 
%\end{equation}
We assemble a matrix of likelihoods that will compute the following likelihood at each of the P components: 

\begin{equation}
\begin{aligned}
$\lstar$  \chat _j \sim \Norm (0, $\lstar$  \Sigma_{p} $\lstar$ ' + $\lstar$ \hat{V_j} $\lstar$ ') \\
T_{jp} = $\lstar$ \Sigma_{p}  $\lstar$' + $\lstar$ \hat{V}_{j} $\lstar$'\\
$\lstar$  \chat _j  \sim \Norm (0, T_{jp} )
\end{aligned}
\end{equation}
 


\section{Posteriors}

Now, as before we can compute a posterior distribution such that:

\begin{equation}
\vb | \lstar \chat, \pi, \Sigma, \bm{s} \sim N(\mu^{1} , U^{1})
\end{equation}

Where at each of the P components for each gene J 

\begin{equation}
\begin{aligned}
\mu^{1}_{jp} = \Sigma_{p} \lstar ' T_{jp}^{-1} \lstar  \chat_{j} \\
U^{1}_{jp} = \Sigma_{p} - \Sigma_p \lstar ' T_{jp}^{-1} \lstar  \Sigma_{p}
\end{aligned}
\end{equation}

Analogous to Bovy et al equation 13 and 14, because our $\vb_{jk}$ is centered at 0 (and so their $\bf{m}_j$ is effectively 0 for all components. 

Our question is, if you were to add the matrix of 1s (i.e., $\onemat$ ) to each $\Sigma_p$,  $\mu^{1}_{jp}$ doesn't change. 

\begin{equation}
\begin{aligned}
\mu^{1}_{jp} ( \Sigma_p ) = \Sigma_{p} \lstar ' T_{jp}^{-1} \lstar  \chat_{j} 
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\mu^{1}_{jp} ( \Sigma_p + \onemat) &= \Sigma_{p} \lstar ' T_{jp}^{-1} \lstar  \chat_{j} +  \onemat \lstar ' T_{jp}^{-1} \lstar  \chat_{j}  \\
 &= \Sigma_{p} \lstar ' T_{jp}^{-1} \lstar  \chat_{j} +  \bm{0}' T_{jp}^{-1} \lstar  \chat_{j}  \\
&= \Sigma_{p} \lstar ' T_{jp}^{-1} \lstar  \chat_{j}  \\
\end{aligned}
\end{equation}


And thus we see that $\mu^{1}_{jp} ( \Sigma_p ) =  \mu^{1}_{jp} ( \Sigma_p + \onemat)$ if the projection matrix is equal to the centering matrix $\lstar$. But this result seems strange!


This is correct. We can see that in Bovy (11) and (12), to derive the posterior condition distribution of $\vb | \wfit$, he uses the following formula: 

If x is partitioned as follows:

\mathbf{x}
=
\begin{bmatrix}
 \mathbf{x}_1 \\
 \mathbf{x}_2
\end{bmatrix}

and accordingly  $\boldsymbol\mu$ and $\boldsymbol\Sigma$ are partitioned as follows

\boldsymbol\mu
=
\begin{bmatrix}
 \boldsymbol\mu_1 \\
 \boldsymbol\mu_2
\end{bmatrix}


\begin{bmatrix}
 \boldsymbol\Sigma_{11} & \boldsymbol\Sigma_{12} \\
 \boldsymbol\Sigma_{21} & \boldsymbol\Sigma_{22}
\end{bmatrix}

\boldsymbol\mu
=
\begin{bmatrix}
 \boldsymbol\mu_1 \\
 \boldsymbol\mu_2
\end{bmatrix}



\boldsymbol\Sigma
=
\begin{bmatrix}
 \boldsymbol\Sigma_{11} & \boldsymbol\Sigma_{12} \\
 \boldsymbol\Sigma_{21} & \boldsymbol\Sigma_{22}
\end{bmatrix}

then the distribution of $\mathbf{x}_1$ conditional on $\mathbf{x}_2$ = $\bf{a}_1$ is multivariate normal 

\begin{equation}
\mathbf{x}_1 | \mathbf{x}_2=a \sim N(\bar{\boldsymbol\mu}, \overline{\boldsymbol\Sigma})
\end{equation}

\begin{equation}
\bar{\boldsymbol\mu}
=
\boldsymbol\mu_1 + \boldsymbol\Sigma_{12} \boldsymbol\Sigma_{22}^{-1}
\left(
 \mathbf{a} - \boldsymbol\mu_2
\right)
\end{equation}

and covariance matrix 

\begin{equation}
\overline{\boldsymbol\Sigma}
=
\boldsymbol\Sigma_{11} - \boldsymbol\Sigma_{12} \boldsymbol\Sigma_{22}^{-1} \boldsymbol\Sigma_{21}. 
\end{equation}

Plugging in our matrices at any given component, we have that:


\begin{center}
\begin{equation}
 \mathbf{x}_1 = \vb \\
 $\mathbf{x}_2 = \mathbf{a}$ = \wfit\\
   \boldsymbol\mu_1 = \boldmath{0}\\
    \boldsymbol\mu_2 = \boldmath{0}\\
 \boldsymbol\Sigma_{11} = \Sigma \\
  \boldsymbol\Sigma_{22} = T \\
\boldsymbol\Sigma_{12} = \Cov(\vb \wfit ) = \Sigma \lstar'\\
\end{equation}
\end{center}

The equation below: 

\begin{equation}
 \Cov(\vb \wfit ) = \Sigma \lstar'
 \end{equation}
 
 Is the case because  $\vb \indep \mathbf{E}$:

\begin{center}
\begin{equation}
\begin{aligned}
\Cov(\vb \wfit ) &=  \Cov(\vb , \lstar \vb  + \mathbf{E}) \\
&= \Cov(\vb , \lstar \vb ) +\Cov(\vb , \mathbf{E}) \\
&= \Cov(\vb , \lstar \vb ) + \mathbm{0} \\
&= \Sigma \lstar' \\
\end{aligned} 
\end{equation}
\end{center}


And so this turns out our formula for $\mu^{1}$ and $U^{1}$:

\begin{center}
\begin{equation}
\mu^{1} ( \Sigma ) = \Sigma \lstar ' T^{-1} \wfit  \\
U^{1} ( \Sigma_p ) = \Sigma - \Sigma \lstar ' T^{-1} \lstar  \Sigma_{p}\\
\end{equation}
\end{center}

\subsection{Fundamental Problem}

Using equation 4.7, we can see that L(U) = L(U+a11') and thus U is not identifiable. However, from the equation above, we can see that U matters in the computation of the posterior

\subsection{Selecting The Covariance Matrices}

We initiate our set of covariance matrices for the denoising step as before in \mash{}, where now we compute the empirical covariance matrices and a variety of dimensional reductions on the feature-centered JxR \textit{matrix} of maximum average, $L \hat{C}' $ instead of $\hat{C}$ alone. In practice, we actually use the matrix of maximum uncentered $T$ statistics. Three critical things to note:

\begin{enumerate}
\item Here, L will be $RxR$ because we need $U_{k}$ to be RxR
\item When denoising with Bovy, our previous approach used the matrix of maximum $T$ statistics $T_{Mxr}$ to both initialize and train the BovyEM. Now, we will initialize with the MXR matrix of t($L_{R,R}T')$ (or alternatively, $TL$) and train the EM on the MxR-1 matrix of maximum $t($\lstar$T')$
\item When choosing $\omega$, we will use the diagonal of $L V L'$, where $V$ is $D(s.j^2)$, and select from the MxR matrix of centered T statistics and their centered standard errors.
\item We will choose the maxes as those that have a maximum centered t statistic of at least some threshold in at least one (or averaged across tissues) rather than those that satisfy an ash criteria in at least one tissue because we know that choosing uncorrelated $L V L'$ as the standard errors with which to input to ash  is incorrect.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Previous Model}

If we were to apply the old \mash{} to data simulated as in (\ref{model}) in which we assumed the mean was known, we would write

\begin{equation}
\label{mashmodel}
\begin{aligned}
\ceff | \mu, \vb  &= \mu \bm{1} + \vb \\
\chat &= \mu + \vb + E \\
\chat - \mu &= \vb + E \\
\end{aligned}
\end{equation}

where $E \sim \Norm (0, V)$

So using our previous notation, 

\begin{equation}
\label{mashlik}
L \chat \sim \Norm(\vb,V)
\end{equation}


\textbf{Critically, note that here $L$ is $R$x$R$, not $R-1$ x $R$ because this model does not require that the likelihood of $w$ be non-degenerate}. 

Here, the likelihood of \wfit = $L$ \chat$ integrating over $\vb$ is: 

\begin{equation}
\label{\mashlik}
\wfit_{R}  \sim N_{R} (0, V + U)
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Comparing Models}


In the new model, we incorporate the centering matrix $L$ into our selection of covariance matrices by modeling $L\chat = L \vb + E$ where $E \sim N(0,LVL')$. \

Using old $\mash{}$, we assume that the mean centered estimates \textbf{directly approximate} \vb, that is $\chat-\mu$ = $\vb + E$ where $E \sim N(0,V)$. 
We want to estimate effects with no baseline, such that null effects aren't forced to be negative (by a competitive default, given that non-null effects are seen as positive, or vice versa). We hope that this method will simply shrink null effects to zero and recognize the direction of the non-null effects, instead of making both null and non-null positive (negative) relative to some intermediate average.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Intuition}
%
%Consider the situation in which we observe uncentered expression \chat = \begin{bmatrix} 1 & 4 &1 \end{bmatrix}
%
%We want to produce estimates which recognize that $\vb$ is \begin{bmatrix} 0 & 3 &0 \end{bmatrix},  not  $\begin{bmatrix} -1 & 2 & -1 \end{bmatrix}$. 
%
%We hope to improve upon the measurement $L \chat \begin{bmatrix} -1 & 2 & -1 \end{bmatrix}$ as an approximation of $\vb$ so that rather than 
%
%\begin{equation}
%\begin{aligned}
%L \chat \sim N (v,E)\\
%\lstar \chat \sim N(L\vb,E)\\
%%\end{equation}
%\end{aligned}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary of Discoveries}

\begin{itemize}

\item When using \mash and \mnb to generate inference matrices and compute likelihoods and posteriors, respectively, \mnb 
seems no better (see ../Simulations/fixedomegawithinference/Untitled.html).

\item  When we use the true prior for both frameworks, \mnb is much better 
(see ../Simulations/Withbothtrueandestimatedprior.html) because we can see that using the same set of covariance matrices, \mnb is better able to emphasize the true configurations because the likelihood and posterior computations incorporate L.

\item Recall, for \mash{}:
$$w_{j,R} ~ N(0,V+U)$$
$$w_{j,R-1} ~ N(0,LVL' + LUL')$$ where L is the R-1 x R centering matrix.

In both:

$$c = \mu + \beta$$
$$chat = c + E$$
$$v \sim \sum_p N(0,Uk)$$
$$E \sim N(0,V)$$

So even if the $U_ks$ are different, the likelihood and corresponding $\pi$s will be different.



%In our old code, L was just the identity:
%
%\begin{equation}
%\begin{aligned}
%\mu^{1}_{jp} = \Sigma_{p}  T_{jp}^{-1}  \chat_{j} \\
%U^{1}_{jp} = \Sigma_{p} - \Sigma_p  T_{jp}^{-1}  \Sigma_{p}
%\end{aligned}
%\end{equation}
%
%and was equivalent to :
%
%\begin{equation}
%\begin{aligned}
%U^{1}_{jp}  = (\Sigma_{p}^{-1} + \hat{V}_{j}^{-1})^{-1}\\
%\mu^{1}_{jp} = U^{1}_{jp} (\hat{V}_{j}^{-1} \bm{w})
%\end{aligned}
%\end{equation}
%
%
%
%If we now replace 
%
%\begin{equation}
%\begin{aligned}
%\Sigma*_{p} &= L \Sigma_{p} L' \\
%\hat{V}* &=  L\hat{V_j} L'\\
%\bm{w}&=L \chat_{j}
%\end{equation}
%\end{aligned}
%
%
%And let the new $L$ be the identity, we would get the following:
%
%\begin{equation}
%\begin{aligned}
%\mu^{1}_{jp} = \Sigma*_{p} T_{jp}^{-1} L \chat_{j} \\
%U^{1}_{jp} = \Sigma*_{p} - \Sigma*_p T_{jp}^{-1}  \Sigma*_{p}
%\end{aligned}
%\end{equation}
%
%But as you can see that isn't going to work because in the Bovy equation, $\Sigma$ should represent the prior covariance of the '`true` deviation v, while here, $\Sigma*$ represent the covariance of $\ceff$, $L\bv$ which is $L \Sigma* L$
%
%Then we can return our old posteriors:
%
%\begin{equation}
%\begin{aligned}
%U^{1}_{jp}  = (\Sigma_{p}^{-1} + \hat{V}_{j}^{-1})^{-1}\\
%\mu^{1}_{jp} = U^{1}_{jp} (\hat{V}_{j}^{-1} \bm{w})
%\end{aligned}
%\end{equation}
%
%And 
%\begin{equation}
%\vb \sim \sum_p \tilde{\pi}_{p}   \Norm ( \mu_{p}^{1}, U_{p}^{1})
%\end{equation}
%
%Where 
%\begin{equation}
%\tilde{\pi}_{jp} = \frac{\Norm(L\chat;0, T_{jp})}{\sum_{p} \Norm(L\chat;0, T_{jp})}
%\end{equation}

\section{Differences required over \mash{} implementation}

\begin{itemize}
\item We will now work with a matrix of observed column-centered gene averages, $L \hat{C} '$ in order to:
\begin{enumerate}
	\item initialize our choice of $U_{k}$;
	\item  choose the maxes by which to denoise, 
	\item choose our set of scales, $\omega_{l}$
	\item compute our hierarchical weights, $\bm{\pi}_{p}$ as well as our posteriors. 
\end{enumerate}
\item It is critical to note that here \textbf{$L$ will need to be RxR because $U_{k}$ must be RxR}
\item The new distribution we seek to estimate for each j is then $v | $\lstar$ \chat , \bm{s}_{j}$
\item To choose the maxes, I think we ought to use a $w_{j}$ cutoff since computing the univariate lfsr on $w_{j}$ and the diagonal of $LVL'$ assumes that $LVL'$ is diagonal when we know it cannot be.
\item For the \mash{} implementation, the new distribution we seek to estimate for each j is then $v | L \chat , \bm{s}_{j}$
\item We proceed as before and as in $\mnb$, feeding in the matrix of centered summary statistics and this time, the matrix of standard errors (rather than the list of $\lstar$ V $\lstar$' arrays)

\end{itemize}

%\section
%
%\begin{itemize}
%\item We will now work with a matrix of observed column-centered gene averages, $L \hat{C} '$ in order to:
%\begin{enumerate}
%	\item initialize our choice of $U_{k}$;
%	\item  choose the maxes by which to denoise, 
%	\item choose our set of scales, $\omega_{l}$
%	\item compute our hierarchical weights, $\bm{\pi}_{p}$ as well as our posteriors. 
%\end{enumerate}
%\item It is critical to note that here \textbf{L will need to be RxR because $U_{k}$ must be RxR}
%\item Now, we ignore $\lstar$ and continue with L because we see that \ref{mashlik} only requires the matrix of centered average gene expression
%\item The new distribution we seek to estimate for each j is then $v | L \chat , \bm{s}_{j}$
%\item We proceed as before and as in $\mnb$, feeding in the matrix of centered summary statistics and this time, the matrix of standard errors (rather than the list of $\lstar$ V $\lstar$' arrays)
%\end{itemize}

%Some questions:
%
%\begin{itemize}
%\item How is using $\bm{w}_{j}$ as $L\chat_{j}$ as the data from which we estimate our model different than initializing with the vectors of $\hat{\bm{b}}$ that have been computed on $\textit{feature centered data}$?
%\item  I think it is because we are now broadly incorporating the centering information into our prior on $L \ceff$ as well, such that at each component for each gene, $L \ceff_{j} \sim \Norm (0, L \Sigma_{jp} L')$.
%\item If this is the case, we will probably need to denoise all of the initiation matrices (not just the multirank Uk) since previously our projection matrix was the Identity
%\item Since our input will still be that matrix   $\hat{C}$  of uncentered noisy averages and their standard errors, our scaling parameter $\omega$ ought to be chosen consistent with $L \hat C$, and not $\hat C$, since this will tend to scale with the true deviations $\vb_{j}$. 
%\item In practice, it probably won't matter if we use the $$\lstar$ $ or $L_{R,R}$ to choose the scaling factors, but it becomes a bit tricky when we specify the standard errors as the diagonal of $$\lstar$ V $\lstar$ '$, since we know this marginal covariance is not really diagonal.
%\end{itemize}

%\subsection{E-Step}
%For a data set with J gene snp pairs and $K$ components:
%\begin{itemize}
%\item $U_{k}$ to represent the `true' covariance matrix of effects,
%\item$B_{jk}$ to represent the $RxR$ posterior conditional covariance matrix for each gene-snp pair at each component ($U_{1}$ above) 
%\item$\bm{b_{jk}}$ to represent the $R$-dimensional posterior mean for each gene-snp pair at each component (analogous $\bm{\mu_{1}}$) above.
%\item $\pi_{k}$ to represent the mixture proportions.
%\end{itemize}
%
%In the E- step, using the same notation as the authors Bovy et al where $q_{jk}$ represents the latent 'label' of each gene-snp pair according to its membership:
%
%\begin{equation}
%\begin{align*}
%q_{jk} &= \frac{\pi_{k} N (  \hat{\bm{b}} |0,U_{k}+V_{j})}{\sum_{k}{\pi_{k} N (\hat{\bm{b_{j}}}|0,U_{k}+V_{j}})} \\
%\bm{b_{jk}} &=  B_{jk} (U_{k}^{-1} \bm{m_{k}}+\hat{V_{j}}^{-1}  \hat{\bm{b}}) \\
%B_{jk}&=(U_{k}^{-1} + \hat{V_{j}^{-1}})^{-1}
%\end{align*}
%\end{equation}
%
%Quite simply, the latent indicator label is simply the likelihood at a particular component times the current update of the prior weight $\pi_{k}$ at that component, divided by the marginal probability of observing that gene snp pair. This is equivalent the posterior probability that a data point $j$ arose from component $K$. Note that the distribution $\hat{\bm{b_{j}}}|0,U_{k}+V_{j}})} $ results from integrating over the uncertainty in $\bm{b}_{j}$ and thus represents the variance of the marginal distribution of $\hat{b}$, the T used in ${\it Bovy et al}.$
%
%
%The current component specific posterior covariance $B_{jk}$ is the posterior covariance matrix of a single multivariate normal distribution and the current component-specific posterior mean $\bm{b_{jk}}$ is the posterior mean of a single multivariate normal. (see \href{http://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions}{Wikipedia})
%
%Note that this is slightly different than our expression for the posterior conditional mean $\bm{\mu_{1k}}$ above because in the previous model, we assumed that each $\bm{b_{j}} \sim N(0,U_{k})$ (i.e., the prior mean was $\bm{0}$) where here we estimate the underlying mean for each component, $\bm{m_{k}}$ using the EM algorithm and so we need to use the full formula for a multivariate normal with known residual matrix $V_{j}$ (please see section: Posteriors on genotype effect sizes for algebraic derivation).
%
%\subsection{M-Step}
%Now, let $q_{k}$ = $\sum_{j}{q_{j,k}}$. We can write the following Maximization step down.
%
%
%\begin{equation}
%\begin{align*}
%\pi_{k} &= \frac{1}{J}\sum_{j} {q_{jk}}\\
%\bm{m_{k}}&=\frac{1}{q_{k}}\sum_{j}{q_{jk} \bm{b_{jk}}}\\
%U_{k} &= \frac{1}{q_{k}}\sum_{j} {q_{jk}[(\bm{m_{k}}-\bm{b_{jk}})(\bm{m_{j}}-\bm{b_{jk}})^{T}+B_{jk}}]
%\end{align*}
%\end{equation}

%\section{Derivation of Conditional Posterior}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\subsection{Posteriors on genotype effect sizes}
%
%By maximum likelihood in each tissue separately, we can easily obtain the estimates of the standardized genotype effect sizes, $\hat{\bm{b}}_{j}$, and their standard errors recorded on the diagonal of an $R \times R$ matrix noted $\hat{V}_{j} = \Var(\hat{\bm{b}}_{j})$.
%Using each pair of tissues, we can also fill the off-diagonal elements of $\hat{V}_{j}$.
%
%If we now view $\hat{\bm{b}}_{j}$ and $\hat{V}_{j}$ as \emph{observations} (i.e. known), we can write a new ``likelihood'' (using only the sufficient statistics):
%\begin{equation}
%  \label{new_lik}
%  \hat{\bm{b}}_{j} | \bm{b}_{j} \sim \Norm_R(\bm{b}_{j}, \hat{V}_{j})
%\end{equation}
%
%
%Let us imagine first that the prior on $\bm{b}_{j}$ is not a mixture but a single Normal:
%
%$\bm{b}_{j} \sim \Norm_R (\bm{0}, U_{0})$.
%As this prior is conjuguate to the ``likelihood'' above, the posterior simply is (see \href{http://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions}{Wikipedia}):
%\[
%\bm{b}_{j} | \hat{\bm{b}}_{j} \sim \Norm_R(\bm{\mu}_{j1}, U_{j1})
%\]
%where:
%\begin{itemize}
%\item $\bm{\mu}_{j1} = U_{j1} (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j})$;
%\item $U_{j} = (U_{0}^{-1} + \hat{V}_{j}^{-1})^{-1}$.
%\end{itemize}
%
%In practice however, we use a mixture, as defined at the beginning of the document.
%
%
%Inside the sums, the posterior of the effect size for component $k$ can be written as:
%\begin{equation}
%  \begin{aligned}
%    p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, K) &= p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, U_{0k}) \\
%    &\propto \Lik(\bm{b}_{j}) p(\bm{b}_{j} | U_{0k}) \\
%    &\propto \exp[(\hat{\bm{b}}_{j} - \bm{b}_{j})^T \hat{V}_{j}^{-1} (\hat{\bm{b}}_{j} - \bm{b}_{j})] \; \exp(\bm{b}_{j}^T U_{0k}^{-1} \bm{b}_{j}) \\
%    &\propto \exp[\bm{b}_{j}^T (\hat{V}_{j}^{-1} + U_{0k}^{-1}) \bm{b}_{j} - \hat{\bm{b}}_{j}^T \hat{V}_{j}^{-1} \bm{b}_{j} - \bm{b}_{j}^T \hat{V}_{j}^{-1} \hat{\bm{b}}_{j}]
%  \end{aligned}
%\end{equation}
%
%Note that this also results from the fact that we assume that $\bm{b}_{j} \sim \Norm_R(\bm{0}, U_{0})$
%and thus that $\bm{\mu}_{0}$ is 0. Thus in the EM algorithm where we will keep updated the `true $\mu_{0}$ we can no longer consider this and so instead, we have:
%
%
%
%
%\begin{equation}
%  \begin{aligned}
%    p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, K) &= p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, U_{0k}) \\
%    &\propto \Lik(\bm{b}_{j}) p(\bm{b}_{j} | U_{0k}) \\
%    &\propto \exp[(\hat{\bm{b}}_{j} - \bm{b}_{j})^T \hat{V}_{j}^{-1} (\hat{\bm{b}}_{j} - \bm{b}_{j})] \; \exp(\bm{b}_{j}-\bm{\mu}_{0})^T U_{0k}^{-1} (\bm{b}_{j}-\bm{\mu}_{0}) \\
%    &\propto \exp[\bm{b}_{j}^T (\hat{V}_{j}^{-1} + U_{0k}^{-1}) \bm{b}_{j} - ( \hat{\bm{b}}_{j} ^{T} \hat{V}_{j}^{-1} + U_{0k}^{-1}\bm{\mu}_{0}) \bm{b}_{j} - \bm{b}_{j}^T (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{1}\bm{\mu}_{0})]
%  \end{aligned}
%\end{equation}
%
%
%
%
%Defining $\Omega = (\hat{V}_{j}^{-1} + U_{0k}^{-1})^{-1}$ and noting that it is symmetric, we can use the property $\Omega^{-1} \Omega^T = I$ to factorize everything (and ``complete the square''):
%\begin{equation}
%  \begin{aligned}
%    p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, U_{0k}) &\propto \exp[(\bm{b}_{j} - \Omega (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{-1} \bm{\mu}_{0}))^T \Omega^{-1} (\bm{b}_{j} - \Omega (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{-1} \bm{\mu}_{0}))]
%  \end{aligned}
%\end{equation}
%
%For some configurations, $U_{0k}$ may not be positive-definite, and thus not invertible.
%We therefore need to avoid writing $\Omega$ as a function of $U^{-1}$ in favor of $U$:
%\begin{equation}
%  \label{omega}
%  \begin{aligned}
%    \Omega &= (V^{-1} + U^{-1})^{-1} \\
%    &= ((V^{-1} + U^{-1}) U U^{-1})^{-1} \\
%    &= ((V^{-1} U + I) U^{-1})^{-1} \\
%    &= U (V^{-1} U + I)^{-1}
%  \end{aligned}
%\end{equation}
%
%Recognizing the kernel of a Normal distribution, we get:
%\begin{equation}
%  \label{post_b_jl}
%  \begin{aligned}
%    \bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, K \;  &\sim \; \Norm_R(\bm{\mu}_{j1k}, U_{j1k})
%  \end{aligned}
%\end{equation}
%where
%\begin{equation}
%  \label{post_b_jl_covar}
%  \begin{aligned}
%    U_{j1k} &= \Omega \\
%    &= U_{0k} \left( \hat{V}_{j}^{-1} U_{0k} + I \right)^{-1}
%  \end{aligned}
%\end{equation}
%and
%\begin{equation}
%  \label{post_b_jl_mean}
%  \begin{aligned}
%    \bm{\mu}_{j1k} &= \Omega \hat{V}_{j}^{-1} \hat{\bm{b}}_{j} \\
%    &= U_{j1k} \hat{V}_{j}^{-1} \hat{\bm{b}}_{j} \\
%  \end{aligned}
%\end{equation}
%
%and in the case when we do not assume $\bm{\mu}_{0}$ = 0, we have 
%
%
%\begin{equation}
%  \label{post_b_k_mean_notnull}
%  \begin{aligned}
%    \bm{\mu}_{j1k} &= \Omega (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{-1} \bm{\mu}_{0})) \\
%    &= U_{j1k}  (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{-1} \bm{\mu}_{0})) \\
%  \end{aligned}
%\end{equation}
%
%
%To compute the posterior weights, we will exploit the fact that the marginal likelihood corresponds to a Normal density when the conditional likelihood is Normal with known variance and the prior of its mean is also Normal (see Berger, 1985, example 1 in section 4.2).
%This means that we only need the mean and covariance matrix of this Normal density.
%
%With a small abuse of notation, let us consider below that $\hat{\bm{b}}_{j}$ is random and, using the law of total expectation with $\bm{b}_{j}$ as well as \ref{new_lik}, we obtain:
%\begin{equation}
%  \begin{aligned}
%    \Exp[\hat{\bm{b}}_{j} | \hat{V}_{j}, K] &= \Exp_{\bm{b}_{j}}[ \Exp[\hat{\bm{b}}_{j} | \hat{V}_{j}, K, \bm{b}_{j}] ]\\
%    &= \Exp_{\bm{b}_{j}}[\bm{b}_{j} | K] \\
%    &= \bm{0}
%  \end{aligned}
%\end{equation}
%
%Now using the law of total variance with $\bm{b}_{j}$ as well as \ref{new_lik} and \ref{prior_b_mixt_grid_config}, we obtain:
%\begin{equation}
%  \begin{aligned}
%    \Var[\hat{\bm{b}}_{j} | \hat{V}_{j}, K] &= \Exp_{\bm{b}_{j}}[ \Var[\hat{\bm{b}}_{j} | \hat{V}_{j}, K, \bm{b}_{j}] ] \\
%    &+ \Var_{\bm{b}_{j}}[ \Exp[\hat{\bm{b}}_{j} | \hat{V}_{j}, K, \bm{b}_{j}] ] \\
%    &= \Exp_{\bm{b}_{j}}[\hat{V}_{j}] + \Var_{\bm{b}_{j}}[\bm{b}_{j} | k] \\
%    &= \hat{V}_{j} + U_{0k}
%  \end{aligned}
%\end{equation}
%
%Therefore the posterior weight is:
%\begin{equation}
%  \label{post_w_jl}
%  \begin{aligned}
%    \tilde{w}_{jl} = \frac{\hat{\pi_k} \; \Norm_R(\hat{\bm{b}}_{j}; \bm{0}, U_{0k} + \hat{V}_{j})}{\sum_{k} \hat{\pi}_{k} \; \Norm_R(\hat{\bm{b}}_{j}; \bm{0}, U_{0k} + \hat{V}_{j})}
%  \end{aligned}
%\end{equation}
%
%The notation $\Norm_R(\hat{\bm{b}}_{j}; \bm{0}, U_{0k} + \hat{V}_{j})$ means that we calculate the multivariate Normal density with mean $\bm{0}$ and covariance matrix $U_{0k} + \hat{V}_{j}$ at the point $\hat{\bm{b}}_{j}$.
%
%
%\end{equation}
%
%It is quite straightforward from this document that \ref{post_b_k_mean_notnull} is equivalent to $\bm{b}_{jk}$ (the conditional component-specific posterior mean) and that   \ref{post_b_jl_covar} is equivalent to $B_{jk}$ ((the conditional component-specific posterior covariance matrix). Furthermore, $\mu_{0k}$ is equivalent to the $\bm{m_{k}}$ (representing the component-specific ``true" mean) and $U_{0k}$ is $U_{k}$ the same (the component specific `true covariance of the underlying effect).
%
%\section{Translation Algebra}
%
%For the authors:
%\begin{equation}
%  \begin{aligned}
%    \bm{b}_{jk}=U_{k} [U_{k}+\hat{V}_{j}]^{-1}\hat{\bm{b}_{j}}\\
%    B_{jk}=U_{k}-U_{k}[U_{k}+\hat{V}_{j}]^{-1}U_{k}
%    \end{aligned}
%\end{equation}
%
%For us:
%\begin{equation}
%  \begin{aligned}
%    \bm{b}_{jk}= U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1} \hat{V}_{j}^{-1} \hat{\bm{b}}_{j}\\
%    B_{jk} = U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1}
%    \end{aligned}
%\end{equation}
%
%
%Thus
%
%\begin{equation}
%U_{k}[U_{k}+\hat{V}_{j}]^{-1}U_{k} + U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1} = U_{k}
%\end{equation}
%
%Now, we recognize the $[AB]^{-1}$ = $B^{-1}A^{-1}$ and we let $[U_{k}+\hat{V}_{j}] = B$ and $\hat{V}^{-1} = A$.
%
%Then, if we adjust by $(\hat{V}^{-1})^{-1}\hat{V}^{-1}$, we can get  for the first part of the LHS
%\begin{equation}
%  \begin{aligned}
%U_{k}[U_{k}+\hat{V}_{j}]^{-1}(\hat{V}^{-1})^{-1}\hat{V}^{-1}U_{k}
%  \end{aligned}
%\end{equation}
%
%and break up $[U_{k}+\hat{V}_{j}]^{-1}(\hat{V}^{-1})^{-1}$
%into $(\hat{V}^{-1} [U_{k}+\hat{V}_{j}] )^{-1}$
%
% Now plugging this back in,we have:
%  
%\begin{equation}
%  \begin{aligned}
%&=U_{k}[\hat{V}_{j}^{-1}U_{k}+I]^{-1} \hat{V}_{j}^{-1} U_{k} + U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1} \\
%&= U_{k}[\hat{V}_{j}^{-1}U_{k}+I]^{-1}(\hat{V}_{j}^{-1} U_{k} + I )
%&=U_{k}
%\end{aligned}
%\end{equation}
%

%\section{Translation Algebra}
%
%For the authors:
%\begin{equation}
%\begin{aligned}
%  U( V^{-1} U+ I )^{-1} V^{-1} \hat{\bm{b}}_{j}\ &= U L [U+\hat{V}]^{-1}\hat{\bm{b}_{j}}\\
%U ( V^{-1} U + I )^{-1} &= U-U L' [U+V]^{-1} L U
%\end{aligned}
%\end{equation}
%
%For us:
%\begin{equation}
%  \begin{aligned}
%    \bm{b}_{jk}= U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1} \hat{V}_{j}^{-1} \hat{\bm{b}}_{j}\\
%    B_{jk} = U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1}
%    \end{aligned}
%\end{equation}

\section{Simulation}

 In this simulation framework, there are 1000 real associations in 10000 null across 44 tissues.
 
 Each `real association' is simulated in the following manner:
 
$\begin{verbatim}{

function(n=1000,d=44,betasd=1,esd=0.1,K=10){
  library("MASS")
  library("mvtnorm")
  J=0.10*n
  
  configs = matrix((rnorm(d*K)),byrow=T,ncol=d) # A matrix of K classes (patterns) across R subgroups 
  F=as.matrix(configs);
  covmat=lapply(seq(1:K),function(k){
    A=F[k,]%*%$t(F[k,]);
    A/max(diag(A))})
  ## each entry of F is the the factor of decomposition of covariance of effect sizes
  z = sample(K,J,replace=TRUE) # randomly sample factor to be loaded on for each real snp
  
  
  mus=rnorm(n)  ###generate a list of n mus
  mumat=matrix(rep(mus,d),ncol=d)##generate a matrix of mus for each gene
  omega=abs(rnorm(J,mean=0,sd=betasd))##effect size variance can be big or small
  beta=t(sapply(seq(1:J),function(j){
    k=z[j]
    mvrnorm(1,mu=rep(0,d),Sigma=omega[j]*covmat[[k]])
    #rmvnorm(1,mean = rep(0,d),sigma=omega*covmat[[k]])
  }))
  
  beta=rbind(beta,matrix(rep(0,(n-J)*d),ncol=d))
  c=beta+mumat
  sj=abs(matrix(rnorm(n*d,esd,0.001),ncol=d))##use uniform to simulate 'shrunken'
  e=t(apply(sj,1,function(x){rmvnorm(1,mean=rep(0,d),sigma=diag(x)^2)}))
  chat=c+e
  t=chat/sj
  return(list(beta=beta,chat=chat,covmat=covmat,components=z,t=t,mumat=mumat,
  shat=sj,error=e,ceff=c,F=F,omega=omega))}\end{verbatim}}$
  
  
Such that for every true associations a factor is chosen and 'standardized' such that the maximum value across the diagonal is one. The true effects are then simulated according to the assigned component, scaled by some factor $\omega$, and then and this scaling is added to a chosen mean for the gene, centered at 0 with $\sigma^{2}$ of 1. 

The true $ceff$ is then computed as 
$$ceff = \mu + \beta$$ 

and
$$chat = ceff + E$$ where $E \sim N(0,V)$ and V is diagonal.

This function reports the true $\mu$, the true $\beta$ for the 1000 real genes and their associated component, as well as the standard error.

\section{Simpler Simulation}

 In this simulation framework, there are 1000 real associations in 10000 null across 8 and 20 tissues. Here, the goal was to simulate a case in which the effect was 'off' in a subset of tissues and on in some: 
 
 Each 'real association' is simulated in the following manner:

\begin{verbatim}{function(n=1000,d=8,betasd=1,esd=0.1,K=10){
  library("MASS")
  library("mvtnorm")

  J=0.10*n
  temp=rep(list(c(0,1)),d)
  configs = expand.grid(temp) # all possible 2^d combinations
  S=sample(seq(1:nrow(configs)),size = K,replace = FALSE)##which factors will be used
  F=as.matrix(configs[S,])
  covmat=lapply(seq(1:K),function(k){
    A=F[k,]%*%t(F[k,]);
    A/max(diag(A))})
  ## each entry of F is the the factor of decomposition of covariance of effect sizes
  z = sample(K,J,replace=TRUE) # randomly sample factor to be loaded on for each real snp
mus=rnorm(n)  ###generate a list of n mus
  mumat=matrix(rep(mus,d),ncol=d)##generate a matrix of mus for each gene
  omega=abs(rnorm(J,mean=0,sd=betasd))##effect size variance can be big or small
  beta=t(sapply(seq(1:J),function(j){
    k=z[j]
    mvrnorm(1,mu=rep(0,d),Sigma=omega[j]*covmat[[k]])
    #rmvnorm(1,mean = rep(0,d),sigma=omega*covmat[[k]])
  }))
  
  beta=rbind(beta,matrix(rep(0,(n-J)*d),ncol=d))
  c=beta+mumat
  sj=abs(matrix(rnorm(n*d,esd,0.001),ncol=d))##use uniform to simulate 'shrunken'
  e=t(apply(sj,1,function(x){rmvnorm(1,mean=rep(0,d),sigma=diag(x)^2)}))
  chat=c+e
  t=chat/sj
  return(list(beta=beta,chat=chat,covmat=covmat,components=z,factors=F,t=t,mumat=mumat,shat=sj,error=e,ceff=c,omega=omega))}}\end{verbatim}
%  
  
Such that for every true associations a factor is chosen and 'standardized' such that the maximum value across the diagonal is one. The true effects are then simulated according to the assigned component, scaled by some factor $\omega$, and then and this scaling is added to a chosen mean for the gene, centered at o with $\sigma^{2}$ of 1. 

The true $ceff$ is then computed as 
$$ceff = \mu + \beta$$ 

and
$$chat = ceff + E$$ where $E \sim N(0,V)$ and V is diagonal.

This function reports the true $\mu$, the true $\beta$ for the 1000 real genes and their associated componenent, as well as the standard error.

As mentioned, in $\mnb$ we will use the input matrix of $w = t(\lstar t(\chat))$ and list of $\lstar V \lstar'$ covariance of the errors, while in \mash{}, we simply input w = $t (L t(\chat))$  and the JxR matrix of standard errors.




\end{document}  
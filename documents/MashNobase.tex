\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 

\usepackage{amsmath}

% EPS and PDF figures
\usepackage{graphicx}

% EPS and PDF figures
%\usepackage[nomarkers,figuresonly]{endfloat}


\usepackage{amssymb}
\usepackage{bm}
% \graphicspath{{./figures/}} % save all figures in the same directory
\usepackage{color} 
\usepackage{hyperref}
\usepackage{parskip}

\newcommand{\Prob}{\mathbb{P}} % symbol for proba
\newcommand{\Prd}{\mathsf{P}} % symbol for discrete proba
\newcommand{\Exp}{\mathbb{E}} % symbol for expectation
\newcommand{\Var}{\mathbb{V}} % symbol for variance
\newcommand{\Cov}{\mathbb{C}} % symbol for covariance
\newcommand{\Norm}{{\mathcal{N}}} % symbol for Normal distribution
\newcommand{\BF}{{\text{BF}}} % symbol for Bayes factor
\newcommand{\Lik}{{\mathcal{L}}} % symbol for likelihood
\newcommand{\beff}{\bm{b}}
\newcommand{\ceff}{\bm{C}}
\newcommand{\chat}{\bm{\hat{C}}}
\newcommand{\zhat}{\hat{Z}}
\newcommand{\vb}{\bm{v}}


\def\lfsr{{\it lfsr}}
\def\lfdr{{\it lfdr}}
\def\mash{{\tt mash}}
\def\bnorm{\tilde{b}}
\def\bhat{\hat{b}}
\def\vbhat{\bm \bhat}

\def\shat{\hat{s}}
\def\vshat{\bm{\shat}}
\def\Bhat{\hat{B}}
\def\B{B}
\def\pmean{\hat{\hat{b}}}
\def\L{L}
\def\F{F}
\def\CI{\text{CI}}
\nonstopmode

\title{ $\mash$ No Baseline}
\author{Sarah Urbut}
%\date{}							% Activate to display a given date or no date

\date{\today}
\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Purpose}

The purpose of this document is to propose a method for extending $\mash$ to estimate `true' effects across conditions in a setting in which no obvious baseline exists. We assume that we observe noisy, uncentered averages $\chat_{jr}$ in each of $R$ conditions, and seek to estimate the underlying true `deviations` from average measurement across conditions and can be seen as the effects in $\mash$.

Here, the use of bold-face notation indicates a vector, while matrix quantities are typeset in capital but unboldface letters.

%The purpose of this document is to express a way of selecting a set of covariance matrices for fitting a mixture of multivariate normals. Previously, we have used a fixed set of $U_k$ to represent the prior covariance matrices on the vector of `true` effects across tissues, $\bm{b}$. We then estimate the weights on each of these matrices $\pi$ hierarchically, using the EM algorithm. Here, we propose to estimates these covariance matrices simultaneously, thus reflecting the ideal patterns of covariance present in the data. We then rescale these matrices before, as in $\mash$. For the


\section{Defining the Old Model}

For a given gene-snp pair, $\bm{b}$ represents the $R$ vector of unknown standardized effect. We model the prior distribution from which $\bm{b}$ is drawn as a mixture of multivariate {\it Normals}.
 
 \begin{equation}
  \label{prior_b_mixt_grid}
  \bm{b} | \bm{\pi},\bf{U} \sim \sum_{k,l} \pi_{k,l} \;{\it N}_R(\bm{0}, \omega_l U_{k})
\end{equation}

Furthermore, for a given gene-snp pair, the Likelihood on $\bm{b}$: 
\begin{equation}
  \label{new_lik}
  \hat{\bm{b}} | \bm{b} \sim {\it N}_R(\bm{b}, \hat{V})
\end{equation}

Now, we observe for each gene $j$ a vector of uncentered noisy average feature expression $\chat$ across R conditions:

\begin{equation}
  \label{nobaselike}
\chat | \bm{\ceff} \sim {\it N}_R(\bm{\ceff}, \hat{V})
\end{equation}

where the `true' uncentered averages $\ceff$ can be written as follows:

\begin{equation}
  \label{uncenteredprior}
\ceff | \mu, \vb  = \mu \bm{1} + \vb
\end{equation}

Where $\mu$ is a scalar that is the mean of the `true' uncentered averages $\ceff$.

$\vb$ is a zero-centered mixture of multivariate normals:


 \begin{equation}
  \label{prior_v_mix}
  \vb | \bm{\pi},\bf{U} \sim \sum_{k,l} \pi_{k,l} \;{\it N}_R(\bm{0}, \omega_l U_{k})
\end{equation}

Critically, our quantity of interest now, $\vb$  represents the true `deviations` from average gene expression across each condition and can be seen as the effects in $\mash$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications}

We will again apply a two-step process to our selection of covariance matrices, where we select a set of denoised `pattern` matrices $U_{k}$ by using the EM algorithm on the max effects across conditions,
and then expanding this list by a fixed grid of scalar weights $\omega_{l}$ such that we conclude with a list of $P=KxL$ covariance matrices $\Sigma$. We can then:
\begin{itemize}
\item estimate the p prior weights $\bm\pi$ on this fixed P-list of covariance matrices from a training matrix of randomly selected feature expression measurements across conditions
\item compute the posterior distribution $\vb | L \chat, \bm{s_{j}}$

Let 

\begin{equation}
L \ceff = L \mu \bm{1} + L \vb
\end{equation}

L is the $RxR$ centering matrix $L_{r}=I_{r}-{\tfrac  {1}{r}}{\mathbf  {1}}{\mathbf  {1}}^{\top }$ which removes the mean of each R column vector.

Then :

\begin{equation}
\begin{aligned}
L \ceff &= L \mu \bm{1}+ L \vb \\
L \ceff &= 0 + L \vb \\
L \chat &= L \vb + E \\
\end{aligned}
\end{equation}



Where $E \sim \Norm (0, L\hat{V} L')$


\subsection{Selecting The Covariance Matrices}

We initiate our set of covariance matrices for the denoising step as before in $\mash$, where now we compute the empirical covariance matrices and a variety of dimensional reductions on the feature-centered JxR \textit{matrix} of maximum average, $L \hat{C}' $ instead of $\hat{C}$ alone. In practice, we actually use the matrix of maximum uncentered Z statistics.

Now for each gene J at each component k, integrating over $\vb$,


\begin{equation}
\begin{aligned}
\label{maxlike}
L \ceff \sim \Norm (0, L U_{k} L') \\
L \chat \sim \Norm (0, L U_{k} L' + L\hat{V} L') 
\end{aligned}
\end{equation}

And thus we can use the Bovy et al algorithm invoked in both the Extreme Deconvolution package and in `Sarah's MixEm' where:

\begin{equation}
 T_{jp} = L U_k L' + L\hat{V}_{j} L'
\end{equation}

And as mentioned, L is the $RxR$ centering matrix for each gene, and $w_{j} = L \chat_{j}$.

Recall that our previous approach was simplified by the fact that $\bm{w}_{j}$ was simply $\hat{\bm{b}_{j}}$ and the projection matrix was simply the $I_{r}$ identity matrix. Our inference on $\bm{b}$ was analogous to their inference on $\bv_{j}$. 


As before, we are interested in returning the prior covariance $U_k$ matrices of the `true` deviations $\vb$, which we will then rescale by choosing a set of $\omega$ that are appropriate to $L \chat $ to comprise a set of $P = KxL$ prior covariance matrices $\Sigma$.

and choose the set of $\pi$ that maximizes compute the following likelihood at each of the P components: 

\begin{equation}
L \chat _j \sim \Norm (0, L \Sigma_{p} L' + L\hat{V_j} L') 
\end{equation}
 

\section{Posteriors}

Now, as before we can compute a posterior distribution such that:

\begin{equation}
\vb | \L \chat, \pi, \Sigma, \bm{s} \sim N(\mu^{1} , U^{1})
\end{equation}

Where at each of the P components for each gene J 

\begin{equation}
\begin{aligned}
\mu^{1}_{jp} = \Sigma_{p} L' T_{jp}^{-1} L \chat_{j} \\
U^{1}_{jp} = \Sigma_{p} - \Sigma_p L' T_{jp}^{1} L \Sigma_{p}
\end{aligned}
\end{equation}


And 
\begin{equation}
\vb \sim \sum_p \tilde{\pi}_{p}   \Norm ( \mu_{p}^{1}, U_{p}^{1})
\end{equation}

Where 
\begin{equation}
\tilde{\pi}_{jp} = \frac{\Norm(L\chat;0, T_{jp})}{\sum_{p} \Norm(L\chat;0, T_{jp})}
\end{equation}

\section{Differences required over $\mash$ implementation}

\begin{itemize}
\item We will now work with a matrix of observed column-centered gene averages, $L \hat{C} '$ in order to:
\begin{enumerate}
	\item initialize our choice of $U_{k}$;
	\item  choose the maxes by which to denoise, 
	\item choose our set of scales, $\omega_{l}$
	\item compute our hierarchical weights, $\bm{\pi}_{p}$ as well as our posteriors. 
\end{enumerate}
\item The new distribution we seek to estimate for each j is then $v | L \chat , \bm{s}_{j}$
\end{itemize}

Some questions:

\begin{itemize}
\item How is using $\bm{w}_{j}$ as $L\chat_{j}$ as the data from which we estimate our model different than initializing with the vectors of $\hat{\bm{b}}$ that have been computed on $\textit{feature centered data}$?
\item  I think it is because we are now broadly incorporating the centering information into our prior on $L \ceff$ as well, such that at each component for each gene, $L \ceff_{j} \sim \Norm (0, L \Sigma_{jp} L')$.
\item If this is the case, we will probably need to denoise all of the initiation matrices (not just the multirank Uk) since previously our projection matrix was the Identity
\item Since our input will still be that matrix   \hat{C}  of uncentered noisy averages and their standard errors, our scaling parameter $\omega$ ought to be chosen consistent with $L \hat C$, and not $\hat C$, since this will tend to scale with the true deviations $\vb_{j}$.
\end{itemize}

%\subsection{E-Step}
%For a data set with J gene snp pairs and $K$ components:
%\begin{itemize}
%\item $U_{k}$ to represent the `true' covariance matrix of effects,
%\item$B_{jk}$ to represent the $RxR$ posterior conditional covariance matrix for each gene-snp pair at each component ($U_{1}$ above) 
%\item$\bm{b_{jk}}$ to represent the $R$-dimensional posterior mean for each gene-snp pair at each component (analogous $\bm{\mu_{1}}$) above.
%\item $\pi_{k}$ to represent the mixture proportions.
%\end{itemize}
%
%In the E- step, using the same notation as the authors Bovy et al where $q_{jk}$ represents the latent 'label' of each gene-snp pair according to its membership:
%
%\begin{equation}
%\begin{align*}
%q_{jk} &= \frac{\pi_{k} N (  \hat{\bm{b}} |0,U_{k}+V_{j})}{\sum_{k}{\pi_{k} N (\hat{\bm{b_{j}}}|0,U_{k}+V_{j}})} \\
%\bm{b_{jk}} &=  B_{jk} (U_{k}^{-1} \bm{m_{k}}+\hat{V_{j}}^{-1}  \hat{\bm{b}}) \\
%B_{jk}&=(U_{k}^{-1} + \hat{V_{j}^{-1}})^{-1}
%\end{align*}
%\end{equation}
%
%Quite simply, the latent indicator label is simply the likelihood at a particular component times the current update of the prior weight $\pi_{k}$ at that component, divided by the marginal probability of observing that gene snp pair. This is equivalent the posterior probability that a data point $j$ arose from component $K$. Note that the distribution $\hat{\bm{b_{j}}}|0,U_{k}+V_{j}})} $ results from integrating over the uncertainty in $\bm{b}_{j}$ and thus represents the variance of the marginal distribution of $\hat{b}$, the T used in ${\it Bovy et al}.$
%
%
%The current component specific posterior covariance $B_{jk}$ is the posterior covariance matrix of a single multivariate normal distribution and the current component-specific posterior mean $\bm{b_{jk}}$ is the posterior mean of a single multivariate normal. (see \href{http://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions}{Wikipedia})
%
%Note that this is slightly different than our expression for the posterior conditional mean $\bm{\mu_{1k}}$ above because in the previous model, we assumed that each $\bm{b_{j}} \sim N(0,U_{k})$ (i.e., the prior mean was $\bm{0}$) where here we estimate the underlying mean for each component, $\bm{m_{k}}$ using the EM algorithm and so we need to use the full formula for a multivariate normal with known residual matrix $V_{j}$ (please see section: Posteriors on genotype effect sizes for algebraic derivation).
%
%\subsection{M-Step}
%Now, let $q_{k}$ = $\sum_{j}{q_{j,k}}$. We can write the following Maximization step down.
%
%
%\begin{equation}
%\begin{align*}
%\pi_{k} &= \frac{1}{J}\sum_{j} {q_{jk}}\\
%\bm{m_{k}}&=\frac{1}{q_{k}}\sum_{j}{q_{jk} \bm{b_{jk}}}\\
%U_{k} &= \frac{1}{q_{k}}\sum_{j} {q_{jk}[(\bm{m_{k}}-\bm{b_{jk}})(\bm{m_{j}}-\bm{b_{jk}})^{T}+B_{jk}}]
%\end{align*}
%\end{equation}

%\section{Derivation of Conditional Posterior}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\subsection{Posteriors on genotype effect sizes}
%
%By maximum likelihood in each tissue separately, we can easily obtain the estimates of the standardized genotype effect sizes, $\hat{\bm{b}}_{j}$, and their standard errors recorded on the diagonal of an $R \times R$ matrix noted $\hat{V}_{j} = \Var(\hat{\bm{b}}_{j})$.
%Using each pair of tissues, we can also fill the off-diagonal elements of $\hat{V}_{j}$.
%
%If we now view $\hat{\bm{b}}_{j}$ and $\hat{V}_{j}$ as \emph{observations} (i.e. known), we can write a new ``likelihood'' (using only the sufficient statistics):
%\begin{equation}
%  \label{new_lik}
%  \hat{\bm{b}}_{j} | \bm{b}_{j} \sim \Norm_R(\bm{b}_{j}, \hat{V}_{j})
%\end{equation}
%
%
%Let us imagine first that the prior on $\bm{b}_{j}$ is not a mixture but a single Normal:
%
%$\bm{b}_{j} \sim \Norm_R (\bm{0}, U_{0})$.
%As this prior is conjuguate to the ``likelihood'' above, the posterior simply is (see \href{http://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions}{Wikipedia}):
%\[
%\bm{b}_{j} | \hat{\bm{b}}_{j} \sim \Norm_R(\bm{\mu}_{j1}, U_{j1})
%\]
%where:
%\begin{itemize}
%\item $\bm{\mu}_{j1} = U_{j1} (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j})$;
%\item $U_{j} = (U_{0}^{-1} + \hat{V}_{j}^{-1})^{-1}$.
%\end{itemize}
%
%In practice however, we use a mixture, as defined at the beginning of the document.
%
%
%Inside the sums, the posterior of the effect size for component $k$ can be written as:
%\begin{equation}
%  \begin{aligned}
%    p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, K) &= p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, U_{0k}) \\
%    &\propto \Lik(\bm{b}_{j}) p(\bm{b}_{j} | U_{0k}) \\
%    &\propto \exp[(\hat{\bm{b}}_{j} - \bm{b}_{j})^T \hat{V}_{j}^{-1} (\hat{\bm{b}}_{j} - \bm{b}_{j})] \; \exp(\bm{b}_{j}^T U_{0k}^{-1} \bm{b}_{j}) \\
%    &\propto \exp[\bm{b}_{j}^T (\hat{V}_{j}^{-1} + U_{0k}^{-1}) \bm{b}_{j} - \hat{\bm{b}}_{j}^T \hat{V}_{j}^{-1} \bm{b}_{j} - \bm{b}_{j}^T \hat{V}_{j}^{-1} \hat{\bm{b}}_{j}]
%  \end{aligned}
%\end{equation}
%
%Note that this also results from the fact that we assume that $\bm{b}_{j} \sim \Norm_R(\bm{0}, U_{0})$
%and thus that $\bm{\mu}_{0}$ is 0. Thus in the EM algorithm where we will keep updated the `true $\mu_{0}$ we can no longer consider this and so instead, we have:
%
%
%
%
%\begin{equation}
%  \begin{aligned}
%    p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, K) &= p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, U_{0k}) \\
%    &\propto \Lik(\bm{b}_{j}) p(\bm{b}_{j} | U_{0k}) \\
%    &\propto \exp[(\hat{\bm{b}}_{j} - \bm{b}_{j})^T \hat{V}_{j}^{-1} (\hat{\bm{b}}_{j} - \bm{b}_{j})] \; \exp(\bm{b}_{j}-\bm{\mu}_{0})^T U_{0k}^{-1} (\bm{b}_{j}-\bm{\mu}_{0}) \\
%    &\propto \exp[\bm{b}_{j}^T (\hat{V}_{j}^{-1} + U_{0k}^{-1}) \bm{b}_{j} - ( \hat{\bm{b}}_{j} ^{T} \hat{V}_{j}^{-1} + U_{0k}^{-1}\bm{\mu}_{0}) \bm{b}_{j} - \bm{b}_{j}^T (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{1}\bm{\mu}_{0})]
%  \end{aligned}
%\end{equation}
%
%
%
%
%Defining $\Omega = (\hat{V}_{j}^{-1} + U_{0k}^{-1})^{-1}$ and noting that it is symmetric, we can use the property $\Omega^{-1} \Omega^T = I$ to factorize everything (and ``complete the square''):
%\begin{equation}
%  \begin{aligned}
%    p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, U_{0k}) &\propto \exp[(\bm{b}_{j} - \Omega (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{-1} \bm{\mu}_{0}))^T \Omega^{-1} (\bm{b}_{j} - \Omega (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{-1} \bm{\mu}_{0}))]
%  \end{aligned}
%\end{equation}
%
%For some configurations, $U_{0k}$ may not be positive-definite, and thus not invertible.
%We therefore need to avoid writing $\Omega$ as a function of $U^{-1}$ in favor of $U$:
%\begin{equation}
%  \label{omega}
%  \begin{aligned}
%    \Omega &= (V^{-1} + U^{-1})^{-1} \\
%    &= ((V^{-1} + U^{-1}) U U^{-1})^{-1} \\
%    &= ((V^{-1} U + I) U^{-1})^{-1} \\
%    &= U (V^{-1} U + I)^{-1}
%  \end{aligned}
%\end{equation}
%
%Recognizing the kernel of a Normal distribution, we get:
%\begin{equation}
%  \label{post_b_jl}
%  \begin{aligned}
%    \bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, K \;  &\sim \; \Norm_R(\bm{\mu}_{j1k}, U_{j1k})
%  \end{aligned}
%\end{equation}
%where
%\begin{equation}
%  \label{post_b_jl_covar}
%  \begin{aligned}
%    U_{j1k} &= \Omega \\
%    &= U_{0k} \left( \hat{V}_{j}^{-1} U_{0k} + I \right)^{-1}
%  \end{aligned}
%\end{equation}
%and
%\begin{equation}
%  \label{post_b_jl_mean}
%  \begin{aligned}
%    \bm{\mu}_{j1k} &= \Omega \hat{V}_{j}^{-1} \hat{\bm{b}}_{j} \\
%    &= U_{j1k} \hat{V}_{j}^{-1} \hat{\bm{b}}_{j} \\
%  \end{aligned}
%\end{equation}
%
%and in the case when we do not assume $\bm{\mu}_{0}$ = 0, we have 
%
%
%\begin{equation}
%  \label{post_b_k_mean_notnull}
%  \begin{aligned}
%    \bm{\mu}_{j1k} &= \Omega (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{-1} \bm{\mu}_{0})) \\
%    &= U_{j1k}  (\hat{V}_{j}^{-1} \hat{\bm{b}}_{j} + U_{0k}^{-1} \bm{\mu}_{0})) \\
%  \end{aligned}
%\end{equation}
%
%
%To compute the posterior weights, we will exploit the fact that the marginal likelihood corresponds to a Normal density when the conditional likelihood is Normal with known variance and the prior of its mean is also Normal (see Berger, 1985, example 1 in section 4.2).
%This means that we only need the mean and covariance matrix of this Normal density.
%
%With a small abuse of notation, let us consider below that $\hat{\bm{b}}_{j}$ is random and, using the law of total expectation with $\bm{b}_{j}$ as well as \ref{new_lik}, we obtain:
%\begin{equation}
%  \begin{aligned}
%    \Exp[\hat{\bm{b}}_{j} | \hat{V}_{j}, K] &= \Exp_{\bm{b}_{j}}[ \Exp[\hat{\bm{b}}_{j} | \hat{V}_{j}, K, \bm{b}_{j}] ]\\
%    &= \Exp_{\bm{b}_{j}}[\bm{b}_{j} | K] \\
%    &= \bm{0}
%  \end{aligned}
%\end{equation}
%
%Now using the law of total variance with $\bm{b}_{j}$ as well as \ref{new_lik} and \ref{prior_b_mixt_grid_config}, we obtain:
%\begin{equation}
%  \begin{aligned}
%    \Var[\hat{\bm{b}}_{j} | \hat{V}_{j}, K] &= \Exp_{\bm{b}_{j}}[ \Var[\hat{\bm{b}}_{j} | \hat{V}_{j}, K, \bm{b}_{j}] ] \\
%    &+ \Var_{\bm{b}_{j}}[ \Exp[\hat{\bm{b}}_{j} | \hat{V}_{j}, K, \bm{b}_{j}] ] \\
%    &= \Exp_{\bm{b}_{j}}[\hat{V}_{j}] + \Var_{\bm{b}_{j}}[\bm{b}_{j} | k] \\
%    &= \hat{V}_{j} + U_{0k}
%  \end{aligned}
%\end{equation}
%
%Therefore the posterior weight is:
%\begin{equation}
%  \label{post_w_jl}
%  \begin{aligned}
%    \tilde{w}_{jl} = \frac{\hat{\pi_k} \; \Norm_R(\hat{\bm{b}}_{j}; \bm{0}, U_{0k} + \hat{V}_{j})}{\sum_{k} \hat{\pi}_{k} \; \Norm_R(\hat{\bm{b}}_{j}; \bm{0}, U_{0k} + \hat{V}_{j})}
%  \end{aligned}
%\end{equation}
%
%The notation $\Norm_R(\hat{\bm{b}}_{j}; \bm{0}, U_{0k} + \hat{V}_{j})$ means that we calculate the multivariate Normal density with mean $\bm{0}$ and covariance matrix $U_{0k} + \hat{V}_{j}$ at the point $\hat{\bm{b}}_{j}$.
%
%
%\end{equation}
%
%It is quite straightforward from this document that \ref{post_b_k_mean_notnull} is equivalent to $\bm{b}_{jk}$ (the conditional component-specific posterior mean) and that   \ref{post_b_jl_covar} is equivalent to $B_{jk}$ ((the conditional component-specific posterior covariance matrix). Furthermore, $\mu_{0k}$ is equivalent to the $\bm{m_{k}}$ (representing the component-specific ``true" mean) and $U_{0k}$ is $U_{k}$ the same (the component specific `true covariance of the underlying effect).
%
%\section{Translation Algebra}
%
%For the authors:
%\begin{equation}
%  \begin{aligned}
%    \bm{b}_{jk}=U_{k} [U_{k}+\hat{V}_{j}]^{-1}\hat{\bm{b}_{j}}\\
%    B_{jk}=U_{k}-U_{k}[U_{k}+\hat{V}_{j}]^{-1}U_{k}
%    \end{aligned}
%\end{equation}
%
%For us:
%\begin{equation}
%  \begin{aligned}
%    \bm{b}_{jk}= U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1} \hat{V}_{j}^{-1} \hat{\bm{b}}_{j}\\
%    B_{jk} = U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1}
%    \end{aligned}
%\end{equation}
%
%
%Thus
%
%\begin{equation}
%U_{k}[U_{k}+\hat{V}_{j}]^{-1}U_{k} + U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1} = U_{k}
%\end{equation}
%
%Now, we recognize the $[AB]^{-1}$ = $B^{-1}A^{-1}$ and we let $[U_{k}+\hat{V}_{j}] = B$ and $\hat{V}^{-1} = A$.
%
%Then, if we adjust by $(\hat{V}^{-1})^{-1}\hat{V}^{-1}$, we can get  for the first part of the LHS
%\begin{equation}
%  \begin{aligned}
%U_{k}[U_{k}+\hat{V}_{j}]^{-1}(\hat{V}^{-1})^{-1}\hat{V}^{-1}U_{k}
%  \end{aligned}
%\end{equation}
%
%and break up $[U_{k}+\hat{V}_{j}]^{-1}(\hat{V}^{-1})^{-1}$
%into $(\hat{V}^{-1} [U_{k}+\hat{V}_{j}] )^{-1}$
%
% Now plugging this back in,we have:
%  
%\begin{equation}
%  \begin{aligned}
%&=U_{k}[\hat{V}_{j}^{-1}U_{k}+I]^{-1} \hat{V}_{j}^{-1} U_{k} + U_{k} \left( \hat{V}_{j}^{-1} U_{k} + I \right)^{-1} \\
%&= U_{k}[\hat{V}_{j}^{-1}U_{k}+I]^{-1}(\hat{V}_{j}^{-1} U_{k} + I )
%&=U_{k}
%\end{aligned}
%\end{equation}
%



\end{document}  